{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74568855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports & Initialization ---\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from openai import OpenAI\n",
    "import configparser\n",
    "import tiktoken\n",
    "import logging\n",
    "import nltk\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Phrases\n",
    "\n",
    "SAVE_DIR = \"Saved_files_new\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "459d00f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Utility Functions: Stopwords, Keyword Extraction, and API ---\n",
    "\n",
    "def extract_keywords_from_filename(filename):\n",
    "    # Extract search keywords from filename\n",
    "    base = os.path.splitext(os.path.basename(filename))[0]\n",
    "    parts = base.split('_')\n",
    "    keywords = [part for i, part in enumerate(parts) if i > 2 and part != 'results' and not part.isdigit()]\n",
    "    return keywords\n",
    "\n",
    "def keywords_to_filename_part(keywords):\n",
    "    return '_'.join([kw.lower().replace(' ', '_') for kw in keywords])\n",
    "\n",
    "def get_custom_stop_words(search_keywords=None):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words_to_keep = set()\n",
    "    if search_keywords:\n",
    "        for keyword in search_keywords:\n",
    "            keyword = keyword.lower()\n",
    "            words_to_keep.add(keyword)\n",
    "            for word in keyword.split():\n",
    "                words_to_keep.add(word)\n",
    "    stop_words = stop_words - words_to_keep\n",
    "    scientific_terms = {'et', 'al', 'ref', 'reference', 'references', 'cited', 'cite', 'fig', 'figure', 'figures', 'table', 'tables',\n",
    "                        'chart', 'charts', 'published', 'journal', 'conference', 'proceedings', 'vol', 'volume', 'pp', 'page', 'pages', 'doi'}\n",
    "    stop_words = stop_words.union(scientific_terms)\n",
    "    return stop_words\n",
    "\n",
    "def initialize_openai():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('config_LLM.txt')\n",
    "    api_key = config['LLM'].get('OPENAI_API_KEY')\n",
    "    model_type = config['LLM'].get('MODEL_TYPE')\n",
    "    return OpenAI(api_key=api_key), model_type\n",
    "\n",
    "class CreditTracker:\n",
    "    def __init__(self):\n",
    "        self.total_tokens = 0\n",
    "        self.total_cost = 0\n",
    "        self.cost_per_1k_tokens = 0.00015\n",
    "\n",
    "    def update(self, tokens):\n",
    "        self.total_tokens += tokens\n",
    "        self.total_cost += (tokens / 1000) * self.cost_per_1k_tokens\n",
    "\n",
    "    def get_stats(self):\n",
    "        return {\"total_tokens\": self.total_tokens, \"total_cost\": round(self.total_cost, 4)}\n",
    "\n",
    "def num_tokens_from_string(string: str, model_name: str) -> int:\n",
    "    encoding = tiktoken.encoding_for_model(model_name)\n",
    "    return len(encoding.encode(string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07b89680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Preprocess and Lemmatize ---\n",
    "def preprocess_text(text, search_keywords=None, min_word_length=2, remove_numbers=True):\n",
    "    if not isinstance(text, (str, int, float)):\n",
    "        return ''\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    if remove_numbers:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s-]', '', text)\n",
    "    text = re.sub(r'--+', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = get_custom_stop_words(search_keywords)\n",
    "    tokens = [t for t in tokens if len(t) >= min_word_length and t not in stop_words and len(t) > 1 and not t.isdigit()]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    try:\n",
    "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    except:\n",
    "        pass\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess_dataframe(df, text_col, search_keywords, processed_col='processed_text'):\n",
    "    df[text_col] = df[text_col].fillna('')\n",
    "    df[text_col] = df[text_col].astype(str)\n",
    "    df[processed_col] = df[text_col].apply(lambda x: preprocess_text(x, search_keywords))\n",
    "    df = df[df[processed_col].str.strip() != '']\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0789ac91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted search keywords: ['reliability', 'resilience', 'power', 'systems']\n"
     ]
    }
   ],
   "source": [
    "# --- Load, Clean, and Preprocess Input ---\n",
    "filename = \"semantic_scholar_2025_02_14_reliability_resilience_power_systems_results.csv\"\n",
    "filepath = os.path.join(\"Saved_files\", filename)\n",
    "\n",
    "df = pd.read_csv(filepath, sep=\";\")\n",
    "df['text'] = df['title'].fillna('') + ' ' + df['abstract'].fillna('')\n",
    "search_keywords = extract_keywords_from_filename(filename)\n",
    "print(f\"Extracted search keywords: {search_keywords}\")\n",
    "df = preprocess_dataframe(df, text_col='text', search_keywords=search_keywords)\n",
    "\n",
    "# Remove or impute 'fieldsOfStudy'\n",
    "def clean_fields_of_study(s):\n",
    "    valid_fields = ['Computer Science', 'Economics', 'Engineering', 'Physics', 'Mathematics',\n",
    "                    'Medicine', 'Business', 'Environmental Science', 'Chemistry', 'Materials Science',\n",
    "                    'Geography', 'Biology', 'Geology', 'Political Science', 'Psychology', 'Com']\n",
    "    if pd.isna(s) or s == '[]':\n",
    "        return [\"Unknown\"]\n",
    "    if isinstance(s, str):\n",
    "        fields = [field.strip().strip(\"'\\\"\") for field in s.strip('[]').split(',')]\n",
    "        cleaned_fields = [f if f in valid_fields else \"Unknown\" for f in fields]\n",
    "        return cleaned_fields if cleaned_fields else [\"Unknown\"]\n",
    "    return [\"Unknown\"]\n",
    "\n",
    "df['fieldsOfStudy'] = df['fieldsOfStudy'].apply(clean_fields_of_study)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20439ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-16 08:33:43,301 - INFO - collecting all words and their counts\n",
      "2025-08-16 08:33:43,302 - INFO - PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2025-08-16 08:33:45,361 - INFO - PROGRESS: at sentence #10000, processed 1524957 words and 863761 word types\n",
      "2025-08-16 08:33:47,387 - INFO - PROGRESS: at sentence #20000, processed 3004878 words and 1467564 word types\n",
      "2025-08-16 08:33:49,218 - INFO - collected 1902495 token types (unigram + bigrams) from a corpus of 4290297 words and 28934 sentences\n",
      "2025-08-16 08:33:49,219 - INFO - merged Phrases<1902495 vocab, min_count=10, threshold=50, max_vocab_size=40000000>\n",
      "2025-08-16 08:33:49,220 - INFO - Phrases lifecycle event {'msg': 'built Phrases<1902495 vocab, min_count=10, threshold=50, max_vocab_size=40000000> in 5.92s', 'datetime': '2025-08-16T08:33:49.220518', 'gensim': '4.3.2', 'python': '3.11.13 (main, Jun 12 2025, 12:41:34) [MSC v.1943 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'created'}\n",
      "2025-08-16 08:33:49,222 - INFO - collecting all words and their counts\n",
      "2025-08-16 08:33:49,224 - INFO - PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2025-08-16 08:33:53,219 - INFO - PROGRESS: at sentence #10000, processed 1471162 words and 886074 word types\n",
      "2025-08-16 08:33:58,078 - INFO - PROGRESS: at sentence #20000, processed 2890563 words and 1519087 word types\n",
      "2025-08-16 08:34:01,484 - INFO - collected 1981629 token types (unigram + bigrams) from a corpus of 4117577 words and 28934 sentences\n",
      "2025-08-16 08:34:01,485 - INFO - merged Phrases<1981629 vocab, min_count=5, threshold=50, max_vocab_size=40000000>\n",
      "2025-08-16 08:34:01,486 - INFO - Phrases lifecycle event {'msg': 'built Phrases<1981629 vocab, min_count=5, threshold=50, max_vocab_size=40000000> in 12.26s', 'datetime': '2025-08-16T08:34:01.486743', 'gensim': '4.3.2', 'python': '3.11.13 (main, Jun 12 2025, 12:41:34) [MSC v.1943 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "# --- LDA Topic Modeling (papers grouped by scientific themes) ---\n",
    "def model_topics(df, num_topics=10, num_words=100):\n",
    "    \"\"\"\n",
    "    Standard thematic topic modeling.\n",
    "    \"\"\"\n",
    "    tokenized_texts = df['processed_text'].apply(lambda x: x.split()).tolist()\n",
    "    bigram = Phrases(tokenized_texts, min_count=10, threshold=50, delimiter='_')\n",
    "    trigram = Phrases(bigram[tokenized_texts], threshold=50, delimiter='_')\n",
    "    phrased = []\n",
    "    for doc in tokenized_texts:\n",
    "        bigrams_ = [w for w in bigram[doc] if '_' in w]\n",
    "        trigrams_ = [w for w in trigram[bigram[doc]] if '_' in w]\n",
    "        combined = doc + bigrams_ + trigrams_\n",
    "        phrased.append(' '.join(combined))\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 1), token_pattern=r'\\b[\\w_-]+\\b', max_df=0.95, min_df=2, max_features=10000)\n",
    "    doc_term_matrix = vectorizer.fit_transform(phrased)\n",
    "    lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "    topic_distributions = lda_model.fit_transform(doc_term_matrix)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    # Store top words for topics\n",
    "    def extract_topic_keywords(lda_model, feature_names, num_words=10):\n",
    "        topic_keywords = {}\n",
    "        for topic_idx, topic in enumerate(lda_model.components_):\n",
    "            top_indices = topic.argsort()[:-num_words-1:-1]\n",
    "            top_words = [feature_names[i] for i in top_indices]\n",
    "            word_weights = [(feature_names[i], topic[i]) for i in top_indices]\n",
    "            topic_keywords[topic_idx] = {'top_words': top_words, 'word_weights': word_weights}\n",
    "        return topic_keywords\n",
    "    topic_keywords = extract_topic_keywords(lda_model, feature_names, num_words)\n",
    "    return lda_model, vectorizer, topic_distributions, df, topic_keywords\n",
    "\n",
    "lda_model, vectorizer, topic_distributions, df_topic, topic_keywords = model_topics(df, num_topics=10, num_words=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fabcbd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-16 08:36:37,810 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-16 08:36:38,595 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-16 08:36:39,448 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-16 08:36:40,061 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-16 08:36:40,677 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-16 08:36:41,139 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-16 08:36:41,626 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-16 08:36:42,313 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-16 08:36:42,827 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-16 08:36:43,560 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Names: {0: '\"Thermal Efficiency of Solar Power Plants\"', 1: '\"Adaptive Channel Allocation in Wireless Networks\"', 2: '\"Cloud-Based IoT Performance Optimization\"', 3: '\"Smart Grid Data Management\"', 4: '\"Fault-Tolerant Control for DC Grid Systems\"', 5: '\"Optimizing Wind Energy Distribution Systems\"', 6: '\"Renewable Energy Storage Optimization\"', 7: '\"Reliability Assessment Models\"', 8: '\"High-Performance Battery Materials\"', 9: '\"Renewable Energy Generation Costs\"'}\n"
     ]
    }
   ],
   "source": [
    "# --- Assign Names to Each Topic Using LLM ---\n",
    "def generate_topic_name(top_words, client, model_type, credit_tracker):\n",
    "    prompt = f\"From this list of keywords: {', '.join(top_words)}, generate a concise, specific research topic name (preferably a bigram or trigram):\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_type,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert scientific assistant generating concise topic names.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    credit_tracker.update(num_tokens_from_string(prompt, model_type))\n",
    "    # Assume plain output, but adjust this as needed\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "client, model_type = initialize_openai()\n",
    "credit_tracker = CreditTracker()\n",
    "\n",
    "topic_names = {}\n",
    "for topic_idx, keywords in topic_keywords.items():\n",
    "    name = generate_topic_name(keywords['top_words'], client, model_type, credit_tracker)\n",
    "    topic_names[topic_idx] = name\n",
    "\n",
    "print(\"Topic Names:\", topic_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d9bffec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 12:15:41,985 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top method phrases: ['load flow analysis', 'reliability assessment', 'monte carlo simulation', 'loss of load probability', 'probabilistic methods', 'power flow', 'energy management', 'optimization', 'neural network', 'fuzzy', 'dynamic modeling', 'state estimation', 'dispatch', 'signal processing', 'computer simulation']\n"
     ]
    }
   ],
   "source": [
    "# --- Extract Method Phrases Using LLM ---\n",
    "def extract_candidate_terms(df, text_col='processed_text', max_features=300):\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 3), max_df=0.95, min_df=2, max_features=max_features, token_pattern=r'\\b[\\w-]+\\b')\n",
    "    matrix = vectorizer.fit_transform(df[text_col].fillna(''))\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    freqs = matrix.sum(axis=0).A1\n",
    "    sorted_terms = sorted(zip(terms, freqs), key=lambda x: x[1], reverse=True)\n",
    "    return [term for term, freq in sorted_terms]\n",
    "\n",
    "\n",
    "def extract_candidate_terms(df, text_col='processed_text', max_features=100):\n",
    "    \"\"\"Extract candidate keywords and n-grams from processed text for LLM prompt testing\"\"\"\n",
    "    vectorizer = CountVectorizer(\n",
    "        ngram_range=(1, 3),\n",
    "        max_df=0.95,\n",
    "        min_df=2,\n",
    "        max_features=max_features,\n",
    "        token_pattern=r'\\b[\\w-]+\\b'\n",
    "    )\n",
    "    matrix = vectorizer.fit_transform(df[text_col].fillna(''))\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    freqs = matrix.sum(axis=0).A1\n",
    "    # Sort terms by frequency descending\n",
    "    sorted_terms = sorted(zip(terms, freqs), key=lambda x: x[1], reverse=True)\n",
    "    # Return just the terms\n",
    "    return [term for term, freq in sorted_terms]\n",
    "\n",
    "def get_method_phrases(corpus_terms, client, model_type, credit_tracker):\n",
    "    \"\"\"Improved prompt that considers the actual domain of your corpus\"\"\"\n",
    "    \n",
    "    # Analyze the candidate terms to understand the domain\n",
    "    sample_terms = ', '.join(corpus_terms[:50])\n",
    "    \n",
    "    prompt = f\"\"\"Here are the most frequent terms from a corpus of scientific papers:\n",
    "{sample_terms}\n",
    "\n",
    "Based on these terms, this appears to be a corpus focused on power systems, electrical engineering, and reliability analysis.\n",
    "\n",
    "From the full list of terms: {', '.join(corpus_terms)}\n",
    "\n",
    "Extract ONLY the terms that represent specific methodologies, techniques, or named approaches that would actually appear in this type of engineering research. Focus on:\n",
    "- Power system analysis methods\n",
    "- Reliability analysis techniques  \n",
    "- Engineering design approaches\n",
    "- Computational methods used in power/electrical engineering\n",
    "- Statistical methods for engineering\n",
    "\n",
    "Do NOT include: generic words like \"analysis\", \"method\", \"approach\", \"design\", \"system\" by themselves, nor general expressions like distributed generation, renewable resources that dont specifically describe a method or technique.\n",
    "DO include: specific named methods like \"monte carlo simulation\", \"load flow analysis\", \"reliability assessment\", loss of load probability, probabilitstic methods, etc.\n",
    "\n",
    "Return as a simple Python list of strings, no code blocks or formatting.\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_type,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        return ast.literal_eval(response.choices[0].message.content)\n",
    "    except:\n",
    "        # Fallback parsing\n",
    "        content = response.choices[0].message.content\n",
    "        content = content.replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", '')\n",
    "        return [term.strip() for term in content.split(',') if len(term.strip()) > 3]\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_method_phrases_fixed(method_phrases):\n",
    "    \"\"\"Clean and validate method phrases from LLM output\"\"\"\n",
    "    cleaned_phrases = []\n",
    "    \n",
    "    for phrase in method_phrases:\n",
    "        # Remove code block markers, quotes, and extra whitespace\n",
    "        cleaned = phrase.strip()\n",
    "        cleaned = cleaned.replace('```python', '').replace('```','')\n",
    "        cleaned = cleaned.replace('[', '').replace(']', '')\n",
    "        cleaned = cleaned.replace('\"', '').replace(\"'\", '')\n",
    "        cleaned = cleaned.replace('\\n', ' ')\n",
    "        cleaned = ' '.join(cleaned.split())  # Remove extra whitespace\n",
    "        \n",
    "        # Skip empty or very short phrases\n",
    "        if len(cleaned) > 2:\n",
    "            cleaned_phrases.append(cleaned.lower())\n",
    "    \n",
    "    return list(set(cleaned_phrases))\n",
    "\n",
    "\n",
    "def validate_method_phrases_improved_fixed(df, method_phrases):\n",
    "    \"\"\"Improved validation that handles multi-word phrases\"\"\"\n",
    "    all_text = ' '.join(df['processed_text']).lower()\n",
    "    matched_phrases = []\n",
    "    \n",
    "    for phrase in method_phrases:\n",
    "        phrase_clean = phrase.lower().strip()\n",
    "        \n",
    "        # Check for exact phrase match\n",
    "        if phrase_clean in all_text:\n",
    "            matched_phrases.append(phrase)\n",
    "        # Check for partial word matches (for compound terms)\n",
    "        elif any(word in all_text for word in phrase_clean.split() if len(word) > 3):\n",
    "            matched_phrases.append(phrase)\n",
    "    \n",
    "    return matched_phrases\n",
    "candidate_terms = extract_candidate_terms(df, text_col='processed_text', max_features=1000)\n",
    "method_phrases = get_method_phrases(candidate_terms, client, model_type, credit_tracker)\n",
    "print(\"Top method phrases:\", method_phrases[:15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8831dcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 12:11:34,554 - INFO - Reducing method phrases (40) to top 25.\n"
     ]
    }
   ],
   "source": [
    "# --- LDA-based Method Detection ---\n",
    "def lda_method_assignment(\n",
    "    df, method_phrases, processed_col='processed_text', max_method_topics=30, min_papers_per_topic=15\n",
    "):\n",
    "    # Reduction if many method phrases\n",
    "    if len(method_phrases) > max_method_topics:\n",
    "        logger.info(f\"Reducing method phrases ({len(method_phrases)}) to top {max_method_topics}.\")\n",
    "        tfidf_vectorizer = TfidfVectorizer(vocabulary=method_phrases, ngram_range=(1, 3), min_df=1, max_df=0.95, norm='l2')\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(df[processed_col])\n",
    "        total_method_scores = np.asarray(tfidf_matrix.sum(axis=0)).ravel()\n",
    "        phrase_ranking = np.argsort(total_method_scores)[-max_method_topics:][::-1]\n",
    "        best_phrases = [tfidf_vectorizer.get_feature_names_out()[i] for i in phrase_ranking]\n",
    "    else:\n",
    "        best_phrases = method_phrases\n",
    "\n",
    "    # LDA with [number of] method topics\n",
    "    vectorizer = CountVectorizer(vocabulary=best_phrases, ngram_range=(1, 3), token_pattern=r'\\b[\\w-]+\\b')\n",
    "    doc_term_matrix = vectorizer.fit_transform(df[processed_col])\n",
    "    n_method_topics = len(best_phrases)\n",
    "    if n_method_topics < 2:\n",
    "        logger.warning(\"Not enough method phrases for LDA method assignment. Skipping.\")\n",
    "        df['Primary_Method_LDA'] = 'No_Method_Found'\n",
    "        df['Method_LDA_Score'] = 0.0\n",
    "        return df\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=n_method_topics, learning_method='batch', random_state=42, max_iter=20)\n",
    "    doc_topic_dist = lda.fit_transform(doc_term_matrix)\n",
    "    topic_labels = best_phrases\n",
    "\n",
    "    best_topic_idx = doc_topic_dist.argmax(axis=1)\n",
    "    best_topic_val = doc_topic_dist[np.arange(len(df)), best_topic_idx]\n",
    "    assigned_methods = [topic_labels[i] if best_topic_val[j] > 1/n_method_topics+0.05 else 'LowConfidence'\n",
    "                        for j, i in enumerate(best_topic_idx)]\n",
    "\n",
    "    # Mask rare method-topics\n",
    "    topic_assignment_counts = pd.Series(best_topic_idx).value_counts()\n",
    "    rare_topics = topic_assignment_counts[topic_assignment_counts < min_papers_per_topic].index.tolist()\n",
    "    assigned_methods = [\n",
    "        'LowConfidence' if idx in rare_topics or label == 'LowConfidence' else topic_labels[idx]\n",
    "        for (idx, label) in zip(best_topic_idx, assigned_methods)\n",
    "    ]\n",
    "\n",
    "    df['Primary_Method_LDA'] = assigned_methods\n",
    "    df['Method_LDA_Score'] = best_topic_val\n",
    "    # Save the full method-topic distribution for each doc (optional)\n",
    "    df['Top_3_Methods_LDA'] = [\n",
    "        [topic_labels[i] for i in doc_topic_dist[j].argsort()[-3:][::-1]]\n",
    "        for j in range(doc_topic_dist.shape[0])\n",
    "    ]\n",
    "    df['Top_3_Methods_LDA_Scores'] = [\n",
    "        [doc_topic_dist[j, i] for i in doc_topic_dist[j].argsort()[-3:][::-1]]\n",
    "        for j in range(doc_topic_dist.shape[0])\n",
    "    ]\n",
    "    return df\n",
    "\n",
    "df_methods = lda_method_assignment(df, method_phrases, processed_col='processed_text', max_method_topics=25, min_papers_per_topic=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12d7edd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to Saved_files_new\\semantic_scholar_2025_08_18_final_lda_method.csv\n",
      "LDA-based method label distribution:\n",
      "Primary_Method_LDA\n",
      "LowConfidence                  16816\n",
      "distributed generation          3277\n",
      "fault analysis                  1861\n",
      "dynamic modeling                1348\n",
      "state estimation                1146\n",
      "energy management                711\n",
      "evaluation                       578\n",
      "detection                        497\n",
      "simulation result                481\n",
      "load forecasting                 461\n",
      "dispatch strategy                263\n",
      "reliability assessment           250\n",
      "strategic planning               235\n",
      "optimization                     191\n",
      "fuzzy logic                      172\n",
      "sensitivity analysis             168\n",
      "performance evaluation           166\n",
      "energy storage optimization      134\n",
      "monte carlo simulation            53\n",
      "load flow analysis                41\n",
      "risk assessment                   36\n",
      "machine learning                  30\n",
      "control theory                    19\n",
      "Name: count, dtype: int64\n",
      "Top 3 methods for a sample document: <pandas.core.indexing._iLocIndexer object at 0x000001F8875E72F0> <pandas.core.indexing._iLocIndexer object at 0x000001F8875E4AA0>\n",
      "API token usage and cost: {'total_tokens': 749, 'total_cost': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "# --- Save and Summarize ---\n",
    "\n",
    "current_date = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "output_filename = os.path.join(SAVE_DIR, f\"semantic_scholar_{current_date}_final_lda_method.csv\")\n",
    "df_methods.to_csv(output_filename, sep=';', encoding='utf-8', quoting=csv.QUOTE_NONNUMERIC, escapechar='\\\\')\n",
    "print(f\"Results saved to {output_filename}\")\n",
    "\n",
    "print(\"LDA-based method label distribution:\")\n",
    "print(df_methods['Primary_Method_LDA'].value_counts())\n",
    "print(\"Top 3 methods for a sample document:\", df_methods['Top_3_Methods_LDA'].iloc, df_methods['Top_3_Methods_LDA_Scores'].iloc)\n",
    "print(\"API token usage and cost:\", credit_tracker.get_stats())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "literature-search-and-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
