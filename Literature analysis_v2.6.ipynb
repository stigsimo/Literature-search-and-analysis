{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8771c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "# Cell 1: Imports and Setup\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import configparser\n",
    "import tiktoken\n",
    "import logging\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Phrases\n",
    "import openai\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "SAVE_DIR = \"Saved_files_new\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f5fab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Cell 2: OpenAI Setup and Utility\n",
    "class CreditTracker:\n",
    "    def __init__(self):\n",
    "        self.total_tokens = 0\n",
    "        self.total_cost = 0\n",
    "        self.cost_per_1k_tokens = 0.00015\n",
    "    def update(self, tokens):\n",
    "        self.total_tokens += tokens\n",
    "        self.total_cost += (tokens / 1000) * self.cost_per_1k_tokens\n",
    "    def get_stats(self):\n",
    "        return {\"total_tokens\": self.total_tokens, \"total_cost\": round(self.total_cost, 4)}\n",
    "\n",
    "def initialize_openai():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('config_LLM.txt')\n",
    "    api_key = config['LLM'].get('OPENAI_API_KEY')\n",
    "    model_type = config['LLM'].get('MODEL_TYPE')\n",
    "    client = openai.OpenAI(api_key=api_key)\n",
    "    return client, model_type\n",
    "\n",
    "def num_tokens_from_string(string: str, model_name: str) -> int:\n",
    "    encoding = tiktoken.encoding_for_model(model_name)\n",
    "    return len(encoding.encode(string))\n",
    "\n",
    "client, model_type = initialize_openai()\n",
    "credit_tracker = CreditTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "278214e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Cell 3: Data Preprocessing Utilities\n",
    "\n",
    "def extract_keywords_from_filename(filename):\n",
    "    base = os.path.splitext(os.path.basename(filename))[0]\n",
    "    parts = base.split('_')\n",
    "    return [part for i, part in enumerate(parts) if i > 2 and part != 'results' and not part.isdigit()]\n",
    "\n",
    "def get_custom_stop_words(search_keywords=None):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words_to_keep = set()\n",
    "    if search_keywords:\n",
    "        for keyword in search_keywords:\n",
    "            keyword = keyword.lower()\n",
    "            words_to_keep.add(keyword)\n",
    "            for word in keyword.split():\n",
    "                words_to_keep.add(word)\n",
    "    stop_words = stop_words - words_to_keep\n",
    "    scientific_terms = {'et', 'al', 'ref', 'reference', 'references', 'cited', 'cite',\n",
    "        'fig', 'figure', 'figures', 'table', 'tables', 'chart', 'charts',\n",
    "        'published', 'journal', 'conference', 'proceedings', 'vol', 'volume', 'pp', 'page', 'pages', 'doi'}\n",
    "    return stop_words.union(scientific_terms)\n",
    "\n",
    "def preprocess_text(text, search_keywords=None, min_word_length=2, remove_numbers=True):\n",
    "    if not isinstance(text, (str, int, float)):\n",
    "        return ''\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    if remove_numbers:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s-]', '', text)\n",
    "    text = re.sub(r'--+', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = get_custom_stop_words(search_keywords)\n",
    "    tokens = [t for t in tokens if len(t) >= min_word_length and t not in stop_words and len(t) > 1 and not t.isdigit()]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    try:\n",
    "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    except:\n",
    "        pass\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess_dataframe(df, text_col, search_keywords, processed_col='processed_text'):\n",
    "    df[text_col] = df[text_col].fillna('').astype(str)\n",
    "    df[processed_col] = df[text_col].apply(lambda x: preprocess_text(x, search_keywords))\n",
    "    return df[df[processed_col].str.strip() != '']\n",
    "\n",
    "def clean_fields_of_study(s):\n",
    "    valid_fields = ['Computer Science', 'Economics', 'Engineering', 'Physics', 'Mathematics',\n",
    "        'Medicine','Business','Environmental Science','Chemistry','Materials Science',\n",
    "        'Geography','Biology','Geology','Political Science','Psychology','Com']\n",
    "    if pd.isna(s) or s == '[]':\n",
    "        return [\"Unknown\"]\n",
    "    if isinstance(s, str):\n",
    "        fields = [field.strip().strip(\"'\\\"\") for field in s.strip('[]').split(',')]\n",
    "        return [f if f in valid_fields else \"Unknown\" for f in fields] or [\"Unknown\"]\n",
    "    return [\"Unknown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c587ee3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m search_keywords \u001b[38;5;241m=\u001b[39m extract_keywords_from_filename(filename)\n\u001b[1;32m----> 9\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_keywords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_keywords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfieldsOfStudy\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfieldsOfStudy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(clean_fields_of_study)\n\u001b[0;32m     11\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded and preprocessed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m papers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[21], line 46\u001b[0m, in \u001b[0;36mpreprocess_dataframe\u001b[1;34m(df, text_col, search_keywords, processed_col)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpreprocess_dataframe\u001b[39m(df, text_col, search_keywords, processed_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     45\u001b[0m     df[text_col] \u001b[38;5;241m=\u001b[39m df[text_col]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m     df[processed_col] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext_col\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_keywords\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df[df[processed_col]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\git_repos\\Literature-search-and-analysis\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\git_repos\\Literature-search-and-analysis\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\git_repos\\Literature-search-and-analysis\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\git_repos\\Literature-search-and-analysis\\.venv\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\git_repos\\Literature-search-and-analysis\\.venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[21], line 46\u001b[0m, in \u001b[0;36mpreprocess_dataframe.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpreprocess_dataframe\u001b[39m(df, text_col, search_keywords, processed_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     45\u001b[0m     df[text_col] \u001b[38;5;241m=\u001b[39m df[text_col]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m     df[processed_col] \u001b[38;5;241m=\u001b[39m df[text_col]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_keywords\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df[df[processed_col]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[21], line 35\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text, search_keywords, min_word_length, remove_numbers)\u001b[0m\n\u001b[0;32m     33\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m     34\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text)\n\u001b[1;32m---> 35\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[43mget_custom_stop_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_keywords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(t) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_word_length \u001b[38;5;129;01mand\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(t) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39misdigit()]\n\u001b[0;32m     37\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n",
      "Cell \u001b[1;32mIn[21], line 10\u001b[0m, in \u001b[0;36mget_custom_stop_words\u001b[1;34m(search_keywords)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_custom_stop_words\u001b[39m(search_keywords\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 10\u001b[0m     stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43menglish\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     11\u001b[0m     words_to_keep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m search_keywords:\n",
      "File \u001b[1;32mc:\\git_repos\\Literature-search-and-analysis\\.venv\\Lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py:21\u001b[0m, in \u001b[0;36mWordListCorpusReader.words\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwords\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_lines_startswith\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     20\u001b[0m         line\n\u001b[1;32m---> 21\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m line_tokenize(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileids\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(ignore_lines_startswith)\n\u001b[0;32m     23\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\git_repos\\Literature-search-and-analysis\\.venv\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:218\u001b[0m, in \u001b[0;36mCorpusReader.raw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m    216\u001b[0m contents \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fileids:\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m    219\u001b[0m         contents\u001b[38;5;241m.\u001b[39mappend(fp\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concat(contents)\n",
      "File \u001b[1;32mc:\\git_repos\\Literature-search-and-analysis\\.venv\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:231\u001b[0m, in \u001b[0;36mCorpusReader.open\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03mReturn an open stream that can be used to read the given file.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03mIf the file's encoding is not None, then the stream will\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m:param file: The file identifier of the file to read.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    230\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding(file)\n\u001b[1;32m--> 231\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_root\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen(encoding)\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\n",
      "File \u001b[1;32mc:\\git_repos\\Literature-search-and-analysis\\.venv\\Lib\\site-packages\\nltk\\data.py:333\u001b[0m, in \u001b[0;36mFileSystemPathPointer.join\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mjoin\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileid):\n\u001b[0;32m    332\u001b[0m     _path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path, fileid)\n\u001b[1;32m--> 333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFileSystemPathPointer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\git_repos\\Literature-search-and-analysis\\.venv\\Lib\\site-packages\\nltk\\data.py:310\u001b[0m, in \u001b[0;36mFileSystemPathPointer.__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03mCreate a new path pointer for the given absolute path.\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m:raise IOError: If the given path does not exist.\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    309\u001b[0m _path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(_path)\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file or directory: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m _path)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m _path\n",
      "File \u001b[1;32m<frozen genericpath>:19\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Cell 4: Data Loading & Cleaning\n",
    "\n",
    "filename = \"semantic_scholar_2025_02_14_reliability_resilience_power_systems_results.csv\"\n",
    "filepath = os.path.join(\"Saved_files\", filename)\n",
    "df = pd.read_csv(filepath, sep=\";\")\n",
    "df['text'] = df['title'].fillna('') + ' ' + df['abstract'].fillna('')\n",
    "search_keywords = extract_keywords_from_filename(filename)\n",
    "df = preprocess_dataframe(df, text_col='text', search_keywords=search_keywords)\n",
    "df['fieldsOfStudy'] = df['fieldsOfStudy'].apply(clean_fields_of_study)\n",
    "logger.info(f\"Loaded and preprocessed {len(df)} papers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5725559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Cell 5: Method Phrase Extraction & Standardization (Iterative LLM)\n",
    "\n",
    "def extract_candidate_terms(df, text_col='processed_text', max_features=20000):\n",
    "    vectorizer = CountVectorizer(\n",
    "        ngram_range=(1, 4), max_df=0.95, min_df=2, max_features=max_features, token_pattern=r'\\b[\\w-]+\\b'\n",
    "    )\n",
    "    matrix = vectorizer.fit_transform(df[text_col].fillna(''))\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    freqs = matrix.sum(axis=0).A1\n",
    "    return [term for term, freq in sorted(zip(terms, freqs), key=lambda x: x[1], reverse=True)]\n",
    "\n",
    "def get_method_phrases_robust(\n",
    "    corpus_terms,\n",
    "    client,\n",
    "    model_type,\n",
    "    credit_tracker,\n",
    "    n_runs=3,\n",
    "    temp=0,\n",
    "    top_p=1.0,\n",
    "    show_progress=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Robustly extract method/technique phrases from candidate terms using an LLM.\n",
    "    The prompt is self-contained in the function for maximum reproducibility.\n",
    "    Handles common LLM output issues (markdown, formatting) safely.\n",
    "    Returns a sorted method list and their frequency counts across runs.\n",
    "    \"\"\"\n",
    "    import collections\n",
    "    import re\n",
    "    import ast\n",
    "\n",
    "    def parse_llm_python_list(output_text):\n",
    "        \"\"\"\n",
    "        Parse a Python list from any LLM string output, robust to markdown, stray text, or malformed formatting.\n",
    "        \"\"\"\n",
    "        content = re.sub(r\"```python\", \"\", output_text, flags=re.DOTALL)\t\n",
    "        list_match = re.search(r'$$.*?$$', content, re.DOTALL)\n",
    "        if list_match:\n",
    "            try:\n",
    "                return ast.literal_eval(list_match.group(0))\n",
    "            except Exception:\n",
    "                temp = list_match.group(0).strip(\"[]\")\n",
    "                return [item.strip().strip(\"'\\\"\") for item in temp.split(\",\") if item.strip()]\n",
    "        content = content.replace(\"\\n\", \" \").replace(\";\", \",\")\n",
    "        return [item.strip().strip(\"'\\\"\") for item in content.split(\",\") if item.strip()]\n",
    "\n",
    "    # Construct prompt inside function\n",
    "    sample_terms = ', '.join(corpus_terms[:100])\n",
    "     \n",
    "    \n",
    "\n",
    "    prompt = (\n",
    "        \"Here are the most frequent terms from a corpus of scientific papers:\\n\"\n",
    "        f\"{sample_terms}\\n\"\n",
    "        \"From the full list: \" + \", \".join(corpus_terms) + \"\\n\"\n",
    "        \"Extract ONLY the terms that represent specific methodologies, techniques, or named approaches. \"\n",
    "        \"Focus on computational, statistical, engineering, and reliability methods.\\n\"\n",
    "        \"DO include: e.g. 'monte carlo simulation', 'unit commitment', 'load flow analysis', 'genetic algorithm', \"\n",
    "        \"'neural network', 'stochastic optimization', 'reinforcement learning', 'fault tree analysis'.\\n\"\n",
    "        \"DO NOT include generic terms like 'framework', 'analysis', 'system', 'method', 'procedure', 'approach', 'application', 'performance', 'review', 'assesment', \"\n",
    "        \"by themselves or in combination with only other generic terms.\\n\"\n",
    "        \"Return as a single-line Python list; comma separated, no extra formatting.\"\n",
    "    )\n",
    "\n",
    "    all_phrases_sets = []\n",
    "    for i in range(n_runs):\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_type,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temp,\n",
    "            top_p=top_p\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        phrases = parse_llm_python_list(content)\n",
    "        phrases = [p.lower() for p in phrases if p.strip()]\n",
    "        all_phrases_sets.append(set(phrases))\n",
    "        credit_tracker.update(len(content))\n",
    "        if show_progress:\n",
    "            print(f\"Run {i+1}: found {len(phrases)} method phrases.\")\n",
    "    # Count occurrences, flatten all phrases\n",
    "    all_flat = [p for s in all_phrases_sets for p in s]\n",
    "    counts = collections.Counter(all_flat)\n",
    "    sorted_methods = sorted(counts, key=lambda x: (-counts[x], x))\n",
    "    print(f\"\\nTotal unique phrases: {len(counts)}. Most stable top 10: {sorted_methods[:10]}\")\n",
    "    return sorted_methods, counts\n",
    "\n",
    "\n",
    "def filter_methods_with_llm(\n",
    "    method_list, client, model_type, credit_tracker,\n",
    "    n_batched=25, verbose=True, fallback_to_input=True, min_methods=20, temp=0.0, top_p=1.0\n",
    "):\n",
    "    from tqdm import tqdm\n",
    "    filtered = []\n",
    "    all_rejected = False\n",
    "\n",
    "    for i in tqdm(range(0, len(method_list), n_batched)):\n",
    "        batch = method_list[i:i + n_batched]\n",
    "        \n",
    "        prompt = (\n",
    "            \"You are a scientific methods editor.\\n\"\n",
    "            \"Here is a list of candidate phrases:\\n\"\n",
    "            f\"{batch}\\n\\n\"\n",
    "            \"For each phrase, KEEP if it is:\"\n",
    "            \"\\n- a specifically named algorithm, computational/statistical technique\"\n",
    "            \"\\n- a well-known engineering analysis or optimization procedure that would be used in a scientific context\"\n",
    "            \"\\n- a specific mathematical/statistical model or method\"\n",
    "            \"\\nREMOVE if it is a general term that describes a generic group of methods (e.g. statistical methods) research area, system/process label, or property.\"\n",
    "            \"\\nIf unsure, KEEP the phrase.\"\n",
    "            \"\\nReturn ONLY a Python list (no markdown, code block, or explanation).\"\n",
    "        )\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model_type,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temp,\n",
    "                top_p=top_p\n",
    "            )\n",
    "            content = response.choices[0].message.content\n",
    "            batch_filtered = parse_llm_python_list(content)\n",
    "            if verbose and not batch_filtered:\n",
    "                print(f\"LLM WARNING: All phrases removed from batch (starting at {i}). Batch was: {batch}\")\n",
    "            filtered.extend([p for p in batch_filtered if isinstance(p, str) and p.strip()])\n",
    "            credit_tracker.update(len(content))\n",
    "        except Exception as ex:\n",
    "            print(f\"LLM FILTER ERROR (batch {i}): {ex}\\nBatch: {batch}\")\n",
    "            continue\n",
    "\n",
    "    if len(filtered) < min_methods:\n",
    "        all_rejected = True\n",
    "        if verbose:\n",
    "            print(f\"\\n[DEBUG] LLM filter returned only {len(filtered)} methods, falling back to unfiltered list of top {min_methods} from input.\")\n",
    "        if fallback_to_input:\n",
    "            filtered = list(method_list)[:min_methods]\n",
    "    if verbose:\n",
    "        print(f\"[DEBUG] Number of method phrases after LLM filtering: {len(filtered)}\")\n",
    "        print(f\"[DEBUG] Sample output: {filtered[:5]}\")\n",
    "\n",
    "    return filtered, all_rejected\n",
    "\n",
    "\n",
    "def get_method_abbreviation_dict(method_phrases, client, model_type, credit_tracker, batch_size=100):\n",
    "    import ast\n",
    "    results = {}\n",
    "    for i in range(0, len(method_phrases), batch_size):\n",
    "        batch = method_phrases[i:i+batch_size]\n",
    "        prompt = f\"\"\"For each of the following phrases, extract ALL common scientific abbreviations, synonyms, and aliases for methods/techniques.\n",
    "Methods:\\n{chr(10).join(batch)}\n",
    "Return as Python dict: {{'canonical method': [aliases, ...]}}\"\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_type,\n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are a scientific abbreviation expert.\"},\n",
    "                      {\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        start, end = content.find('{'), content.rfind('}')+1\n",
    "        method_dict = {}\n",
    "        if start >= 0 and end > start:\n",
    "            try:\n",
    "                method_dict = ast.literal_eval(content[start:end])\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to parse dictionary from LLM batch: {e}\")\n",
    "        results.update(method_dict)\n",
    "    logger.info(f\"LLM mapped {len(results)} methods to abbreviations/variants.\")\n",
    "    return results\n",
    "\n",
    "def build_abbr_to_canonical_map(method_dict):\n",
    "    abbr_map = {}\n",
    "    for canonical, variants in method_dict.items():\n",
    "        abbr_map[canonical.lower()] = canonical\n",
    "        for v in variants:\n",
    "            abbr_map[v.lower()] = canonical\n",
    "    return abbr_map\n",
    "\n",
    "import re\n",
    "import ast\n",
    "\n",
    "def parse_llm_python_list(output_text):\n",
    "    \"\"\"\n",
    "    Parse a Python list from any LLM string output, robust to markdown, stray text, or malformed formatting.\n",
    "    \"\"\"\n",
    "    # Remove common start/end code block markers and markdown fences\n",
    "    content = re.sub(r\"```python\", \"\", output_text, flags=re.DOTALL)\n",
    "    # Find the first list in the string\n",
    "    list_match = re.search(r'$$.*?$$', content, re.DOTALL)\n",
    "    if list_match:\n",
    "        try:\n",
    "            return ast.literal_eval(list_match.group(0))\n",
    "        except Exception:\n",
    "            # fallback: comma-split and strip (will overparse, but better than nothing)\n",
    "            temp = list_match.group(0).strip(\"[]\")\n",
    "            return [item.strip().strip(\"'\\\"\") for item in temp.split(\",\") if item.strip()]\n",
    "    # Fallback: comma-split the whole thing\n",
    "    content = content.replace(\"\\n\", \" \").replace(\";\", \",\")\n",
    "    return [item.strip().strip(\"'\\\"\") for item in content.split(\",\") if item.strip()]\n",
    "\n",
    "\n",
    "def standardize_methods_in_text(text, abbr_to_canonical):\n",
    "    import re\n",
    "    sorted_vars = sorted(abbr_to_canonical, key=lambda x: -len(x))\n",
    "    for var in sorted_vars:\n",
    "        pattern = r'\\b' + re.escape(var) + r'\\b'\n",
    "        text = re.sub(pattern, abbr_to_canonical[var], text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3b73a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Cell 6: Method Scoring Functions\n",
    "\n",
    "def compute_tfidf_scores(processed_texts, method_phrases, ngram_range=(1, 4), min_df=1, max_df=0.95, norm='l2'):\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        vocabulary=method_phrases, ngram_range=ngram_range,\n",
    "        min_df=min_df, max_df=max_df, norm=norm\n",
    "    )\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(processed_texts)\n",
    "    scores = tfidf_matrix.toarray()\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    return scores, feature_names\n",
    "\n",
    "def compute_lda_scores(processed_texts, method_phrases, ngram_range=(1, 3), n_topics=100, max_iter=20):\n",
    "    vectorizer = CountVectorizer(\n",
    "        vocabulary=method_phrases, ngram_range=ngram_range, token_pattern=r'\\b[\\w-]+\\b'\n",
    "    )\n",
    "    doc_term_matrix = vectorizer.fit_transform(processed_texts)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    if n_topics >= 2:\n",
    "        lda = LatentDirichletAllocation(n_components=n_topics, learning_method='batch',\n",
    "                                       random_state=42, max_iter=max_iter)\n",
    "        lda_matrix = lda.fit_transform(doc_term_matrix)\n",
    "    else:\n",
    "        lda_matrix = np.zeros((doc_term_matrix.shape[0], len(method_phrases)))\n",
    "    return lda_matrix, feature_names\n",
    "\n",
    "def compute_compound_scores(df, method_phrases, processed_col='standardized_text', window=150, min_word_len=4):\n",
    "    n_docs = len(df)\n",
    "    n_methods = len(method_phrases)\n",
    "    scores = np.zeros((n_docs, n_methods), dtype=np.float32)\n",
    "    docs = df[processed_col].fillna('').str.lower().tolist()\n",
    "    for j, phrase in enumerate(method_phrases):\n",
    "        phrase_l = phrase.lower()\n",
    "        words = [w for w in phrase_l.split() if len(w) >= min_word_len]\n",
    "        for i, text in enumerate(docs):\n",
    "            # Only count if full phrase or all tokens are present\n",
    "            if phrase_l in text:\n",
    "                scores[i, j] = 1.0\n",
    "            elif len(words) > 1 and all(w in text for w in words):\n",
    "                scores[i, j] = 0.8  # partial but very strong\n",
    "            else:\n",
    "                scores[i, j] = 0.0\n",
    "    return scores\n",
    "\n",
    "\n",
    "def combine_method_scores(tfidf_scores, lda_scores, compound_scores, weights=(0.4, 0.3, 0.3)):\n",
    "    return weights[0]*tfidf_scores + weights[1]*lda_scores + weights[2]*compound_scores\n",
    "\n",
    "def assign_top_methods_by_total_score(\n",
    "    df, total_scores, method_names,\n",
    "    tfidf_scores, lda_scores, compound_scores,\n",
    "    top_n=3, min_score=0.03\n",
    "):\n",
    "    \"\"\"\n",
    "    Assigns top_n methods based on total_scores, with confidence defined by individual method scores.\n",
    "    - 'super_confident': all method scores above min_score and at least 2 of 3 methods agree on assignment at this rank.\n",
    "    - 'confident': all method scores above min_score for assignment at this rank.\n",
    "    - 'low_confidence': assigned even if some scores are below threshold.\n",
    "    - '' (empty): No method assigned if all scores are equal for all methods in a row.\n",
    "    \"\"\"\n",
    "    for rank in range(top_n):\n",
    "        top_method = []\n",
    "        top_score = []\n",
    "        top_confidence = []\n",
    "\n",
    "        for i, row in enumerate(total_scores):\n",
    "            # If all methods are exactly equal, assign nothing.\n",
    "            if np.allclose(row, row[0]):\n",
    "                top_method.append(\"\")\n",
    "                top_score.append(0.0)\n",
    "                top_confidence.append(\"\")\n",
    "                continue\n",
    "\n",
    "            idxs = np.argsort(row)[::-1]\n",
    "            nth_idx = idxs[rank] if rank < len(idxs) else None\n",
    "\n",
    "            if nth_idx is None:\n",
    "                top_method.append(\"\")\n",
    "                top_score.append(0.0)\n",
    "                top_confidence.append(\"\")\n",
    "                continue\n",
    "\n",
    "            assigned_method = method_names[nth_idx]\n",
    "            score = row[nth_idx]\n",
    "\n",
    "            tfidf = tfidf_scores[i][nth_idx]\n",
    "            lda = lda_scores[i][nth_idx]\n",
    "            comp = compound_scores[i][nth_idx]\n",
    "\n",
    "            above_thresh = [(tfidf >= min_score), (lda >= min_score), (comp >= min_score)]\n",
    "            agree2 = (\n",
    "                (assigned_method == method_names[np.argmax(tfidf_scores[i])]) +\n",
    "                (assigned_method == method_names[np.argmax(lda_scores[i])]) +\n",
    "                (assigned_method == method_names[np.argmax(compound_scores[i])])\n",
    "            )\n",
    "\n",
    "            if all(above_thresh):\n",
    "                if agree2 >= 2:\n",
    "                    confidence = \"super_confident\"\n",
    "                else:\n",
    "                    confidence = \"confident\"\n",
    "            else:\n",
    "                confidence = \"low_confidence\"\n",
    "\n",
    "            # If all three individual method scores are zero (row is all zeros), treat as no assignment\n",
    "            if tfidf == lda == comp == 0:\n",
    "                top_method.append(\"\")\n",
    "                top_score.append(0.0)\n",
    "                top_confidence.append(\"\")\n",
    "            else:\n",
    "                top_method.append(assigned_method)\n",
    "                top_score.append(score)\n",
    "                top_confidence.append(confidence)\n",
    "\n",
    "        df[f'Top_{rank+1}_Method'] = top_method\n",
    "        df[f'Top_{rank+1}_Score'] = top_score\n",
    "        df[f'Top_{rank+1}_Confidence'] = top_confidence\n",
    "\n",
    "    # Set primary assignment columns\n",
    "    df['Primary_Method'] = df['Top_1_Method']\n",
    "    df['Primary_Method_Score'] = df['Top_1_Score']\n",
    "    df['Method_Confidence'] = df['Top_1_Confidence']\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553a576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Cell 7: Topic Modeling + Naming (Iterative/Consensus LLM) + Author Functions\n",
    "\n",
    "def run_lda_topic_modeling(df, num_topics=10, num_words=25):\n",
    "    tokenized_texts = df['processed_text'].apply(lambda x: x.split()).tolist()\n",
    "    bigram = Phrases(tokenized_texts, min_count=10, threshold=50, delimiter='_')\n",
    "    trigram = Phrases(bigram[tokenized_texts], threshold=50, delimiter='_')\n",
    "    phrased = []\n",
    "    for doc in tokenized_texts:\n",
    "        bigrams_ = [w for w in bigram[doc] if '_' in w]\n",
    "        trigrams_ = [w for w in trigram[bigram[doc]] if '_' in w]\n",
    "        combined = doc + bigrams_ + trigrams_\n",
    "        phrased.append(' '.join(combined))\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 1), token_pattern=r'\\b[\\w_-]+\\b', max_df=0.95, min_df=2, max_features=10000)\n",
    "    doc_term_matrix = vectorizer.fit_transform(phrased)\n",
    "    lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "    topic_distributions = lda_model.fit_transform(doc_term_matrix)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topic_keywords = {}\n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        top_indices = topic.argsort()[:-num_words-1:-1]\n",
    "        top_words = [feature_names[i] for i in top_indices]\n",
    "        topic_keywords[topic_idx] = {'top_words': top_words}\n",
    "    return lda_model, vectorizer, topic_distributions, topic_keywords\n",
    "\n",
    "def assign_papers_to_topics(topic_distributions):\n",
    "    paper_classifications = []\n",
    "    for idx, dist in enumerate(topic_distributions):\n",
    "        top_2_topics = np.argsort(dist)[-2:][::-1]\n",
    "        primary_score = dist[top_2_topics[0]]\n",
    "        other_topics_sum = sum(dist) - primary_score\n",
    "        dominance_ratio = primary_score / (other_topics_sum + 1e-10)\n",
    "        paper_classifications.append({\n",
    "            'paper_idx': idx,\n",
    "            'primary_topic': top_2_topics[0],\n",
    "            'secondary_topic': top_2_topics[1],\n",
    "            'primary_score': primary_score,\n",
    "            'dominance_ratio': dominance_ratio\n",
    "        })\n",
    "    return paper_classifications\n",
    "\n",
    "def string_similarity(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def topic_name_llm_robust(\n",
    "    lda_keywords, tfidf_ngrams, top_titles,\n",
    "    client, model_type, credit_tracker,\n",
    "    initial_iterations=3, max_iterations=10, similarity_threshold=0.7,\n",
    "    temp=0, top_p=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Iteratively calls LLM for topic naming, detects dominant result by string similarity.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"Based on the following keywords and n-grams from LDA and TF-IDF, plus top paper titles, provide a concise topic name \"\n",
    "        \"(bigram or trigram, single word if very specific):\\n\"\n",
    "        f\"LDA: {', '.join(lda_keywords)}\\n\"\n",
    "        f\"TFIDF: {', '.join(tfidf_ngrams)}\\n\"\n",
    "        f\"TITLES: {', '.join(top_titles)}\\n\"\n",
    "        \"Return ONLY the topic name.\"\n",
    "    )\n",
    "    iterations = initial_iterations\n",
    "    from collections import Counter\n",
    "    while iterations <= max_iterations:\n",
    "        generated_names = []\n",
    "        for _ in range(iterations):\n",
    "            response = client.chat.completions.create(\n",
    "                model=model_type,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a science topic-naming assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=temp,\n",
    "                top_p=top_p\n",
    "            )\n",
    "            content = response.choices[0].message.content.strip()\n",
    "            if content:\n",
    "                generated_names.append(content)\n",
    "        # Majority/consensus with string similarity threshold\n",
    "        for i, name in enumerate(generated_names):\n",
    "            matches = [other for j, other in enumerate(generated_names)\n",
    "                       if i != j and string_similarity(name, other) >= similarity_threshold]\n",
    "            if len(matches) >= len(generated_names)//2:  # Majority found\n",
    "                print(f\"Topic name stabilized after {iterations} iterations: {name}\")\n",
    "                return name\n",
    "        iterations += 2\n",
    "        print(f\"No majority topic name found, increasing iterations to {iterations}.\")\n",
    "    most_common = Counter(generated_names).most_common(1)[0]\n",
    "    print(f\"Returning most common topic name after {max_iterations} iterations: {most_common}\")\n",
    "    return most_common\n",
    "\n",
    "def get_top_titles_for_topic(df, paper_classifications, topic_idx, n_titles=10):\n",
    "    dominant_papers = [p for p in paper_classifications if p['primary_topic'] == topic_idx]\n",
    "    paper_infos = [\n",
    "        (df.iloc[p['paper_idx']]['citationCount'] if 'citationCount' in df.columns else 0, df.iloc[p['paper_idx']]['title'])\n",
    "        for p in dominant_papers if not pd.isna(df.iloc[p['paper_idx']]['title'])\n",
    "    ]\n",
    "    top_titles = [title for _, title in sorted(paper_infos, key=lambda x: -x[0])[:n_titles]]\n",
    "    return top_titles\n",
    "def get_top_tfidf_ngrams_per_topic(df, tfidf_matrix, feature_names, topic_col='Primary_Topic_Index', top_k=10):\n",
    "    tfidf_ngrams = {}\n",
    "    for topic_idx in df[topic_col].dropna().unique():\n",
    "        topic_idx = int(topic_idx)\n",
    "        doc_indices = df[df[topic_col] == topic_idx].index\n",
    "        if len(doc_indices) == 0:\n",
    "            continue\n",
    "        topic_tfidf = np.asarray(tfidf_matrix[doc_indices].mean(axis=0)).ravel()\n",
    "        top_indices = topic_tfidf.argsort()[-top_k:][::-1]\n",
    "        top_terms = [(feature_names[i], topic_tfidf[i]) for i in top_indices if topic_tfidf[i] > 0]\n",
    "        tfidf_ngrams[topic_idx] = top_terms\n",
    "    return tfidf_ngrams\n",
    "def get_author_stats(paper_classifications, df_field, n_top=5):\n",
    "    top_papers = {}\n",
    "    author_topic_stats = {}\n",
    "    for topic in set(p['primary_topic'] for p in paper_classifications):\n",
    "        topic_papers = [p for p in paper_classifications if p['primary_topic'] == topic]\n",
    "        topic_papers.sort(key=lambda x: x['dominance_ratio'], reverse=True)\n",
    "        top_papers[topic] = []\n",
    "        for p in topic_papers[:n_top]:\n",
    "            paper_idx = p['paper_idx']\n",
    "            try:\n",
    "                authors = df_field.iloc[paper_idx]['authors']\n",
    "                if isinstance(authors, str):\n",
    "                    try: authors = ast.literal_eval(authors)\n",
    "                    except (ValueError, SyntaxError): authors = []\n",
    "                if isinstance(authors, list):\n",
    "                    author_list = []\n",
    "                    for author in authors:\n",
    "                        if isinstance(author, dict):\n",
    "                            author_list.append({'name': author.get('name', 'Unknown'), 'id': author.get('authorId', 'Unknown')})\n",
    "                else: author_list = []\n",
    "                top_papers[topic].append({\n",
    "                    'paperId': df_field.iloc[paper_idx].get('paperId',''),\n",
    "                    'title': df_field.iloc[paper_idx].get('title',''),\n",
    "                    'authors': author_list,\n",
    "                    'score': float(p['primary_score']),\n",
    "                    'dominance_ratio': float(p['dominance_ratio'])\n",
    "                })\n",
    "            except Exception as e: continue\n",
    "    return top_papers, author_topic_stats\n",
    "# %%\n",
    "# Cell X: Save term frequency (keywords, bigrams, trigrams) for visualization\n",
    "\n",
    "def save_term_frequencies(df, suffix_string, save_dir=SAVE_DIR, max_keywords=5000):\n",
    "    \"\"\"Save .json containing keywords, bigrams, trigrams with their counts for later visualization.\"\"\"\n",
    "    freq_data = {}\n",
    "    processed_text = df['processed_text'].fillna('').astype(str)\n",
    "    for n in range(1, 4):\n",
    "        vectorizer = CountVectorizer(ngram_range=(n, n), stop_words='english', max_features=max_keywords)\n",
    "        matrix = vectorizer.fit_transform(processed_text)\n",
    "        terms = vectorizer.get_feature_names_out()\n",
    "        freqs = matrix.sum(axis=0).A1\n",
    "        freq_dict = {term: int(freq) for term, freq in sorted(zip(terms, freqs), key=lambda x: -x[1])}\n",
    "        if n == 1: freq_data['keywords'] = freq_dict\n",
    "        elif n == 2: freq_data['bigrams'] = freq_dict\n",
    "        elif n == 3: freq_data['trigrams'] = freq_dict\n",
    "    out_fn = os.path.join(save_dir, f'term_frequencies_{suffix_string}.json')\n",
    "    with open(out_fn, 'w', encoding='utf-8') as f:\n",
    "        json.dump(freq_data, f, indent=2)\n",
    "    print(f\"✓ Saved term frequency summary to {out_fn}\")\n",
    "    return out_fn\n",
    "\n",
    "# %%\n",
    "# Cell Y: Save author and venue frequencies for barplot visualization\n",
    "\n",
    "def save_author_and_venue_frequencies(df, suffix_string, save_dir=SAVE_DIR):\n",
    "    \"\"\"Save author and venue frequencies for visualization.\"\"\"\n",
    "    # Authors\n",
    "    if 'authors' in df.columns:\n",
    "        authors_all = []\n",
    "        for item in df['authors']:\n",
    "            # Handle dict, list, or string\n",
    "            if isinstance(item, str) and item.strip():\n",
    "                try:\n",
    "                    obj = eval(item) if (item.strip().startswith(\"[\") or item.strip().startswith(\"{\")) else item.strip()\n",
    "                except Exception:\n",
    "                    obj = item.strip()\n",
    "            else:\n",
    "                obj = item\n",
    "            if isinstance(obj, list):\n",
    "                for author in obj:\n",
    "                    if isinstance(author, dict) and 'name' in author:\n",
    "                        authors_all.append(author['name'])\n",
    "                    elif isinstance(author, str):\n",
    "                        authors_all.append(author)\n",
    "            elif isinstance(obj, dict) and 'name' in obj:\n",
    "                authors_all.append(obj['name'])\n",
    "            elif isinstance(obj, str):\n",
    "                authors_all.append(obj)\n",
    "        author_counts = pd.Series(authors_all).value_counts().reset_index()\n",
    "        author_counts.columns = ['Author', 'Frequency']\n",
    "        author_fn = os.path.join(save_dir, f\"semantic_scholar_{suffix_string}_author_analysis.csv\")\n",
    "        author_counts.to_csv(author_fn, sep=';', encoding='utf-8', index=False)\n",
    "        print(f\"✓ Saved author frequencies: {author_fn}\")\n",
    "    else:\n",
    "        print(\"No 'authors' column found in DF: skipping author frequencies.\")\n",
    "    # Venues\n",
    "    if 'venue' in df.columns:\n",
    "        venue_counts = df['venue'].value_counts().reset_index()\n",
    "        venue_counts.columns = ['Venue', 'Frequency']\n",
    "        venue_fn = os.path.join(save_dir, f\"semantic_scholar_{suffix_string}_venue_frequencies.csv\")\n",
    "        venue_counts.to_csv(venue_fn, sep=';', encoding='utf-8', index=False)\n",
    "        print(f\"✓ Saved venue frequencies: {venue_fn}\")\n",
    "    else:\n",
    "        print(\"No 'venue' column found in DF: skipping venue frequencies.\")\n",
    "\n",
    "\n",
    "\n",
    "def save_topic_analysis_outputs(\n",
    "    df, lda_model, lda_vectorizer, topic_distributions, topic_keywords, topic_names, topic_ngrams,\n",
    "    author_stats, top_papers, tfidf_ngrams, suffix_string\n",
    "):\n",
    "    topic_metadata = {\n",
    "        \"topics\": {int(k): v for k,v in topic_keywords.items()},\n",
    "        \"topic_names\": {int(k): v for k,v in topic_names.items()},\n",
    "        \"topic_ngrams\": {int(k): v for k,v in topic_ngrams.items()},\n",
    "    }\n",
    "    with open(os.path.join(SAVE_DIR, f\"topics_{suffix_string}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(topic_metadata, f, indent=2)\n",
    "    with open(os.path.join(SAVE_DIR, f\"topic_names_{suffix_string}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({int(k):v for k,v in topic_names.items()}, f, indent=2)\n",
    "    np.save(os.path.join(SAVE_DIR, f\"topic_distributions_{suffix_string}.npy\"), topic_distributions)\n",
    "    import joblib\n",
    "    joblib.dump(lda_model, os.path.join(SAVE_DIR, f\"lda_model_{suffix_string}.joblib\"))\n",
    "    joblib.dump(lda_vectorizer, os.path.join(SAVE_DIR, f\"lda_vectorizer_{suffix_string}.joblib\"))\n",
    "    with open(os.path.join(SAVE_DIR, f\"top_papers_{suffix_string}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({int(k): v for k, v in top_papers.items()}, f, ensure_ascii=False, indent=2, default=str)\n",
    "    pd.DataFrame.from_dict(author_stats, orient='index').to_csv(\n",
    "        os.path.join(SAVE_DIR, f\"author_stats_{suffix_string}.csv\"))\n",
    "    with open(os.path.join(SAVE_DIR, f\"topic_specific_tfidf_ngrams_{suffix_string}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({int(k):[(term,float(score)) for term,score in v] for k,v in topic_ngrams.items()}, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5a1618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Cell 8: Diagnostics Function\n",
    "\n",
    "def diagnostics_with_scores(\n",
    "    df, tfidf_scores, lda_scores, compound_scores, combined_scores, method_names\n",
    "):\n",
    "    n_docs, n_methods = tfidf_scores.shape\n",
    "    print(\"=== DIAGNOSTICS ===\")\n",
    "    print(f\"Total documents: {n_docs}\")\n",
    "    print(f\"Methods: {n_methods}\")\n",
    "    print(f\"TF-IDF coverage: {(tfidf_scores > 0).any(axis=1).sum()}/{n_docs} ({100*(tfidf_scores>0).any(axis=1).mean():.1f}%)\")\n",
    "    print(f\"LDA coverage: {(lda_scores > 0).any(axis=1).sum()}/{n_docs} ({100*(lda_scores>0).any(axis=1).mean():.1f}%)\")\n",
    "    print(f\"Compound coverage: {(compound_scores > 0).any(axis=1).sum()}/{n_docs} ({100*(compound_scores>0).any(axis=1).mean():.1f}%)\")\n",
    "    print(f\"Combined coverage: {(combined_scores > 0).any(axis=1).sum()}/{n_docs} ({100*(combined_scores>0).any(axis=1).mean():.1f}%)\")\n",
    "    if 'Primary_Method' in df.columns:\n",
    "        print(\"\\nMethod label distribution (top 10):\")\n",
    "        print(df['Primary_Method'].value_counts().head(10))\n",
    "    if 'Method_Confidence' in df.columns:\n",
    "        print(\"\\nMethod confidence distribution:\")\n",
    "        print(df['Method_Confidence'].value_counts())\n",
    "    print(\"\\nMethod vocabulary sample:\", ', '.join(method_names[:10]))\n",
    "    print(f\"\\nTFIDF stats: mean={tfidf_scores.mean():.3f}, std={tfidf_scores.std():.3f}\")\n",
    "    print(f\"LDA stats: mean={lda_scores.mean():.3f}, std={lda_scores.std():.3f}\")\n",
    "    print(f\"Compound stats: mean={compound_scores.mean():.3f}, std={compound_scores.std():.3f}\")\n",
    "    print(f\"Combined stats: mean={combined_scores.mean():.3f}, std={combined_scores.std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22740085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ee4d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved term frequency summary to Saved_files_new\\term_frequencies_2025_08_23reliability_resilience_power_systems.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 16:41:14,937 - INFO - Starting topic modeling workflow...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved author frequencies: Saved_files_new\\semantic_scholar_2025_08_23reliability_resilience_power_systems_author_analysis.csv\n",
      "✓ Saved venue frequencies: Saved_files_new\\semantic_scholar_2025_08_23reliability_resilience_power_systems_venue_frequencies.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 16:41:15,212 - INFO - collecting all words and their counts\n",
      "2025-08-23 16:41:15,212 - INFO - PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2025-08-23 16:41:16,694 - INFO - PROGRESS: at sentence #10000, processed 1524957 words and 863761 word types\n",
      "2025-08-23 16:41:18,154 - INFO - PROGRESS: at sentence #20000, processed 3004878 words and 1467564 word types\n",
      "2025-08-23 16:41:19,369 - INFO - collected 1902495 token types (unigram + bigrams) from a corpus of 4290297 words and 28934 sentences\n",
      "2025-08-23 16:41:19,370 - INFO - merged Phrases<1902495 vocab, min_count=10, threshold=50, max_vocab_size=40000000>\n",
      "2025-08-23 16:41:19,372 - INFO - Phrases lifecycle event {'msg': 'built Phrases<1902495 vocab, min_count=10, threshold=50, max_vocab_size=40000000> in 4.16s', 'datetime': '2025-08-23T16:41:19.371104', 'gensim': '4.3.2', 'python': '3.11.13 (main, Jun 12 2025, 12:41:34) [MSC v.1943 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'created'}\n",
      "2025-08-23 16:41:19,373 - INFO - collecting all words and their counts\n",
      "2025-08-23 16:41:19,376 - INFO - PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2025-08-23 16:41:22,711 - INFO - PROGRESS: at sentence #10000, processed 1471162 words and 886074 word types\n",
      "2025-08-23 16:41:26,037 - INFO - PROGRESS: at sentence #20000, processed 2890563 words and 1519087 word types\n",
      "2025-08-23 16:41:28,989 - INFO - collected 1981629 token types (unigram + bigrams) from a corpus of 4117577 words and 28934 sentences\n",
      "2025-08-23 16:41:28,990 - INFO - merged Phrases<1981629 vocab, min_count=5, threshold=50, max_vocab_size=40000000>\n",
      "2025-08-23 16:41:28,990 - INFO - Phrases lifecycle event {'msg': 'built Phrases<1981629 vocab, min_count=5, threshold=50, max_vocab_size=40000000> in 9.62s', 'datetime': '2025-08-23T16:41:28.990748', 'gensim': '4.3.2', 'python': '3.11.13 (main, Jun 12 2025, 12:41:34) [MSC v.1943 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'created'}\n",
      "2025-08-23 16:43:32,191 - INFO - ✓ LDA topic modeling completed.\n",
      "2025-08-23 16:43:32,192 - INFO - Identified 12 topics with 15 top words each.\n",
      "2025-08-23 16:43:32,192 - INFO - Assigning papers to topics based on LDA distributions...\n",
      "2025-08-23 16:43:32,316 - INFO - ✓ Papers assigned to topics based on LDA distributions.\n",
      "2025-08-23 16:43:32,317 - INFO - Total papers assigned to topics: 12\n",
      "2025-08-23 16:43:32,318 - INFO - Startting Topic n-gram extraction and naming...\n",
      "2025-08-23 16:43:46,622 - INFO - ✓ Extracted topic-specific TF-IDF n-grams for naming.\n",
      "2025-08-23 16:43:46,623 - INFO - Starting iterative LLM topic naming...\n",
      "2025-08-23 16:43:47,375 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Unsupported value: 'temperature' does not support 0 with this model. Only the default (1) value is supported.\", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 54\u001b[0m\n\u001b[0;32m     52\u001b[0m tfidf_ng \u001b[38;5;241m=\u001b[39m [ngram \u001b[38;5;28;01mfor\u001b[39;00m ngram, _ \u001b[38;5;129;01min\u001b[39;00m topic_ngrams\u001b[38;5;241m.\u001b[39mget(topic_idx, [])][:NUM_TOPIC_WORDS]\n\u001b[0;32m     53\u001b[0m top_titles \u001b[38;5;241m=\u001b[39m get_top_titles_for_topic(df, paper_classifications, topic_idx, n_titles\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m---> 54\u001b[0m topic_name \u001b[38;5;241m=\u001b[39m \u001b[43mtopic_name_llm_robust\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlda_ngrams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtfidf_ng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_titles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredit_tracker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTOPIC_LLM_ITER_INIT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTOPIC_LLM_ITER_MAX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43msimilarity_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTOPIC_LLM_SIM_THRESH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTOPIC_LLM_TEMP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTOPIC_LLM_TOP_P\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m topic_names[topic_idx] \u001b[38;5;241m=\u001b[39m topic_name\n\u001b[0;32m     63\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopic \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic_name\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mtopic_name\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnnamed\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 67\u001b[0m, in \u001b[0;36mtopic_name_llm_robust\u001b[1;34m(lda_keywords, tfidf_ngrams, top_titles, client, model_type, credit_tracker, initial_iterations, max_iterations, similarity_threshold, temp, top_p)\u001b[0m\n\u001b[0;32m     65\u001b[0m generated_names \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[1;32m---> 67\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a science topic-naming assistant.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     content \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content:\n",
      "File \u001b[1;32mc:\\git_repos\\Literature-search-and-analysis\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\git_repos\\Literature-search-and-analysis\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:879\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    839\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    876\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    877\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    878\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    906\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    907\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    908\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    909\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    910\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    911\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    912\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    913\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    914\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\git_repos\\Literature-search-and-analysis\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1290\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1278\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1285\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1286\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1287\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1288\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1289\u001b[0m     )\n\u001b[1;32m-> 1290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\git_repos\\Literature-search-and-analysis\\.venv\\Lib\\site-packages\\openai\\_base_client.py:967\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    965\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\git_repos\\Literature-search-and-analysis\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1071\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1068\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1070\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1071\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1074\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1075\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1079\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1080\u001b[0m )\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Unsupported value: 'temperature' does not support 0 with this model. Only the default (1) value is supported.\", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}"
     ]
    }
   ],
   "source": [
    "# Cell 9: Topic Analysis Workflow (with robust LLM topic naming)\n",
    "\n",
    "# -- Parameters for topic workflow --\n",
    "NUM_TOPICS = 12\n",
    "NUM_TOPIC_WORDS = 15\n",
    "TOPIC_LLM_ITER_INIT = 3      # Initial LLM naming iterations for topic consensus\n",
    "TOPIC_LLM_ITER_MAX = 9       # Max LLM naming iterations\n",
    "TOPIC_LLM_SIM_THRESH = 0.72  # Majority string similarity threshold\n",
    "TOPIC_LLM_TEMP = 0           # LLM temperature for topic naming, higher = more creative, lower = more deterministic, min= 0, max=1.0\n",
    "TOPIC_LLM_TOP_P = 1.0        # LLM top_p for topic naming, Indicates what portion of the probability mass to consider, 1.0 = all, 0.9 = top 90%, etc.\n",
    "\n",
    "\n",
    "current_date = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "keyword_str = '_'.join(extract_keywords_from_filename(filename)) if 'filename' in locals() else \"\"\n",
    "suffix_string = f\"{current_date}{keyword_str}\"\n",
    "save_term_frequencies(df, suffix_string)\n",
    "# Usage example after suffix_string was defined as above:\n",
    "save_author_and_venue_frequencies(df, suffix_string)\n",
    "\n",
    "\n",
    "logger.info(\"Starting topic modeling workflow...\")  \n",
    "lda_model, lda_vectorizer, topic_distributions, topic_keywords = run_lda_topic_modeling(\n",
    "    df, num_topics=NUM_TOPICS, num_words=NUM_TOPIC_WORDS)\n",
    "logger.info(\"✓ LDA topic modeling completed.\")\n",
    "logger.info(f\"Identified {len(topic_keywords)} topics with {NUM_TOPIC_WORDS} top words each.\")\n",
    "logger.info(\"Assigning papers to topics based on LDA distributions...\")\n",
    "paper_classifications = assign_papers_to_topics(topic_distributions)\n",
    "df['Primary_Topic_Index'] = [int(p['primary_topic']) for p in paper_classifications]\n",
    "df['Primary_Score'] = [p['primary_score'] for p in paper_classifications]\n",
    "df['Dominance_Ratio'] = [p['dominance_ratio'] for p in paper_classifications]\n",
    "\n",
    "logger.info(\"✓ Papers assigned to topics based on LDA distributions.\")\n",
    "logger.info(f\"Total papers assigned to topics: {df['Primary_Topic_Index'].nunique()}\")\n",
    "\n",
    "logger.info(\"Startting Topic n-gram extraction and naming...\")\n",
    "# Per-topic TF-IDF n-grams for naming/interpretation\n",
    "topic_tfidf_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 3), min_df=2, max_df=0.95, token_pattern=r'\\b[\\w_-]+\\b'\n",
    ")\n",
    "topic_tfidf_matrix = topic_tfidf_vectorizer.fit_transform(df['processed_text'])\n",
    "topic_tfidf_feature_names = topic_tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "topic_ngrams = get_top_tfidf_ngrams_per_topic(\n",
    "    df, topic_tfidf_matrix, topic_tfidf_feature_names, topic_col='Primary_Topic_Index', top_k=10)\n",
    "\n",
    "logger.info(\"✓ Extracted topic-specific TF-IDF n-grams for naming.\")\n",
    "logger.info(\"Starting iterative LLM topic naming...\")\n",
    "# Iterative LLM topic naming\n",
    "topic_names = {}\n",
    "for topic_idx, keywords in topic_keywords.items():\n",
    "    lda_ngrams = keywords['top_words'][:NUM_TOPIC_WORDS]\n",
    "    tfidf_ng = [ngram for ngram, _ in topic_ngrams.get(topic_idx, [])][:NUM_TOPIC_WORDS]\n",
    "    top_titles = get_top_titles_for_topic(df, paper_classifications, topic_idx, n_titles=10)\n",
    "    topic_name = topic_name_llm_robust(\n",
    "        lda_ngrams, tfidf_ng, top_titles,\n",
    "        client, model_type, credit_tracker,\n",
    "        initial_iterations=TOPIC_LLM_ITER_INIT,\n",
    "        max_iterations=TOPIC_LLM_ITER_MAX,\n",
    "        similarity_threshold=TOPIC_LLM_SIM_THRESH,\n",
    "        temp=TOPIC_LLM_TEMP, top_p=TOPIC_LLM_TOP_P\n",
    "    )\n",
    "    topic_names[topic_idx] = topic_name\n",
    "    logger.info(f\"Topic {topic_idx}: {topic_name if topic_name else 'Unnamed'}\")\n",
    "df['Primary_Topic'] = df['Primary_Topic_Index'].map(lambda x: topic_names.get(x, f\"Topic_{x}\"))\n",
    "logger.info(\"✓ Topic naming and assignment completed.\")\n",
    "logger.info(f\"Total unique topics named: {len(topic_names)}\")\n",
    "\n",
    "\n",
    "# Author analysis and top papers per topic\n",
    "logger.info(\"Starting author statistics and top papers extraction...\")\n",
    "top_papers, author_stats = get_author_stats(paper_classifications, df, n_top=5)\n",
    "\n",
    "# Save topic/author analysis results\n",
    "logger.info(\"Saving topic analysis outputs...\")\n",
    "current_date = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "keyword_str = '_'.join(search_keywords) if search_keywords else \"\"\n",
    "suffix_string = f\"{current_date}{keyword_str}\"\n",
    "\n",
    "\n",
    "\n",
    "save_topic_analysis_outputs(df, lda_model, lda_vectorizer, topic_distributions, topic_keywords, topic_names, topic_ngrams, author_stats, top_papers, topic_ngrams, suffix_string)\n",
    "print(\"\\nSample topics and names:\")\n",
    "print({k: topic_names[k] for k in list(topic_names)[:5]})\n",
    "print(\"\\nTop authors and top papers by topic (first 2):\")\n",
    "print(dict(list(top_papers.items())[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb3b5b8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dffda39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Old workflow, # %%\\n# Cell 10: Method Extraction and Assignment Workflow (Iterative LLM, With Parameterization)\\n\\n# ----------- Parameters for Method Extraction Workflow -----------\\nMAX_FEATURES = 10000         # Max n-grams for candidate extraction\\nNGRAM_RANGE = (1, 4)         # For TF-IDF/LDA\\nWINDOW_COMPOUND = 150        # Window for proximity scoring\\nMIN_WORD_LEN = 4             # Minimum token length\\nTFIDF_WEIGHT = 0.5           # Weight for TF-IDF\\nLDA_WEIGHT = 0.3             # Weight for LDA\\nCOMPOUND_WEIGHT = 0.2        # Weight for compound\\nTOP_METHODS_PER_PAPER = 4    # Number of methods assigned per paper\\nMIN_ASSIGN_SCORE = 0.02      # Minimum combined score\\nBATCH_SIZE_LLM = 100         # LLM batch for abbreviations\\n\\n# LLM parameters for method phrase extraction\\nMETHOD_LLM_N_RUNS = 3\\nMETHOD_LLM_TEMP = 0.05\\nMETHOD_LLM_TOP_P = 0.95\\n\\nlogger.info(\"Starting pipeline for method detection and assignment...\")\\n\\n# 1. Extract broad candidate n-grams from the corpus\\ncandidate_terms = extract_candidate_terms(\\n    df, text_col=\\'processed_text\\', max_features=MAX_FEATURES)\\nlogger.info(f\"Step 1: Extracted {len(candidate_terms)} candidate terms from the corpus.\")\\n\\n# 2. Use robust/iterative LLM to filter for method/technique phrases\\nmethod_phrases_all, method_phrase_counts = get_method_phrases_robust(\\n    candidate_terms,\\n    client,\\n    model_type,\\n    credit_tracker,\\n    n_runs=METHOD_LLM_N_RUNS,\\n    temp=METHOD_LLM_TEMP,\\n    top_p=METHOD_LLM_TOP_P\\n)\\nlogger.info(f\"Step 2: Extracted {len(method_phrases_all)} method phrases from the corpus.\")\\nmethod_phrases = filter_methods_with_llm(\\n    method_phrases_all,\\n    client,\\n    model_type,\\n    credit_tracker,\\n    n_batched=100\\n)\\nlogger.info(f\"Filtered out {len(method_phrases_all) - len(method_phrases)} generic phrases, leaving {len(method_phrases)} specific method phrases.\")\\n\\n# 3. Use LLM to build abbreviation/synonym dictionary\\nlogger.info(\"Building method abbreviation dictionary with LLM...\")\\nmethod_dict = get_method_abbreviation_dict(\\n    method_phrases, client, model_type, credit_tracker, batch_size=BATCH_SIZE_LLM)\\nlogger. info(f\"Get_method_abbreviation_dict found {len(method_dict)} methods with abbreviations/synonyms.\")\\nabbr_to_canonical_map = build_abbr_to_canonical_map(method_dict)\\nlogger.info(f\"Built abbreviation map for {len(abbr_to_canonical_map)} methods.\")\\nlogger\\ndf[\\'standardized_text\\'] = df[\\'processed_text\\'].apply(\\n    lambda t: standardize_methods_in_text(t, abbr_to_canonical_map))\\nlogger.info(\"Standardized method phrases in the corpus text.\")\\nmethod_vocabulary = sorted(method_dict.keys())\\nlogger.info(f\"Step 3: Built abbreviation map for {len(method_vocabulary)} methods.\")\\n\\n# 4. Compute all method score matrices\\nlogger.info(\"Computing method score matrices...\")\\ntfidf_scores, method_names = compute_tfidf_scores(\\n    df[\\'standardized_text\\'], method_vocabulary, ngram_range=NGRAM_RANGE)\\nlogger.info(f\"Step 4a: Computed TF-IDF scores for {len(method_names)} methods.\")\\nlda_n_topics = len(method_vocabulary)\\nlogger.info(f\"Step 4b: Using {lda_n_topics} topics for LDA scoring.\")\\nlda_scores, lda_names = compute_lda_scores(\\n    df[\\'standardized_text\\'], method_vocabulary, ngram_range=NGRAM_RANGE, n_topics=lda_n_topics)\\nassert list(method_names) == list(lda_names)\\n\\nlogger.info(f\"Step 4c: Computing compound scores for {len(method_names)} methods.\")\\ncompound_scores = compute_compound_scores(\\n    df, method_names, processed_col=\\'standardized_text\\',\\n    window=WINDOW_COMPOUND, min_word_len=MIN_WORD_LEN)\\nlogger.info(f\"Step 4d: Combined scores from TF-IDF, LDA, and compound methods.\")\\ncombined_scores = combine_method_scores(\\n    tfidf_scores, lda_scores, compound_scores,\\n    weights=(TFIDF_WEIGHT, LDA_WEIGHT, COMPOUND_WEIGHT))\\nlogger.info(f\"Step 4: Computed complete method score matrices with {len(method_names)} methods.\")\\n\\n# 5. Assign methods to papers with confidence\\ndf = assign_top_methods_by_total_score(\\n    df, combined_scores, method_names,\\n    top_n=TOP_METHODS_PER_PAPER, min_score=MIN_ASSIGN_SCORE)\\nlogger.info(f\"Step 5: Assigned top {TOP_METHODS_PER_PAPER} methods to {len(df)} papers with confidence levels.\")\\n\\n# 6. Save all matrix DataFrames for visualization\\nfor scores, label in zip([tfidf_scores, lda_scores, compound_scores, combined_scores],\\n                         [\"tfidf\", \"lda\", \"compound\", \"combined\"]):\\n    pd.DataFrame(scores, columns=method_names, index=df.index).to_csv(\\n        os.path.join(SAVE_DIR, f\"semantic_scholar_{suffix_string}_method_{label}_scores.csv\")\\n    )\\nlogger.info(f\"Step 6: Saved method score matrices to {SAVE_DIR}.\")\\n\\n# 7. Run diagnostics\\ndiagnostics_with_scores(df, tfidf_scores, lda_scores, compound_scores, combined_scores, method_names)\\nlogger.info(f\"Step 7: Diagnostics completed.\")\\n\\n# 8. Output short preview\\nprint(df[[\\'Primary_Method\\', \\'Primary_Method_Score\\', \\'Method_Confidence\\', \\'Top_1_Method\\', \\'Top_1_Score\\']].head())\\nlogger.info(f\"Step 8: Output preview of method assignments.\")\\n\\n# 9. Final saving of the DataFrame with method assignments\\ndf.to_csv(os.path.join(SAVE_DIR, f\"semantic_scholar_{suffix_string}_all_results.csv\"))\\nlogger.info(f\"Step 9: Saved final DataFrame with method assignments to {SAVE_DIR}.\")\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Old workflow, # %%\n",
    "# Cell 10: Method Extraction and Assignment Workflow (Iterative LLM, With Parameterization)\n",
    "\n",
    "# ----------- Parameters for Method Extraction Workflow -----------\n",
    "MAX_FEATURES = 10000         # Max n-grams for candidate extraction\n",
    "NGRAM_RANGE = (1, 4)         # For TF-IDF/LDA\n",
    "WINDOW_COMPOUND = 150        # Window for proximity scoring\n",
    "MIN_WORD_LEN = 4             # Minimum token length\n",
    "TFIDF_WEIGHT = 0.5           # Weight for TF-IDF\n",
    "LDA_WEIGHT = 0.3             # Weight for LDA\n",
    "COMPOUND_WEIGHT = 0.2        # Weight for compound\n",
    "TOP_METHODS_PER_PAPER = 4    # Number of methods assigned per paper\n",
    "MIN_ASSIGN_SCORE = 0.02      # Minimum combined score\n",
    "BATCH_SIZE_LLM = 100         # LLM batch for abbreviations\n",
    "\n",
    "# LLM parameters for method phrase extraction\n",
    "METHOD_LLM_N_RUNS = 3\n",
    "METHOD_LLM_TEMP = 0.05\n",
    "METHOD_LLM_TOP_P = 0.95\n",
    "\n",
    "logger.info(\"Starting pipeline for method detection and assignment...\")\n",
    "\n",
    "# 1. Extract broad candidate n-grams from the corpus\n",
    "candidate_terms = extract_candidate_terms(\n",
    "    df, text_col='processed_text', max_features=MAX_FEATURES)\n",
    "logger.info(f\"Step 1: Extracted {len(candidate_terms)} candidate terms from the corpus.\")\n",
    "\n",
    "# 2. Use robust/iterative LLM to filter for method/technique phrases\n",
    "method_phrases_all, method_phrase_counts = get_method_phrases_robust(\n",
    "    candidate_terms,\n",
    "    client,\n",
    "    model_type,\n",
    "    credit_tracker,\n",
    "    n_runs=METHOD_LLM_N_RUNS,\n",
    "    temp=METHOD_LLM_TEMP,\n",
    "    top_p=METHOD_LLM_TOP_P\n",
    ")\n",
    "logger.info(f\"Step 2: Extracted {len(method_phrases_all)} method phrases from the corpus.\")\n",
    "method_phrases = filter_methods_with_llm(\n",
    "    method_phrases_all,\n",
    "    client,\n",
    "    model_type,\n",
    "    credit_tracker,\n",
    "    n_batched=100\n",
    ")\n",
    "logger.info(f\"Filtered out {len(method_phrases_all) - len(method_phrases)} generic phrases, leaving {len(method_phrases)} specific method phrases.\")\n",
    "\n",
    "# 3. Use LLM to build abbreviation/synonym dictionary\n",
    "logger.info(\"Building method abbreviation dictionary with LLM...\")\n",
    "method_dict = get_method_abbreviation_dict(\n",
    "    method_phrases, client, model_type, credit_tracker, batch_size=BATCH_SIZE_LLM)\n",
    "logger. info(f\"Get_method_abbreviation_dict found {len(method_dict)} methods with abbreviations/synonyms.\")\n",
    "abbr_to_canonical_map = build_abbr_to_canonical_map(method_dict)\n",
    "logger.info(f\"Built abbreviation map for {len(abbr_to_canonical_map)} methods.\")\n",
    "logger\n",
    "df['standardized_text'] = df['processed_text'].apply(\n",
    "    lambda t: standardize_methods_in_text(t, abbr_to_canonical_map))\n",
    "logger.info(\"Standardized method phrases in the corpus text.\")\n",
    "method_vocabulary = sorted(method_dict.keys())\n",
    "logger.info(f\"Step 3: Built abbreviation map for {len(method_vocabulary)} methods.\")\n",
    "\n",
    "# 4. Compute all method score matrices\n",
    "logger.info(\"Computing method score matrices...\")\n",
    "tfidf_scores, method_names = compute_tfidf_scores(\n",
    "    df['standardized_text'], method_vocabulary, ngram_range=NGRAM_RANGE)\n",
    "logger.info(f\"Step 4a: Computed TF-IDF scores for {len(method_names)} methods.\")\n",
    "lda_n_topics = len(method_vocabulary)\n",
    "logger.info(f\"Step 4b: Using {lda_n_topics} topics for LDA scoring.\")\n",
    "lda_scores, lda_names = compute_lda_scores(\n",
    "    df['standardized_text'], method_vocabulary, ngram_range=NGRAM_RANGE, n_topics=lda_n_topics)\n",
    "assert list(method_names) == list(lda_names)\n",
    "\n",
    "logger.info(f\"Step 4c: Computing compound scores for {len(method_names)} methods.\")\n",
    "compound_scores = compute_compound_scores(\n",
    "    df, method_names, processed_col='standardized_text',\n",
    "    window=WINDOW_COMPOUND, min_word_len=MIN_WORD_LEN)\n",
    "logger.info(f\"Step 4d: Combined scores from TF-IDF, LDA, and compound methods.\")\n",
    "combined_scores = combine_method_scores(\n",
    "    tfidf_scores, lda_scores, compound_scores,\n",
    "    weights=(TFIDF_WEIGHT, LDA_WEIGHT, COMPOUND_WEIGHT))\n",
    "logger.info(f\"Step 4: Computed complete method score matrices with {len(method_names)} methods.\")\n",
    "\n",
    "# 5. Assign methods to papers with confidence\n",
    "df = assign_top_methods_by_total_score(\n",
    "    df, combined_scores, method_names,\n",
    "    top_n=TOP_METHODS_PER_PAPER, min_score=MIN_ASSIGN_SCORE)\n",
    "logger.info(f\"Step 5: Assigned top {TOP_METHODS_PER_PAPER} methods to {len(df)} papers with confidence levels.\")\n",
    "\n",
    "# 6. Save all matrix DataFrames for visualization\n",
    "for scores, label in zip([tfidf_scores, lda_scores, compound_scores, combined_scores],\n",
    "                         [\"tfidf\", \"lda\", \"compound\", \"combined\"]):\n",
    "    pd.DataFrame(scores, columns=method_names, index=df.index).to_csv(\n",
    "        os.path.join(SAVE_DIR, f\"semantic_scholar_{suffix_string}_method_{label}_scores.csv\")\n",
    "    )\n",
    "logger.info(f\"Step 6: Saved method score matrices to {SAVE_DIR}.\")\n",
    "\n",
    "# 7. Run diagnostics\n",
    "diagnostics_with_scores(df, tfidf_scores, lda_scores, compound_scores, combined_scores, method_names)\n",
    "logger.info(f\"Step 7: Diagnostics completed.\")\n",
    "\n",
    "# 8. Output short preview\n",
    "print(df[['Primary_Method', 'Primary_Method_Score', 'Method_Confidence', 'Top_1_Method', 'Top_1_Score']].head())\n",
    "logger.info(f\"Step 8: Output preview of method assignments.\")\n",
    "\n",
    "# 9. Final saving of the DataFrame with method assignments\n",
    "df.to_csv(os.path.join(SAVE_DIR, f\"semantic_scholar_{suffix_string}_all_results.csv\"))\n",
    "logger.info(f\"Step 9: Saved final DataFrame with method assignments to {SAVE_DIR}.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e4ecf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Old workflow %%\\n# Cell 10: Method Extraction and Assignment Workflow (Iterative LLM, With Parameterization, Robust Filtering)\\n\\n# ----------- Parameters for Method Extraction Workflow -----------\\nMAX_FEATURES = 10000         # Max n-grams for candidate extraction\\nNGRAM_RANGE = (1, 4)         # For TF-IDF/LDA\\nWINDOW_COMPOUND = 150        # Window for proximity scoring\\nMIN_WORD_LEN = 4             # Minimum token length\\nTFIDF_WEIGHT = 0.5           # Weight for TF-IDF\\nLDA_WEIGHT = 0.3             # Weight for LDA\\nCOMPOUND_WEIGHT = 0.2        # Weight for compound\\nTOP_METHODS_PER_PAPER = 4    # Number of methods assigned per paper\\nMIN_ASSIGN_SCORE = 0.2      # Minimum combined score\\nBATCH_SIZE_LLM = 100         # LLM batch for abbreviations\\n\\n# LLM parameters for method phrase extraction\\nMETHOD_LLM_N_RUNS = 3\\nMETHOD_LLM_TEMP = 0.05\\nMETHOD_LLM_TOP_P = 0.95\\n\\n\\n\\n# --- Begin workflow ---\\n\\nlogger.info(\"Starting pipeline for method detection and assignment...\")\\n\\n# 1. Extract candidate n-grams\\ncandidate_terms = extract_candidate_terms(\\n    df, text_col=\\'processed_text\\', max_features=MAX_FEATURES)\\nlogger.info(f\"Step 1: Extracted {len(candidate_terms)} candidate terms from the corpus.\")\\n\\n# 2. LLM method extraction\\nmethod_phrases_all, method_phrase_counts = get_method_phrases_robust(\\n    candidate_terms,\\n    client,\\n    model_type,\\n    credit_tracker,\\n    n_runs=METHOD_LLM_N_RUNS,\\n    temp=METHOD_LLM_TEMP,\\n    top_p=METHOD_LLM_TOP_P\\n)\\nlogger.info(f\"Step 2: Extracted {len(method_phrases_all)} method phrases from the corpus.\")\\nprint(\"DEBUG: Raw LLM method extraction sample:\", method_phrases_all[:10])\\n\\n# 3. LLM filtering (robust, with logging/fallback)\\nmethod_phrases, filter_all = filter_methods_with_llm(\\n    method_phrases_all,\\n    client,\\n    model_type,\\n    credit_tracker,\\n    n_batched=100,\\n    verbose=True,\\n    fallback_to_input=True,\\n    min_methods=20\\n)\\n\\nlogger.info(f\"Filtered out {len(method_phrases_all) - len(method_phrases)} generic phrases, leaving {len(method_phrases)} specific method phrases.\")\\n\\nif filter_all:\\n    logger.warning(\"All method phrases filtered out by LLM—fallback or relaxation engaged.\")\\n\\n# --- STOP if vocabulary is empty ---\\nif not method_phrases:\\n    logger.error(\"No method phrases after filtering. Stopping workflow! Adjust LLM extraction, candidate set, or prompt.\")\\n    raise RuntimeError(\"Critical: Method phrase vocabulary is empty after LLM filtering. Workflow terminated.\")\\n\\n# 4. Use LLM to build abbreviation/synonym dictionary\\nlogger.info(\"Building method abbreviation dictionary with LLM...\")\\nmethod_dict = get_method_abbreviation_dict(\\n    method_phrases, client, model_type, credit_tracker, batch_size=BATCH_SIZE_LLM)\\nlogger.info(f\"Get_method_abbreviation_dict found {len(method_dict)} methods with abbreviations/synonyms.\")\\nabbr_to_canonical_map = build_abbr_to_canonical_map(method_dict)\\nlogger.info(f\"Built abbreviation map for {len(abbr_to_canonical_map)} methods.\")\\n\\n# 5. Standardize and check vocabulary\\ndf[\\'standardized_text\\'] = df[\\'processed_text\\'].apply(\\n    lambda t: standardize_methods_in_text(t, abbr_to_canonical_map))\\nmethod_vocabulary = sorted(method_dict.keys())\\nprint(\"DEBUG: Final method vocabulary (sample):\", method_vocabulary[:10])\\nif not method_vocabulary:\\n    logger.error(\"No methods left after LLM filtering and abbreviation mapping. Stopping workflow!\")\\n    raise RuntimeError(\"Critical: Method vocabulary is empty after all processing.\")\\n\\nlogger.info(f\"Step 3: Ready with {len(method_vocabulary)} final methods.\")\\n\\n# 6. Compute all method score matrices\\nlogger.info(\"Computing method score matrices...\")\\ntfidf_scores, method_names = compute_tfidf_scores(\\n    df[\\'standardized_text\\'], method_vocabulary, ngram_range=NGRAM_RANGE)\\nlogger.info(f\"Step 4a: Computed TF-IDF scores for {len(method_names)} methods.\")\\nlda_n_topics = len(method_vocabulary)\\nlogger.info(f\"Step 4b: Using {lda_n_topics} topics for LDA scoring.\")\\nlda_scores, lda_names = compute_lda_scores(\\n    df[\\'standardized_text\\'], method_vocabulary, ngram_range=NGRAM_RANGE, n_topics=lda_n_topics)\\nassert list(method_names) == list(lda_names)\\n\\nlogger.info(f\"Step 4c: Computing compound scores for {len(method_names)} methods.\")\\ncompound_scores = compute_compound_scores(\\n    df, method_names, processed_col=\\'standardized_text\\',\\n    window=WINDOW_COMPOUND, min_word_len=MIN_WORD_LEN)\\nlogger.info(f\"Step 4d: Combined scores from TF-IDF, LDA, and compound methods.\")\\ncombined_scores = combine_method_scores(\\n    tfidf_scores, lda_scores, compound_scores,\\n    weights=(TFIDF_WEIGHT, LDA_WEIGHT, COMPOUND_WEIGHT)\\n)\\nlogger.info(f\"Step 4: Computed complete method score matrices with {len(method_names)} methods.\")\\n\\n# 7. Assign methods to papers with confidence\\ndf = assign_top_methods_by_total_score(\\n    df, combined_scores, method_names,\\n    top_n=TOP_METHODS_PER_PAPER, min_score=MIN_ASSIGN_SCORE)\\nlogger.info(f\"Step 5: Assigned top {TOP_METHODS_PER_PAPER} methods to {len(df)} papers with confidence levels.\")\\n\\n# 8. Save all matrix DataFrames for visualization\\nfor scores, label in zip([tfidf_scores, lda_scores, compound_scores, combined_scores],\\n                         [\"tfidf\", \"lda\", \"compound\", \"combined\"]):\\n    pd.DataFrame(scores, columns=method_names, index=df.index).to_csv(\\n        os.path.join(SAVE_DIR, f\"semantic_scholar_{suffix_string}_method_{label}_scores.csv\")\\n    )\\nlogger.info(f\"Step 6: Saved method score matrices to {SAVE_DIR}.\")\\n\\n# 9. Run diagnostics\\ndiagnostics_with_scores(df, tfidf_scores, lda_scores, compound_scores, combined_scores, method_names)\\nlogger.info(f\"Step 7: Diagnostics completed.\")\\n\\n# 10. Output short preview\\nprint(df[[\\'Primary_Method\\', \\'Primary_Method_Score\\', \\'Method_Confidence\\', \\'Top_1_Method\\', \\'Top_1_Score\\']].head())\\nlogger.info(f\"Step 8: Output preview of method assignments.\")\\n\\n# 11. Final saving of the DataFrame with method assignments\\ndf.to_csv(os.path.join(SAVE_DIR, f\"semantic_scholar_{suffix_string}_all_results.csv\"))\\nlogger.info(f\"Step 9: Saved final DataFrame with method assignments to {SAVE_DIR}.\")\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Old workflow %%\n",
    "# Cell 10: Method Extraction and Assignment Workflow (Iterative LLM, With Parameterization, Robust Filtering)\n",
    "\n",
    "# ----------- Parameters for Method Extraction Workflow -----------\n",
    "MAX_FEATURES = 10000         # Max n-grams for candidate extraction\n",
    "NGRAM_RANGE = (1, 4)         # For TF-IDF/LDA\n",
    "WINDOW_COMPOUND = 150        # Window for proximity scoring\n",
    "MIN_WORD_LEN = 4             # Minimum token length\n",
    "TFIDF_WEIGHT = 0.5           # Weight for TF-IDF\n",
    "LDA_WEIGHT = 0.3             # Weight for LDA\n",
    "COMPOUND_WEIGHT = 0.2        # Weight for compound\n",
    "TOP_METHODS_PER_PAPER = 4    # Number of methods assigned per paper\n",
    "MIN_ASSIGN_SCORE = 0.2      # Minimum combined score\n",
    "BATCH_SIZE_LLM = 100         # LLM batch for abbreviations\n",
    "\n",
    "# LLM parameters for method phrase extraction\n",
    "METHOD_LLM_N_RUNS = 3\n",
    "METHOD_LLM_TEMP = 0.05\n",
    "METHOD_LLM_TOP_P = 0.95\n",
    "\n",
    "\n",
    "\n",
    "# --- Begin workflow ---\n",
    "\n",
    "logger.info(\"Starting pipeline for method detection and assignment...\")\n",
    "\n",
    "# 1. Extract candidate n-grams\n",
    "candidate_terms = extract_candidate_terms(\n",
    "    df, text_col='processed_text', max_features=MAX_FEATURES)\n",
    "logger.info(f\"Step 1: Extracted {len(candidate_terms)} candidate terms from the corpus.\")\n",
    "\n",
    "# 2. LLM method extraction\n",
    "method_phrases_all, method_phrase_counts = get_method_phrases_robust(\n",
    "    candidate_terms,\n",
    "    client,\n",
    "    model_type,\n",
    "    credit_tracker,\n",
    "    n_runs=METHOD_LLM_N_RUNS,\n",
    "    temp=METHOD_LLM_TEMP,\n",
    "    top_p=METHOD_LLM_TOP_P\n",
    ")\n",
    "logger.info(f\"Step 2: Extracted {len(method_phrases_all)} method phrases from the corpus.\")\n",
    "print(\"DEBUG: Raw LLM method extraction sample:\", method_phrases_all[:10])\n",
    "\n",
    "# 3. LLM filtering (robust, with logging/fallback)\n",
    "method_phrases, filter_all = filter_methods_with_llm(\n",
    "    method_phrases_all,\n",
    "    client,\n",
    "    model_type,\n",
    "    credit_tracker,\n",
    "    n_batched=100,\n",
    "    verbose=True,\n",
    "    fallback_to_input=True,\n",
    "    min_methods=20\n",
    ")\n",
    "\n",
    "logger.info(f\"Filtered out {len(method_phrases_all) - len(method_phrases)} generic phrases, leaving {len(method_phrases)} specific method phrases.\")\n",
    "\n",
    "if filter_all:\n",
    "    logger.warning(\"All method phrases filtered out by LLM—fallback or relaxation engaged.\")\n",
    "\n",
    "# --- STOP if vocabulary is empty ---\n",
    "if not method_phrases:\n",
    "    logger.error(\"No method phrases after filtering. Stopping workflow! Adjust LLM extraction, candidate set, or prompt.\")\n",
    "    raise RuntimeError(\"Critical: Method phrase vocabulary is empty after LLM filtering. Workflow terminated.\")\n",
    "\n",
    "# 4. Use LLM to build abbreviation/synonym dictionary\n",
    "logger.info(\"Building method abbreviation dictionary with LLM...\")\n",
    "method_dict = get_method_abbreviation_dict(\n",
    "    method_phrases, client, model_type, credit_tracker, batch_size=BATCH_SIZE_LLM)\n",
    "logger.info(f\"Get_method_abbreviation_dict found {len(method_dict)} methods with abbreviations/synonyms.\")\n",
    "abbr_to_canonical_map = build_abbr_to_canonical_map(method_dict)\n",
    "logger.info(f\"Built abbreviation map for {len(abbr_to_canonical_map)} methods.\")\n",
    "\n",
    "# 5. Standardize and check vocabulary\n",
    "df['standardized_text'] = df['processed_text'].apply(\n",
    "    lambda t: standardize_methods_in_text(t, abbr_to_canonical_map))\n",
    "method_vocabulary = sorted(method_dict.keys())\n",
    "print(\"DEBUG: Final method vocabulary (sample):\", method_vocabulary[:10])\n",
    "if not method_vocabulary:\n",
    "    logger.error(\"No methods left after LLM filtering and abbreviation mapping. Stopping workflow!\")\n",
    "    raise RuntimeError(\"Critical: Method vocabulary is empty after all processing.\")\n",
    "\n",
    "logger.info(f\"Step 3: Ready with {len(method_vocabulary)} final methods.\")\n",
    "\n",
    "# 6. Compute all method score matrices\n",
    "logger.info(\"Computing method score matrices...\")\n",
    "tfidf_scores, method_names = compute_tfidf_scores(\n",
    "    df['standardized_text'], method_vocabulary, ngram_range=NGRAM_RANGE)\n",
    "logger.info(f\"Step 4a: Computed TF-IDF scores for {len(method_names)} methods.\")\n",
    "lda_n_topics = len(method_vocabulary)\n",
    "logger.info(f\"Step 4b: Using {lda_n_topics} topics for LDA scoring.\")\n",
    "lda_scores, lda_names = compute_lda_scores(\n",
    "    df['standardized_text'], method_vocabulary, ngram_range=NGRAM_RANGE, n_topics=lda_n_topics)\n",
    "assert list(method_names) == list(lda_names)\n",
    "\n",
    "logger.info(f\"Step 4c: Computing compound scores for {len(method_names)} methods.\")\n",
    "compound_scores = compute_compound_scores(\n",
    "    df, method_names, processed_col='standardized_text',\n",
    "    window=WINDOW_COMPOUND, min_word_len=MIN_WORD_LEN)\n",
    "logger.info(f\"Step 4d: Combined scores from TF-IDF, LDA, and compound methods.\")\n",
    "combined_scores = combine_method_scores(\n",
    "    tfidf_scores, lda_scores, compound_scores,\n",
    "    weights=(TFIDF_WEIGHT, LDA_WEIGHT, COMPOUND_WEIGHT)\n",
    ")\n",
    "logger.info(f\"Step 4: Computed complete method score matrices with {len(method_names)} methods.\")\n",
    "\n",
    "# 7. Assign methods to papers with confidence\n",
    "df = assign_top_methods_by_total_score(\n",
    "    df, combined_scores, method_names,\n",
    "    top_n=TOP_METHODS_PER_PAPER, min_score=MIN_ASSIGN_SCORE)\n",
    "logger.info(f\"Step 5: Assigned top {TOP_METHODS_PER_PAPER} methods to {len(df)} papers with confidence levels.\")\n",
    "\n",
    "# 8. Save all matrix DataFrames for visualization\n",
    "for scores, label in zip([tfidf_scores, lda_scores, compound_scores, combined_scores],\n",
    "                         [\"tfidf\", \"lda\", \"compound\", \"combined\"]):\n",
    "    pd.DataFrame(scores, columns=method_names, index=df.index).to_csv(\n",
    "        os.path.join(SAVE_DIR, f\"semantic_scholar_{suffix_string}_method_{label}_scores.csv\")\n",
    "    )\n",
    "logger.info(f\"Step 6: Saved method score matrices to {SAVE_DIR}.\")\n",
    "\n",
    "# 9. Run diagnostics\n",
    "diagnostics_with_scores(df, tfidf_scores, lda_scores, compound_scores, combined_scores, method_names)\n",
    "logger.info(f\"Step 7: Diagnostics completed.\")\n",
    "\n",
    "# 10. Output short preview\n",
    "print(df[['Primary_Method', 'Primary_Method_Score', 'Method_Confidence', 'Top_1_Method', 'Top_1_Score']].head())\n",
    "logger.info(f\"Step 8: Output preview of method assignments.\")\n",
    "\n",
    "# 11. Final saving of the DataFrame with method assignments\n",
    "df.to_csv(os.path.join(SAVE_DIR, f\"semantic_scholar_{suffix_string}_all_results.csv\"))\n",
    "logger.info(f\"Step 9: Saved final DataFrame with method assignments to {SAVE_DIR}.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c72043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a831bef1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m METHOD_LLM_TEMP \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m\n\u001b[0;32m     22\u001b[0m METHOD_LLM_TOP_P \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.95\u001b[39m\n\u001b[1;32m---> 24\u001b[0m \u001b[43mlogger\u001b[49m\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting pipeline for method detection and assignment...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# 1. Extract candidate n-grams\u001b[39;00m\n\u001b[0;32m     27\u001b[0m candidate_terms \u001b[38;5;241m=\u001b[39m extract_candidate_terms(\n\u001b[0;32m     28\u001b[0m     df, text_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m, max_features\u001b[38;5;241m=\u001b[39mMAX_FEATURES\n\u001b[0;32m     29\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "# New workflow \n",
    "# ===============================================\n",
    "# ========== Method Extraction Workflow =========\n",
    "# ========== (Robust LLM/Pipeline) =============\n",
    "# ===============================================\n",
    "\n",
    "# ----------- Parameters for Method Extraction Workflow -----------\n",
    "MAX_FEATURES = 10000         # Max n-grams for candidate extraction\n",
    "NGRAM_RANGE = (1, 4)         # For TF-IDF/LDA\n",
    "WINDOW_COMPOUND = 150        # Window for proximity scoring\n",
    "MIN_WORD_LEN = 4             # Minimum token length\n",
    "TFIDF_WEIGHT = 0.5           # Weight for TF-IDF\n",
    "LDA_WEIGHT = 0.3             # Weight for LDA\n",
    "COMPOUND_WEIGHT = 0.2        # Weight for compound\n",
    "TOP_METHODS_PER_PAPER = 5    # Number of methods assigned per paper\n",
    "MIN_ASSIGN_SCORE = 0.2       # Minimum combined score\n",
    "BATCH_SIZE_LLM = 100         # LLM batch for abbreviations\n",
    "\n",
    "# LLM parameters for method phrase extraction\n",
    "METHOD_LLM_N_RUNS = 3\n",
    "METHOD_LLM_TEMP = 0.05\n",
    "METHOD_LLM_TOP_P = 0.95\n",
    "\n",
    "logger.info(\"Starting pipeline for method detection and assignment...\")\n",
    "\n",
    "# 1. Extract candidate n-grams\n",
    "candidate_terms = extract_candidate_terms(\n",
    "    df, text_col='processed_text', max_features=MAX_FEATURES\n",
    ")\n",
    "logger.info(f\"Step 1: Extracted {len(candidate_terms)} candidate terms from the corpus.\")\n",
    "\n",
    "# 2. LLM method extraction\n",
    "method_phrases_all, method_phrase_counts = get_method_phrases_robust(\n",
    "    candidate_terms,\n",
    "    client,\n",
    "    model_type,\n",
    "    credit_tracker,\n",
    "    n_runs=METHOD_LLM_N_RUNS,\n",
    "    temp=METHOD_LLM_TEMP,\n",
    "    top_p=METHOD_LLM_TOP_P\n",
    ")\n",
    "logger.info(f\"Step 2: Extracted {len(method_phrases_all)} method phrases from the corpus.\")\n",
    "print(\"DEBUG: Raw LLM method extraction sample:\", method_phrases_all[:10])\n",
    "\n",
    "# 3. LLM filtering (robust, with logging/fallback)\n",
    "method_phrases, filter_all = filter_methods_with_llm(\n",
    "    method_phrases_all,\n",
    "    client,\n",
    "    model_type,\n",
    "    credit_tracker,\n",
    "    n_batched=BATCH_SIZE_LLM,\n",
    "    verbose=True,\n",
    "    fallback_to_input=True,\n",
    "    min_methods=20,\n",
    "    temp=METHOD_LLM_TEMP,\n",
    "    top_p=METHOD_LLM_TOP_P\n",
    ")\n",
    "\n",
    "logger.info(f\"Filtered out {len(method_phrases_all) - len(method_phrases)} generic phrases, leaving {len(method_phrases)} specific method phrases.\")\n",
    "\n",
    "if filter_all:\n",
    "    logger.warning(\"All method phrases filtered out by LLM—fallback or relaxation engaged.\")\n",
    "\n",
    "# --- STOP if vocabulary is empty ---\n",
    "if not method_phrases:\n",
    "    logger.error(\"No method phrases after filtering. Stopping workflow! Adjust LLM extraction, candidate set, or prompt.\")\n",
    "    raise RuntimeError(\"Critical: Method phrase vocabulary is empty after LLM filtering. Workflow terminated.\")\n",
    "\n",
    "# 4. Use LLM to build abbreviation/synonym dictionary\n",
    "logger.info(\"Building method abbreviation dictionary with LLM...\")\n",
    "method_dict = get_method_abbreviation_dict(\n",
    "    method_phrases, client, model_type, credit_tracker, batch_size=BATCH_SIZE_LLM)\n",
    "logger.info(f\"Get_method_abbreviation_dict found {len(method_dict)} methods with abbreviations/synonyms.\")\n",
    "abbr_to_canonical_map = build_abbr_to_canonical_map(method_dict)\n",
    "logger.info(f\"Built abbreviation map for {len(abbr_to_canonical_map)} methods.\")\n",
    "\n",
    "# 5. Standardize and check vocabulary\n",
    "df['standardized_text'] = df['processed_text'].apply(\n",
    "    lambda t: standardize_methods_in_text(t, abbr_to_canonical_map))\n",
    "method_vocabulary = sorted(method_dict.keys())\n",
    "print(\"DEBUG: Final method vocabulary (sample):\", method_vocabulary[:10])\n",
    "if not method_vocabulary:\n",
    "    logger.error(\"No methods left after LLM filtering and abbreviation mapping. Stopping workflow!\")\n",
    "    raise RuntimeError(\"Critical: Method vocabulary is empty after all processing.\")\n",
    "\n",
    "logger.info(f\"Step 3: Ready with {len(method_vocabulary)} final methods.\")\n",
    "\n",
    "# 6. Compute all method score matrices\n",
    "logger.info(\"Computing method score matrices...\")\n",
    "tfidf_scores, method_names = compute_tfidf_scores(\n",
    "    df['standardized_text'], method_vocabulary, ngram_range=NGRAM_RANGE\n",
    ")\n",
    "logger.info(f\"Step 4a: Computed TF-IDF scores for {len(method_names)} methods.\")\n",
    "lda_n_topics = len(method_vocabulary)\n",
    "logger.info(f\"Step 4b: Using {lda_n_topics} topics for LDA scoring.\")\n",
    "lda_scores, lda_names = compute_lda_scores(\n",
    "    df['standardized_text'], method_vocabulary, ngram_range=NGRAM_RANGE, n_topics=lda_n_topics\n",
    ")\n",
    "assert list(method_names) == list(lda_names)\n",
    "\n",
    "logger.info(f\"Step 4c: Computing compound scores for {len(method_names)} methods.\")\n",
    "compound_scores = compute_compound_scores(\n",
    "    df, method_names, processed_col='standardized_text',\n",
    "    window=WINDOW_COMPOUND, min_word_len=MIN_WORD_LEN\n",
    ")\n",
    "logger.info(f\"Step 4d: Combined scores from TF-IDF, LDA, and compound methods.\")\n",
    "combined_scores = combine_method_scores(\n",
    "    tfidf_scores, lda_scores, compound_scores,\n",
    "    weights=(TFIDF_WEIGHT, LDA_WEIGHT, COMPOUND_WEIGHT)\n",
    ")\n",
    "logger.info(f\"Step 4: Computed complete method score matrices with {len(method_names)} methods.\")\n",
    "\n",
    "# 7. Assign methods to papers with confidence\n",
    "df = assign_top_methods_by_total_score(\n",
    "    df, combined_scores, method_names,\n",
    "    top_n=TOP_METHODS_PER_PAPER, min_score=MIN_ASSIGN_SCORE\n",
    ")\n",
    "logger.info(f\"Step 5: Assigned top {TOP_METHODS_PER_PAPER} methods to {len(df)} papers with confidence levels.\")\n",
    "\n",
    "# 8. Save all matrix DataFrames for visualization\n",
    "for scores, label in zip([tfidf_scores, lda_scores, compound_scores, combined_scores],\n",
    "                         [\"tfidf\", \"lda\", \"compound\", \"combined\"]):\n",
    "    pd.DataFrame(scores, columns=method_names, index=df.index).to_csv(\n",
    "        os.path.join(SAVE_DIR, f\"semantic_scholar_{suffix_string}_method_{label}_scores.csv\")\n",
    "    )\n",
    "logger.info(f\"Step 6: Saved method score matrices to {SAVE_DIR}.\")\n",
    "\n",
    "# 9. Run diagnostics\n",
    "diagnostics_with_scores(df, tfidf_scores, lda_scores, compound_scores, combined_scores, method_names)\n",
    "logger.info(f\"Step 7: Diagnostics completed.\")\n",
    "\n",
    "# 10. Output short preview\n",
    "print(df[['Primary_Method', 'Primary_Method_Score', 'Method_Confidence', 'Top_1_Method', 'Top_1_Score']].head())\n",
    "logger.info(f\"Step 8: Output preview of method assignments.\")\n",
    "\n",
    "# 11. Final saving of the DataFrame with method assignments\n",
    "df.to_csv(os.path.join(SAVE_DIR, f\"semantic_scholar_{suffix_string}_all_results.csv\"))\n",
    "logger.info(f\"Step 9: Saved final DataFrame with method assignments to {SAVE_DIR}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "literature-search-and-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
