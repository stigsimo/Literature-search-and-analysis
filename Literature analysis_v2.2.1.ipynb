{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd30d3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import configparser\n",
    "import tiktoken\n",
    "import logging\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Phrases\n",
    "import openai\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "SAVE_DIR = \"Saved_files_new\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "104bc22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI Setup and Credit Tracking\n",
    "class CreditTracker:\n",
    "    def __init__(self):\n",
    "        self.total_tokens = 0\n",
    "        self.total_cost = 0\n",
    "        self.cost_per_1k_tokens = 0.00015\n",
    "    \n",
    "    def update(self, tokens):\n",
    "        self.total_tokens += tokens\n",
    "        self.total_cost += (tokens / 1000) * self.cost_per_1k_tokens\n",
    "    \n",
    "    def get_stats(self):\n",
    "        return {\"total_tokens\": self.total_tokens, \"total_cost\": round(self.total_cost, 4)}\n",
    "\n",
    "def initialize_openai():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('config_LLM.txt')\n",
    "    api_key = config['LLM'].get('OPENAI_API_KEY')\n",
    "    model_type = config['LLM'].get('MODEL_TYPE')\n",
    "    client = openai.OpenAI(api_key=api_key)\n",
    "    return client, model_type\n",
    "\n",
    "def num_tokens_from_string(string: str, model_name: str) -> int:\n",
    "    encoding = tiktoken.encoding_for_model(model_name)\n",
    "    return len(encoding.encode(string))\n",
    "\n",
    "client, model_type = initialize_openai()\n",
    "credit_tracker = CreditTracker()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d7b1c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Processing Functions\n",
    "# %%\n",
    "def extract_keywords_from_filename(filename):\n",
    "    base = os.path.splitext(os.path.basename(filename))[0]\n",
    "    parts = base.split('_')\n",
    "    return [part for i, part in enumerate(parts) if i > 2 and part != 'results' and not part.isdigit()]\n",
    "\n",
    "def get_custom_stop_words(search_keywords=None):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words_to_keep = set()\n",
    "    if search_keywords:\n",
    "        for keyword in search_keywords:\n",
    "            keyword = keyword.lower()\n",
    "            words_to_keep.add(keyword)\n",
    "            for word in keyword.split():\n",
    "                words_to_keep.add(word)\n",
    "    stop_words = stop_words - words_to_keep\n",
    "    scientific_terms = {'et', 'al','ref','reference','references','cited','cite',\n",
    "        'fig','figure','figures','table','tables','chart','charts',\n",
    "        'published','journal','conference','proceedings','vol','volume','pp','page','pages','doi'}\n",
    "    return stop_words.union(scientific_terms)\n",
    "\n",
    "def preprocess_text(text, search_keywords=None, min_word_length=2, remove_numbers=False):\n",
    "    if not isinstance(text, (str, int, float)):\n",
    "        return ''\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    if remove_numbers:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s-]', '', text)\n",
    "    text = re.sub(r'--+', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = get_custom_stop_words(search_keywords)\n",
    "    tokens = [t for t in tokens if len(t) >= min_word_length and t not in stop_words and len(t) > 1 and not t.isdigit()]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    try:\n",
    "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    except:\n",
    "        pass\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess_dataframe(df, text_col, search_keywords, processed_col='processed_text'):\n",
    "    df[text_col] = df[text_col].fillna('').astype(str)\n",
    "    df[processed_col] = df[text_col].apply(lambda x: preprocess_text(x, search_keywords))\n",
    "    return df[df[processed_col].str.strip() != '']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ec81b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data Loading and Preprocessing\n",
    "\n",
    "filename = \"semantic_scholar_2025_02_14_reliability_resilience_power_systems_results.csv\"\n",
    "filepath = os.path.join(\"Saved_files\", filename)\n",
    "df = pd.read_csv(filepath, sep=\";\")\n",
    "df['text'] = df['title'].fillna('') + ' ' + df['abstract'].fillna('')\n",
    "search_keywords = extract_keywords_from_filename(filename)\n",
    "df = preprocess_dataframe(df, text_col='text', search_keywords=search_keywords)\n",
    "\n",
    "def clean_fields_of_study(s):\n",
    "    valid_fields = ['Computer Science', 'Economics', 'Engineering', 'Physics', 'Mathematics',\n",
    "                    'Medicine','Business','Environmental Science','Chemistry','Materials Science',\n",
    "                    'Geography','Biology','Geology','Political Science','Psychology','Com']\n",
    "    if pd.isna(s) or s == '[]':\n",
    "        return [\"Unknown\"]\n",
    "    if isinstance(s, str):\n",
    "        fields = [field.strip().strip(\"'\\\"\") for field in s.strip('[]').split(',')]\n",
    "        return [f if f in valid_fields else \"Unknown\" for f in fields] or [\"Unknown\"]\n",
    "    return [\"Unknown\"]\n",
    "\n",
    "df['fieldsOfStudy'] = df['fieldsOfStudy'].apply(clean_fields_of_study)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0c54a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 12:25:17,725 - INFO - collecting all words and their counts\n",
      "2025-08-19 12:25:17,726 - INFO - PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2025-08-19 12:25:20,515 - INFO - PROGRESS: at sentence #10000, processed 1528760 words and 873903 word types\n",
      "2025-08-19 12:25:23,535 - INFO - PROGRESS: at sentence #20000, processed 3012255 words and 1486274 word types\n",
      "2025-08-19 12:25:26,017 - INFO - collected 1928697 token types (unigram + bigrams) from a corpus of 4300658 words and 28934 sentences\n",
      "2025-08-19 12:25:26,019 - INFO - merged Phrases<1928697 vocab, min_count=10, threshold=50, max_vocab_size=40000000>\n",
      "2025-08-19 12:25:26,020 - INFO - Phrases lifecycle event {'msg': 'built Phrases<1928697 vocab, min_count=10, threshold=50, max_vocab_size=40000000> in 8.29s', 'datetime': '2025-08-19T12:25:26.020938', 'gensim': '4.3.2', 'python': '3.11.13 (main, Jun 12 2025, 12:41:34) [MSC v.1943 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'created'}\n",
      "2025-08-19 12:25:26,022 - INFO - collecting all words and their counts\n",
      "2025-08-19 12:25:26,022 - INFO - PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2025-08-19 12:25:32,052 - INFO - PROGRESS: at sentence #10000, processed 1474857 words and 896239 word types\n",
      "2025-08-19 12:25:39,112 - INFO - PROGRESS: at sentence #20000, processed 2897746 words and 1537900 word types\n",
      "2025-08-19 12:25:45,153 - INFO - collected 2008055 token types (unigram + bigrams) from a corpus of 4127557 words and 28934 sentences\n",
      "2025-08-19 12:25:45,155 - INFO - merged Phrases<2008055 vocab, min_count=5, threshold=50, max_vocab_size=40000000>\n",
      "2025-08-19 12:25:45,157 - INFO - Phrases lifecycle event {'msg': 'built Phrases<2008055 vocab, min_count=5, threshold=50, max_vocab_size=40000000> in 19.13s', 'datetime': '2025-08-19T12:25:45.157437', 'gensim': '4.3.2', 'python': '3.11.13 (main, Jun 12 2025, 12:41:34) [MSC v.1943 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "#Topic Modeling\n",
    "def model_topics(df, num_topics=10, num_words=25):\n",
    "    tokenized_texts = df['processed_text'].apply(lambda x: x.split()).tolist()\n",
    "    bigram = Phrases(tokenized_texts, min_count=10, threshold=50, delimiter='_')\n",
    "    trigram = Phrases(bigram[tokenized_texts], threshold=50, delimiter='_')\n",
    "    phrased = []\n",
    "    for doc in tokenized_texts:\n",
    "        bigrams_ = [w for w in bigram[doc] if '_' in w]\n",
    "        trigrams_ = [w for w in trigram[bigram[doc]] if '_' in w]\n",
    "        combined = doc + bigrams_ + trigrams_\n",
    "        phrased.append(' '.join(combined))\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 1), token_pattern=r'\\b[\\w_-]+\\b', max_df=0.95, min_df=2, max_features=10000)\n",
    "    doc_term_matrix = vectorizer.fit_transform(phrased)\n",
    "    lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "    topic_distributions = lda_model.fit_transform(doc_term_matrix)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topic_keywords = {}\n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        top_indices = topic.argsort()[:-num_words-1:-1]\n",
    "        top_words = [feature_names[i] for i in top_indices]\n",
    "        word_weights = [(feature_names[i], topic[i]) for i in top_indices]\n",
    "        topic_keywords[topic_idx] = {'top_words': top_words, 'word_weights': word_weights}\n",
    "    return lda_model, vectorizer, topic_distributions, df, topic_keywords\n",
    "\n",
    "lda_model, vectorizer, topic_distributions, df_topic, topic_keywords = model_topics(df, num_topics=10, num_words=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c37a2eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 12:31:50,882 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-19 12:31:50,896 - INFO - Augmented vocab size: 133\n"
     ]
    }
   ],
   "source": [
    "#Method Phrase Extraction\n",
    "\n",
    "# %%\n",
    "def extract_candidate_terms(df, text_col='processed_text', max_features=15000):\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 3), max_df=0.95, min_df=2, \n",
    "                                max_features=max_features, token_pattern=r'\\b[\\w-]+\\b')\n",
    "    matrix = vectorizer.fit_transform(df[text_col].fillna(''))\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    freqs = matrix.sum(axis=0).A1\n",
    "    return [term for term, freq in sorted(zip(terms, freqs), key=lambda x: x[1], reverse=True)]\n",
    "\n",
    "def get_method_phrases(corpus_terms, client, model_type, credit_tracker):\n",
    "    sample_terms = ', '.join(corpus_terms[:50])\n",
    "    prompt = f\"\"\"Here are the most frequent terms from a corpus of scientific papers:\n",
    "{sample_terms}\n",
    "From the full list: {', '.join(corpus_terms)}\n",
    "Extract ONLY the terms that represent specific methodologies, techniques, or named approaches that would actually appear in this type of engineering research. Focus on:\n",
    "- Power system analysis methods\n",
    "- Reliability analysis techniques  \n",
    "- Engineering design approaches\n",
    "- Computational methods used in power/electrical engineering\n",
    "- Statistical methods for engineering\n",
    "\n",
    "Do NOT include: generic words like \"analysis\", \"method\", \"approach\", \"design\", \"system\" by themselves.\n",
    "DO include: specific named methods like \"monte carlo simulation\", \"load flow analysis\", \"reliability assessment\", loss of load probability, probabilitstic methods, etc.\n",
    "\n",
    "Return as a simple Python list of strings, no code blocks or formatting.\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model_type,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    try:\n",
    "        return ast.literal_eval(response.choices[0].message.content)\n",
    "    except:\n",
    "        content = response.choices.message.content\n",
    "        content = content.replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", '')\n",
    "        return [term.strip() for term in content.split(',') if len(term.strip()) > 3]\n",
    "\n",
    "def clean_method_phrases_fixed(method_phrases):\n",
    "    cleaned_phrases = []\n",
    "    for phrase in method_phrases:\n",
    "        cleaned = phrase.strip().replace('``````','').replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", '').replace('\\n', ' ')\n",
    "        cleaned = ' '.join(cleaned.split())\n",
    "        if len(cleaned) > 2:\n",
    "            cleaned_phrases.append(cleaned.lower())\n",
    "    return list(set(cleaned_phrases))\n",
    "\n",
    "def stable_normalize_augment_vocab(method_phrases):\n",
    "    base = [p.lower().strip() for p in method_phrases if isinstance(p, str) and p.strip()]\n",
    "    variants = set()\n",
    "    for p in base:\n",
    "        variants.add(p)\n",
    "        variants.add(p.replace('-', ' '))\n",
    "        variants.add(p.replace('_', ' '))\n",
    "        if ' ' in p:\n",
    "            variants.add(p.replace(' ', '_'))\n",
    "        # Add domain-specific variants\n",
    "        if p == 'optimal power flow':\n",
    "            variants.update(['opf','security-constrained opf','scopf'])\n",
    "        if p == 'monte carlo simulation':\n",
    "            variants.update(['mc simulation'])\n",
    "        if p == 'load flow analysis':\n",
    "            variants.update(['power flow','ac power flow','dc power flow'])\n",
    "        if p == 'state estimation':\n",
    "            variants.update(['wls state estimation','kalman filter','extended kalman','unscented kalman'])\n",
    "        if p == 'contingency analysis':\n",
    "            variants.update(['n-1 security','n-1 contingency'])\n",
    "        if p == 'unit commitment':\n",
    "            variants.update(['economic dispatch','security-constrained unit commitment'])\n",
    "    return sorted(set(variants))\n",
    "\n",
    "candidate_terms = extract_candidate_terms(df, text_col='processed_text')\n",
    "method_phrases = get_method_phrases(candidate_terms, client, model_type, credit_tracker)\n",
    "method_phrases = clean_method_phrases_fixed(method_phrases)\n",
    "method_phrases_aug = stable_normalize_augment_vocab(method_phrases)\n",
    "logger.info(f\"Augmented vocab size: {len(method_phrases_aug)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8250461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi-Label Method Scoring\n",
    "\n",
    "# %%\n",
    "def compute_method_scores(df, vocab, processed_col='processed_text', \n",
    "                         w_tfidf=0.6, w_compound=0.4, top_k=5):\n",
    "    \"\"\"\n",
    "    Compute combined scores for all methods and return per-method columns plus top-k scores.\n",
    "    \"\"\"\n",
    "    n_docs = len(df)\n",
    "    n_methods = len(vocab)\n",
    "    \n",
    "    # TF-IDF scores\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        vocabulary=vocab, ngram_range=(1, 3), min_df=1, max_df=0.999,\n",
    "        norm='l2', token_pattern=r'\\b[\\w_-]+\\b'\n",
    "    )\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df[processed_col])\n",
    "    tfidf_scores = tfidf_matrix.toarray()  # (n_docs, n_methods)\n",
    "    \n",
    "    # Compound/proximity scores\n",
    "    compound_scores = compute_compound_scores(df, vocab, processed_col)\n",
    "    \n",
    "    # Combined scores\n",
    "    combined_scores = w_tfidf * tfidf_scores + w_compound * compound_scores\n",
    "    \n",
    "    # Create per-method columns with combined scores\n",
    "    method_columns = {}\n",
    "    for j, method in enumerate(vocab):\n",
    "        safe_name = f\"method_{method.replace(' ', '_').replace('-', '_')}\"\n",
    "        method_columns[safe_name] = combined_scores[:, j]\n",
    "    \n",
    "    # Add method score columns to dataframe\n",
    "    for col_name, scores in method_columns.items():\n",
    "        df[col_name] = scores\n",
    "    \n",
    "    # Top-k TF-IDF and LDA scores\n",
    "    tfidf_topk_idx = np.argsort(tfidf_scores, axis=1)[:, -top_k:][:, ::-1]\n",
    "    compound_topk_idx = np.argsort(compound_scores, axis=1)[:, -top_k:][:, ::-1]\n",
    "    combined_topk_idx = np.argsort(combined_scores, axis=1)[:, -top_k:][:, ::-1]\n",
    "    \n",
    "    # Create top-k columns\n",
    "    for k in range(top_k):\n",
    "        # TF-IDF top-k\n",
    "        df[f'tfidf_top_{k+1}_method'] = [vocab[idx[k]] for idx in tfidf_topk_idx]\n",
    "        df[f'tfidf_top_{k+1}_score'] = [tfidf_scores[i, idx[k]] for i, idx in enumerate(tfidf_topk_idx)]\n",
    "        \n",
    "        # Compound top-k\n",
    "        df[f'compound_top_{k+1}_method'] = [vocab[idx[k]] for idx in compound_topk_idx]\n",
    "        df[f'compound_top_{k+1}_score'] = [compound_scores[i, idx[k]] for i, idx in enumerate(compound_topk_idx)]\n",
    "        \n",
    "        # Combined top-k\n",
    "        df[f'combined_top_{k+1}_method'] = [vocab[idx[k]] for idx in combined_topk_idx]\n",
    "        df[f'combined_top_{k+1}_score'] = [combined_scores[i, idx[k]] for i, idx in enumerate(combined_topk_idx)]\n",
    "    \n",
    "    return df, combined_scores, tfidf_scores, compound_scores, vocab\n",
    "\n",
    "def compute_compound_scores(df, vocab, processed_col='processed_text', window=300, ratio_thresh=0.5):\n",
    "    \"\"\"Compute compound/proximity scores for all methods.\"\"\"\n",
    "    n_docs = len(df)\n",
    "    n_terms = len(vocab)\n",
    "    scores = np.zeros((n_docs, n_terms), dtype=np.float32)\n",
    "    \n",
    "    docs = df[processed_col].fillna('').str.lower().tolist()\n",
    "    \n",
    "    for j, phrase in enumerate(vocab):\n",
    "        phrase_l = phrase.lower()\n",
    "        phrase_words = [w for w in phrase_l.split() if len(w) > 0]\n",
    "        sig_words = [w for w in phrase_words if len(w) > 3]\n",
    "        \n",
    "        for i, text in enumerate(docs):\n",
    "            if phrase_l in text:\n",
    "                scores[i, j] = 1.0\n",
    "                continue\n",
    "                \n",
    "            if len(phrase_words) > 1:\n",
    "                # Coverage score\n",
    "                present = sum(1 for w in sig_words if w in text) if sig_words else 0\n",
    "                coverage = present / len(sig_words) if sig_words else 0.0\n",
    "                \n",
    "                # Proximity check\n",
    "                prox_hit = False\n",
    "                for k in range(len(phrase_words)-1):\n",
    "                    w1, w2 = phrase_words[k], phrase_words[k+1]\n",
    "                    pos = text.find(w1)\n",
    "                    if pos >= 0:\n",
    "                        nearby = text[pos:pos+window]\n",
    "                        if w2 in nearby:\n",
    "                            prox_hit = True\n",
    "                            break\n",
    "                \n",
    "                if coverage >= ratio_thresh or prox_hit:\n",
    "                    scores[i, j] = max(scores[i, j], min(1.0, 0.6 + 0.4*coverage))\n",
    "            else:\n",
    "                # Single technical term\n",
    "                if len(phrase_l) > 6 and phrase_l in text:\n",
    "                    scores[i, j] = max(scores[i, j], 0.7)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def assign_primary_method_and_confidence(df, combined_scores, vocab, \n",
    "                                       th_super=0.85, th_high=0.6, th_low=0.2):\n",
    "    \"\"\"Assign primary method and confidence level to each document.\"\"\"\n",
    "    n_docs = len(df)\n",
    "    primary_methods = []\n",
    "    confidences = []\n",
    "    \n",
    "    for i in range(n_docs):\n",
    "        scores = combined_scores[i]\n",
    "        max_idx = np.argmax(scores)\n",
    "        max_score = scores[max_idx]\n",
    "        best_method = vocab[max_idx]\n",
    "        \n",
    "        # Determine confidence\n",
    "        if max_score >= th_super:\n",
    "            confidence = 'super_high'\n",
    "        elif max_score >= th_high:\n",
    "            confidence = 'high'\n",
    "        elif max_score >= th_low:\n",
    "            confidence = 'low'\n",
    "        else:\n",
    "            confidence = 'not_detected'\n",
    "            best_method = 'LowConfidence'\n",
    "        \n",
    "        primary_methods.append(best_method)\n",
    "        confidences.append(confidence)\n",
    "    \n",
    "    df['Primary_Method'] = primary_methods\n",
    "    df['Method_Confidence'] = confidences\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f39ddc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 12:31:50,943 - INFO - Computing multi-label method scores...\n",
      "C:\\Users\\STSI\\AppData\\Local\\Temp\\ipykernel_45252\\4116996447.py:127: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Primary_Method'] = primary_methods\n",
      "C:\\Users\\STSI\\AppData\\Local\\Temp\\ipykernel_45252\\4116996447.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Method_Confidence'] = confidences\n"
     ]
    }
   ],
   "source": [
    "# Execute Method Scoring\n",
    "logger.info(\"Computing multi-label method scores...\")\n",
    "df, combined_scores, tfidf_scores, compound_scores, vocab = compute_method_scores(\n",
    "    df, method_phrases_aug, processed_col='processed_text', \n",
    "    w_tfidf=0.6, w_compound=0.4, top_k=5\n",
    ")\n",
    "\n",
    "# Assign primary method and confidence\n",
    "df = assign_primary_method_and_confidence(\n",
    "    df, combined_scores, vocab, \n",
    "    th_super=0.85, th_high=0.6, th_low=0.2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dad1367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== METHOD ASSIGNMENT DIAGNOSTICS ===\n",
      "Total documents: 28934\n",
      "Total method phrases: 133\n",
      "\n",
      "Coverage:\n",
      "  TF-IDF coverage: 3775/28934 (13.0%)\n",
      "  Compound coverage: 28279/28934 (97.7%)\n",
      "  Combined coverage: 28279/28934 (97.7%)\n",
      "\n",
      "Confidence distribution:\n",
      "  low: 24504 (84.7%)\n",
      "  super_high: 3563 (12.3%)\n",
      "  not_detected: 655 (2.3%)\n",
      "  high: 212 (0.7%)\n",
      "\n",
      "Top 15 assigned methods:\n",
      "  ac power flow: 6177\n",
      "  mc simulation: 4424\n",
      "  performance based design: 2745\n",
      "  distribution system reliability assessment: 1392\n",
      "  dynamic line rating: 1125\n",
      "  power flow: 974\n",
      "  reliability distribution analysis: 827\n",
      "  reliability assessment: 726\n",
      "  dynamic reactive power control: 660\n",
      "  LowConfidence: 655\n",
      "  model predictive control: 521\n",
      "  genetic algorithm: 492\n",
      "  multi objective optimization: 486\n",
      "  hybrid optimization: 471\n",
      "  techno economic analysis: 441\n",
      "\n",
      "Score statistics:\n",
      "  Combined scores - Mean: 0.0278, Std: 0.0958\n",
      "  TF-IDF scores - Mean: 0.0011, Std: 0.0313\n",
      "  Compound scores - Mean: 0.0678, Std: 0.2284\n"
     ]
    }
   ],
   "source": [
    "#Diagnostics and Results\n",
    "\n",
    "# %%\n",
    "def comprehensive_diagnostics(df, combined_scores, tfidf_scores, compound_scores, vocab):\n",
    "    n_docs = len(df)\n",
    "    \n",
    "    print(\"=== METHOD ASSIGNMENT DIAGNOSTICS ===\")\n",
    "    print(f\"Total documents: {n_docs}\")\n",
    "    print(f\"Total method phrases: {len(vocab)}\")\n",
    "    \n",
    "    # Coverage statistics\n",
    "    tfidf_nonzero = (tfidf_scores > 0).any(axis=1).sum()\n",
    "    compound_nonzero = (compound_scores > 0).any(axis=1).sum()\n",
    "    combined_nonzero = (combined_scores > 0).any(axis=1).sum()\n",
    "    \n",
    "    print(f\"\\nCoverage:\")\n",
    "    print(f\"  TF-IDF coverage: {tfidf_nonzero}/{n_docs} ({100*tfidf_nonzero/n_docs:.1f}%)\")\n",
    "    print(f\"  Compound coverage: {compound_nonzero}/{n_docs} ({100*compound_nonzero/n_docs:.1f}%)\")\n",
    "    print(f\"  Combined coverage: {combined_nonzero}/{n_docs} ({100*combined_nonzero/n_docs:.1f}%)\")\n",
    "    \n",
    "    # Confidence distribution\n",
    "    if 'Method_Confidence' in df.columns:\n",
    "        conf_dist = df['Method_Confidence'].value_counts()\n",
    "        print(f\"\\nConfidence distribution:\")\n",
    "        for conf, count in conf_dist.items():\n",
    "            print(f\"  {conf}: {count} ({100*count/n_docs:.1f}%)\")\n",
    "    \n",
    "    # Top methods\n",
    "    if 'Primary_Method' in df.columns:\n",
    "        method_dist = df['Primary_Method'].value_counts().head(15)\n",
    "        print(f\"\\nTop 15 assigned methods:\")\n",
    "        for method, count in method_dist.items():\n",
    "            print(f\"  {method}: {count}\")\n",
    "    \n",
    "    # Score statistics\n",
    "    print(f\"\\nScore statistics:\")\n",
    "    print(f\"  Combined scores - Mean: {combined_scores.mean():.4f}, Std: {combined_scores.std():.4f}\")\n",
    "    print(f\"  TF-IDF scores - Mean: {tfidf_scores.mean():.4f}, Std: {tfidf_scores.std():.4f}\")\n",
    "    print(f\"  Compound scores - Mean: {compound_scores.mean():.4f}, Std: {compound_scores.std():.4f}\")\n",
    "\n",
    "comprehensive_diagnostics(df, combined_scores, tfidf_scores, compound_scores, vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d9ed0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to Saved_files_new\\semantic_scholar_2025_08_19_multilabel_methods.csv\n",
      "API token usage: {'total_tokens': 0, 'total_cost': 0}\n",
      "\n",
      "Sample results:\n",
      "                               Primary_Method Method_Confidence  \\\n",
      "0                               ac power flow               low   \n",
      "1            reliability centered maintenance               low   \n",
      "2  distribution system reliability assessment               low   \n",
      "3                              control theory               low   \n",
      "4                               ac power flow               low   \n",
      "\n",
      "                        combined_top_1_method  combined_top_1_score  \\\n",
      "0                               ac power flow              0.320000   \n",
      "1            reliability centered maintenance              0.346667   \n",
      "2  distribution system reliability assessment              0.360000   \n",
      "3                              control theory              0.400000   \n",
      "4                               ac power flow              0.400000   \n",
      "\n",
      "     tfidf_top_1_method  tfidf_top_1_score  \\\n",
      "0  wls state estimation                0.0   \n",
      "1  wls state estimation                0.0   \n",
      "2  wls state estimation                0.0   \n",
      "3  wls state estimation                0.0   \n",
      "4  wls state estimation                0.0   \n",
      "\n",
      "                        compound_top_1_method  compound_top_1_score  \n",
      "0                               ac power flow              0.800000  \n",
      "1            reliability centered maintenance              0.866667  \n",
      "2  distribution system reliability assessment              0.900000  \n",
      "3                              control theory              1.000000  \n",
      "4                               ac power flow              1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Save Results\n",
    "current_date = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "output_filename = os.path.join(SAVE_DIR, f\"semantic_scholar_{current_date}_multilabel_methods.csv\")\n",
    "df.to_csv(output_filename, sep=';', encoding='utf-8', quoting=csv.QUOTE_NONNUMERIC, escapechar='\\\\')\n",
    "\n",
    "print(f\"Results saved to {output_filename}\")\n",
    "print(f\"API token usage: {credit_tracker.get_stats()}\")\n",
    "\n",
    "# Display sample results\n",
    "print(\"\\nSample results:\")\n",
    "display_cols = ['Primary_Method', 'Method_Confidence', 'combined_top_1_method', 'combined_top_1_score', \n",
    "                'tfidf_top_1_method', 'tfidf_top_1_score', 'compound_top_1_method', 'compound_top_1_score']\n",
    "available_cols = [col for col in display_cols if col in df.columns]\n",
    "print(df[available_cols].head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "literature-search-and-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
