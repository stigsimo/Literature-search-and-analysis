{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74701ba5",
   "metadata": {},
   "source": [
    "# Software mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a4856d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import anthropic\n",
    "import json\n",
    "import time\n",
    "import configparser\n",
    "import tiktoken\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, asdict\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ed2db008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "def initialize_openai():\n",
    "    \"\"\"Initialize OpenAI client from config file\"\"\"\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('config_LLM.txt')\n",
    "    api_key = config['LLM'].get('OPENAI_API_KEY')\n",
    "    model_type = config['LLM'].get('MODEL_TYPE_adv')\n",
    "    client = openai.OpenAI(api_key=api_key)\n",
    "    return client, model_type\n",
    "\n",
    "def initialize_anthropic():\n",
    "    \"\"\"Initialize Anthropic client from config file\"\"\"\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('config_LLM.txt')\n",
    "    api_key = config['LLM'].get('ANTHROPIC_API_KEY')\n",
    "    client = anthropic.Anthropic(api_key=api_key) if api_key else None\n",
    "    return client\n",
    "\n",
    "def initialize_google():\n",
    "    \"\"\"Initialize Google Gemini client from config file\"\"\"\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('config_LLM.txt')\n",
    "    api_key = config['LLM'].get('GOOGLE_API_KEY')\n",
    "    if api_key:\n",
    "        genai.configure(api_key=api_key)\n",
    "        return True\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "76adce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# TOKEN COUNTING UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def num_tokens_from_string(string: str, model_name: str) -> int:\n",
    "    \"\"\"\n",
    "    Get token count with fallback for unsupported models.\n",
    "    Updated for GPT-5 series and other new models.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model_name)\n",
    "        return len(encoding.encode(string))\n",
    "    except KeyError:\n",
    "        # Fallback for unsupported models\n",
    "        if model_name.startswith('gpt-5'):\n",
    "            # Use o200k_base encoding for GPT-5 series\n",
    "            encoding = tiktoken.get_encoding(\"o200k_base\")\n",
    "            return len(encoding.encode(string))\n",
    "        elif model_name.startswith('gpt-4'):\n",
    "            # Use cl100k_base for GPT-4 series\n",
    "            encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "            return len(encoding.encode(string))\n",
    "        elif model_name.startswith('claude'):\n",
    "            # Approximate for Claude (roughly 3.5 chars per token)\n",
    "            return int(len(string) / 3.5)\n",
    "        else:\n",
    "            # General approximation: 4 chars per token\n",
    "            return len(string) // 4\n",
    "\n",
    "def count_tokens_in_messages(messages: List[Dict], model: str) -> int:\n",
    "    \"\"\"Count tokens in a list of messages\"\"\"\n",
    "    total_tokens = 0\n",
    "    for message in messages:\n",
    "        # Count tokens in content\n",
    "        if isinstance(message.get('content'), str):\n",
    "            total_tokens += num_tokens_from_string(message['content'], model)\n",
    "        # Add overhead for message structure (role, etc.)\n",
    "        total_tokens += 4  # Approximate overhead per message\n",
    "    total_tokens += 3  # Add overhead for prompt formatting\n",
    "    return total_tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f4092181",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CREDIT TRACKING SYSTEM\n",
    "# ============================================================================\n",
    "\n",
    "class CreditTracker:\n",
    "    \"\"\"\n",
    "    Enhanced credit tracker supporting multiple LLM providers with their specific pricing.\n",
    "    Updated with October 2025 pricing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Pricing per 1M tokens (as of October 2025)\n",
    "    PRICING = {\n",
    "        # OpenAI models\n",
    "        'gpt-5': {'input': 0.625, 'output': 5.00, 'cached_input': 0.0625},\n",
    "        'gpt-5-mini': {'input': 0.125, 'output': 1.00, 'cached_input': 0.0125},\n",
    "        'gpt-5-nano': {'input': 0.025, 'output': 0.20, 'cached_input': 0.0025},\n",
    "        'gpt-4o': {'input': 1.25, 'output': 5.00},\n",
    "        'gpt-4o-mini': {'input': 0.075, 'output': 0.30},\n",
    "        'gpt-4.1': {'input': 1.00, 'output': 4.00},\n",
    "        'gpt-4.1-mini': {'input': 0.20, 'output': 0.80},\n",
    "        'o1': {'input': 7.50, 'output': 30.00},\n",
    "        'o1-mini': {'input': 0.55, 'output': 2.20},\n",
    "\n",
    "        # Claude models (Anthropic)\n",
    "        'claude-3-haiku': {'input': 0.25, 'output': 1.25},\n",
    "        'claude-3.5-haiku': {'input': 0.25, 'output': 1.25},\n",
    "        'claude-3.5-sonnet': {'input': 3.00, 'output': 15.00},\n",
    "        'claude-sonnet-4-20250514': {'input': 3.00, 'output': 15.00},\n",
    "        'claude-3-opus': {'input': 15.00, 'output': 75.00},\n",
    "\n",
    "        # Google Gemini models\n",
    "        'gemini-1.5-pro': {'input': 1.25, 'output': 5.00},\n",
    "        'gemini-1.5-flash': {'input': 0.075, 'output': 0.30},\n",
    "        'gemini-2.0-flash-exp': {'input': 0.0, 'output': 0.0},\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        self.total_input_tokens = 0\n",
    "        self.total_output_tokens = 0\n",
    "        self.total_cached_tokens = 0\n",
    "        self.total_cost = 0\n",
    "        self.model_usage = {}  # Track usage per model\n",
    "        self.call_count = 0\n",
    "\n",
    "    def update(self, \n",
    "               model: str, \n",
    "               input_tokens: int, \n",
    "               output_tokens: int,\n",
    "               cached_tokens: int = 0):\n",
    "        \"\"\"\n",
    "        Update tracker with token usage for a specific model.\n",
    "\n",
    "        Args:\n",
    "            model: Model name (e.g., 'gpt-4o', 'claude-3.5-sonnet')\n",
    "            input_tokens: Number of input tokens\n",
    "            output_tokens: Number of output tokens\n",
    "            cached_tokens: Number of cached input tokens (if applicable)\n",
    "        \"\"\"\n",
    "        # Update totals\n",
    "        self.total_input_tokens += input_tokens\n",
    "        self.total_output_tokens += output_tokens\n",
    "        self.total_cached_tokens += cached_tokens\n",
    "        self.call_count += 1\n",
    "\n",
    "        # Get pricing for this model\n",
    "        pricing = self.PRICING.get(model, {'input': 0.00015, 'output': 0.0006})  # Default fallback\n",
    "\n",
    "        # Calculate cost\n",
    "        input_cost = (input_tokens / 1_000_000) * pricing['input']\n",
    "        output_cost = (output_tokens / 1_000_000) * pricing['output']\n",
    "        cached_cost = (cached_tokens / 1_000_000) * pricing.get('cached_input', pricing['input'])\n",
    "\n",
    "        call_cost = input_cost + output_cost + cached_cost\n",
    "        self.total_cost += call_cost\n",
    "\n",
    "        # Track per-model usage\n",
    "        if model not in self.model_usage:\n",
    "            self.model_usage[model] = {\n",
    "                'calls': 0,\n",
    "                'input_tokens': 0,\n",
    "                'output_tokens': 0,\n",
    "                'cached_tokens': 0,\n",
    "                'cost': 0\n",
    "            }\n",
    "\n",
    "        self.model_usage[model]['calls'] += 1\n",
    "        self.model_usage[model]['input_tokens'] += input_tokens\n",
    "        self.model_usage[model]['output_tokens'] += output_tokens\n",
    "        self.model_usage[model]['cached_tokens'] += cached_tokens\n",
    "        self.model_usage[model]['cost'] += call_cost\n",
    "\n",
    "    def get_stats(self):\n",
    "        \"\"\"Get comprehensive statistics about API usage and costs\"\"\"\n",
    "        return {\n",
    "            \"total_calls\": self.call_count,\n",
    "            \"total_input_tokens\": self.total_input_tokens,\n",
    "            \"total_output_tokens\": self.total_output_tokens,\n",
    "            \"total_cached_tokens\": self.total_cached_tokens,\n",
    "            \"total_tokens\": self.total_input_tokens + self.total_output_tokens,\n",
    "            \"total_cost\": round(self.total_cost, 4),\n",
    "            \"average_cost_per_call\": round(self.total_cost / max(self.call_count, 1), 4),\n",
    "            \"model_breakdown\": {\n",
    "                model: {\n",
    "                    'calls': stats['calls'],\n",
    "                    'total_tokens': stats['input_tokens'] + stats['output_tokens'],\n",
    "                    'cost': round(stats['cost'], 4)\n",
    "                }\n",
    "                for model, stats in self.model_usage.items()\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def print_summary(self):\n",
    "        \"\"\"Print a formatted summary of usage\"\"\"\n",
    "        stats = self.get_stats()\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"API USAGE SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Total API Calls: {stats['total_calls']}\")\n",
    "        print(f\"Total Tokens: {stats['total_tokens']:,}\")\n",
    "        print(f\"  - Input: {stats['total_input_tokens']:,}\")\n",
    "        print(f\"  - Output: {stats['total_output_tokens']:,}\")\n",
    "        print(f\"  - Cached: {stats['total_cached_tokens']:,}\")\n",
    "        print(f\"\\nTotal Cost: ${stats['total_cost']:.4f}\")\n",
    "        print(f\"Average Cost per Call: ${stats['average_cost_per_call']:.4f}\")\n",
    "\n",
    "        if self.model_usage:\n",
    "            print(\"\\nBreakdown by Model:\")\n",
    "            print(\"-\" * 60)\n",
    "            for model, breakdown in stats['model_breakdown'].items():\n",
    "                print(f\"  {model}:\")\n",
    "                print(f\"    Calls: {breakdown['calls']}\")\n",
    "                print(f\"    Tokens: {breakdown['total_tokens']:,}\")\n",
    "                print(f\"    Cost: ${breakdown['cost']:.4f}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390050ac",
   "metadata": {},
   "source": [
    "## The method assessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a0631f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ASSESSMENT DATA STRUCTURES\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class AssessmentResult:\n",
    "    \"\"\"Store assessment results from a single LLM\"\"\"\n",
    "    software: str\n",
    "    method: str\n",
    "    rank: int\n",
    "    reasoning: str\n",
    "    sources: List[str]\n",
    "    llm_provider: str\n",
    "    input_tokens: int = 0\n",
    "    output_tokens: int = 0\n",
    "\n",
    "@dataclass\n",
    "class ConsensusResult:\n",
    "    \"\"\"Store consensus results across multiple LLMs\"\"\"\n",
    "    software: str\n",
    "    method: str\n",
    "    final_rank: int\n",
    "    confidence: float\n",
    "    individual_ranks: Dict[str, int]\n",
    "    individual_reasoning: Dict[str, str]\n",
    "    individual_sources: Dict[str, List[str]]\n",
    "    agreement_level: str\n",
    "    total_tokens: int = 0\n",
    "    total_cost: float = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7397873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SOFTWARE METHOD ASSESSOR WITH CREDIT TRACKING\n",
    "# ============================================================================\n",
    "class SoftwareMethodAssessor:\n",
    "    \"\"\"\n",
    "    Assess software implementation support for specific methods using multiple LLM APIs.\n",
    "    Includes comprehensive credit tracking across all providers.\n",
    "\n",
    "    Ranking scale: \n",
    "    0=no support, \n",
    "    1=limited possibility, \n",
    "    2=via APIs/extensions, \n",
    "    3=directly implemented\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 openai_api_key: str = None, \n",
    "                 anthropic_api_key: str = None,\n",
    "                 google_api_key: str = None,\n",
    "                 use_config: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize with API keys\n",
    "\n",
    "        Args:\n",
    "            openai_api_key: OpenAI API key (optional if use_config=True)\n",
    "            anthropic_api_key: Anthropic API key (optional)\n",
    "            google_api_key: Google API key (optional)\n",
    "            use_config: If True, load from config_LLM.txt file\n",
    "        \"\"\"\n",
    "        if use_config:\n",
    "            self.openai_client, self.default_model = initialize_openai()\n",
    "            self.anthropic_client = initialize_anthropic()\n",
    "            self.google_enabled = initialize_google()\n",
    "        else:\n",
    "            self.openai_client = openai.OpenAI(api_key=openai_api_key) if openai_api_key else None\n",
    "            self.anthropic_client = anthropic.Anthropic(api_key=anthropic_api_key) if anthropic_api_key else None\n",
    "            self.default_model = \"gpt-4o\"\n",
    "            if google_api_key:\n",
    "                genai.configure(api_key=google_api_key)\n",
    "                self.google_enabled = True\n",
    "            else:\n",
    "                self.google_enabled = False\n",
    "        self.default_model = \"gpt-4o\"\n",
    "\n",
    "        # Initialize credit tracker\n",
    "        self.credit_tracker = CreditTracker()\n",
    "\n",
    "        # System prompt for assessment\n",
    "       \n",
    "\n",
    "        self.system_prompt = \"\"\"You are a technical software assessment expert specialized in power systems analysis software. Your task is to evaluate the implementation support level for specific methods.\n",
    "\n",
    "Use this ranking scale:\n",
    "0 = No support (method cannot be implemented at all)\n",
    "1 = Limited possibility for implementation or extension (requires significant workarounds)\n",
    "2 = Indirectly supported through APIs or extensions (requires external tools/plugins)\n",
    "3 = Directly implemented (native feature in the software)\n",
    "\n",
    "CRITICAL: You MUST search for and provide actual references. Your assessment must be based on real, verifiable sources.\n",
    "\n",
    "For each assessment:\n",
    "1. Search for scientific papers demonstrating implementation (IEEE Xplore, ScienceDirect, arXiv, Google Scholar)\n",
    "2. Find official documentation from the software vendor or project website\n",
    "3. Look for GitHub repositories with code examples or open-source implementations\n",
    "4. Check API documentation or extension/plugin capabilities\n",
    "5. Review user forums, technical blogs, Stack Overflow, or case studies\n",
    "\n",
    "IMPORTANT: The \"sources\" field is MANDATORY and must contain:\n",
    "- Full URLs to documentation pages\n",
    "- Paper titles with DOIs or direct links\n",
    "- GitHub repository URLs with specific file paths if relevant\n",
    "- Forum discussion URLs\n",
    "- Technical blog post URLs\n",
    "\n",
    "If you cannot find ANY sources, you must state this explicitly in the reasoning and set rank to 0.\n",
    "\n",
    "Return your response in VALID JSON format with this exact structure:\n",
    "{\n",
    "    \"rank\": <0-3>,\n",
    "    \"reasoning\": \"<detailed explanation citing specific sources by number, e.g., 'According to [1], PSS/E supports...''>\",\n",
    "    \"sources\": [\n",
    "        \"https://example.com/documentation - Official PSS/E Manual on OPF\",\n",
    "        \"https://doi.org/10.1109/... - Paper title by Author et al.\",\n",
    "        \"https://github.com/org/repo/file.py - Implementation example\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "Each source must include both the URL and a brief description separated by ' - '.\n",
    "Minimum 2 sources required for ranks 2-3, minimum 1 source for rank 1.\"\"\"\n",
    "\n",
    "# Assessing functions\n",
    "    def create_assessment_prompt(self, software: str, method: str) -> str:\n",
    "        \"\"\"Create prompt for assessing a specific software-method combination\"\"\"\n",
    "        return f\"\"\"Assess the implementation support for:\n",
    "\n",
    "    Software: {software}\n",
    "    Method: {method}\n",
    "\n",
    "    You MUST search comprehensively and provide actual URLs/references for:\n",
    "\n",
    "    1. Scientific papers (IEEE Xplore, ScienceDirect, Google Scholar, arXiv) \n",
    "    Example: \"https://doi.org/10.1109/TPWRS.2020.1234567 - Optimal Power Flow Implementation in PSS/E by Smith et al.\"\n",
    "\n",
    "    2. Official {software} documentation (user manuals, technical references, API docs)\n",
    "    Example: \"https://www.siemens-energy.com/psse/docs/opf-manual.pdf - PSS/E OPF User Manual Chapter 5\"\n",
    "\n",
    "    3. GitHub repositories with code examples\n",
    "    Example: \"https://github.com/username/project/blob/main/opf.py - OPF implementation using {software} API\"\n",
    "\n",
    "    4. Technical forums (Stack Overflow, vendor forums, Reddit)\n",
    "    Example: \"https://stackoverflow.com/questions/12345678 - Discussion on {method} in {software}\"\n",
    "\n",
    "    5. Technical blogs, white papers, or case studies\n",
    "    Example: \"https://blog.example.com/psse-opf-tutorial - Tutorial on OPF implementation\"\n",
    "\n",
    "    Based on your findings, determine the implementation rank (0-3) with:\n",
    "    - Detailed reasoning that cites sources by number [1], [2], etc.\n",
    "    - Complete list of sources with URLs and descriptions\n",
    "    - Specific examples from the sources\n",
    "    - Version information if available\n",
    "\n",
    "    REMEMBER: The sources list is mandatory. If you cannot find sources, state this clearly and rank as 0.\"\"\"\n",
    "\n",
    "    def assess_with_openai(self, \n",
    "                          software: str, \n",
    "                          method: str, \n",
    "                          model: str = None) -> AssessmentResult:\n",
    "        \"\"\"\n",
    "        Assess using OpenAI's model with credit tracking\n",
    "\n",
    "        Args:\n",
    "            software: Software name\n",
    "            method: Method to assess\n",
    "            model: OpenAI model to use (defaults to configured model)\n",
    "        \"\"\"\n",
    "        if model is None:\n",
    "            model = self.default_model\n",
    "\n",
    "        try:\n",
    "            prompt = self.create_assessment_prompt(software, method)\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "\n",
    "            # Estimate input tokens\n",
    "            input_tokens = count_tokens_in_messages(messages, model)\n",
    "\n",
    "            response = self.openai_client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=0.3,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            usage = response.usage\n",
    "\n",
    "            cached_tokens = 0\n",
    "            if hasattr(usage, 'prompt_tokens_details') and usage.prompt_tokens_details:\n",
    "                    cached_tokens = getattr(usage.prompt_tokens_details, 'cached_tokens', 0)\n",
    "            # Track actual usage\n",
    "            \n",
    "            self.credit_tracker.update(\n",
    "                model=model,\n",
    "                input_tokens=usage.prompt_tokens,\n",
    "                output_tokens=usage.completion_tokens,\n",
    "                cached_tokens=cached_tokens\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "\n",
    "                # After parsing JSON:\n",
    "            rank = int(result.get(\"rank\", 0))\n",
    "            sources = result.get(\"sources\", [])\n",
    "            reasoning = result.get(\"reasoning\", \"\")\n",
    "            \n",
    "            # Validate sources\n",
    "            if rank > 0 and len(sources) == 0:\n",
    "                if debug:\n",
    "                    print(f\"WARNING: Rank {rank} assigned but no sources provided. Lowering rank to 0.\")\n",
    "                rank = 0\n",
    "                reasoning += \" [Note: Rank lowered to 0 due to lack of verifiable sources]\"\n",
    "            \n",
    "            return AssessmentResult(\n",
    "                software=software,\n",
    "                method=method,\n",
    "                rank=rank,\n",
    "                reasoning=reasoning,\n",
    "                sources=sources,\n",
    "                llm_provider=f\"openai_{model}\",\n",
    "                input_tokens=usage.prompt_tokens,\n",
    "                output_tokens=usage.completion_tokens\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with OpenAI assessment: {e}\")\n",
    "            return AssessmentResult(\n",
    "                software=software,\n",
    "                method=method,\n",
    "                rank=0,\n",
    "                reasoning=f\"Error: {str(e)}\",\n",
    "                sources=[],\n",
    "                llm_provider=f\"openai_{model}\"\n",
    "            )\n",
    "\n",
    "    def assess_with_claude(self, \n",
    "                          software: str, \n",
    "                          method: str, \n",
    "                          model: str = \"claude-sonnet-4-20250514\") -> AssessmentResult:\n",
    "        \"\"\"\n",
    "        Assess using Anthropic's Claude with credit tracking\n",
    "\n",
    "        Args:\n",
    "            software: Software name\n",
    "            method: Method to assess\n",
    "            model: Claude model to use\n",
    "        \"\"\"\n",
    "        if not self.anthropic_client:\n",
    "            raise ValueError(\"Anthropic API key not provided\")\n",
    "\n",
    "        try:\n",
    "            prompt = self.create_assessment_prompt(software, method)\n",
    "\n",
    "            # Estimate input tokens\n",
    "            input_tokens = num_tokens_from_string(self.system_prompt + prompt, model)\n",
    "\n",
    "            response = self.anthropic_client.messages.create(\n",
    "                model=model,\n",
    "                max_tokens=4096,\n",
    "                temperature=0.3,\n",
    "                system=self.system_prompt,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Track usage\n",
    "            self.credit_tracker.update(\n",
    "                model=model,\n",
    "                input_tokens=response.usage.input_tokens,\n",
    "                output_tokens=response.usage.output_tokens\n",
    "            )\n",
    "\n",
    "            # Extract JSON from response\n",
    "            content = response.content[0].text\n",
    "            content = content.strip()\n",
    "            if content.startswith('```'):\n",
    "                lines = content.split('\\n')\n",
    "                start_idx = 0\n",
    "                end_idx = len(lines)\n",
    "                for i, line in enumerate(lines):\n",
    "                    if line.strip().startswith('```'):\n",
    "                        if start_idx == 0:\n",
    "                            start_idx = i + 1\n",
    "                        else:\n",
    "                            end_idx = i\n",
    "                            break\n",
    "                content = '\\n'.join(lines[start_idx:end_idx])\n",
    "            # Try to parse JSON with fallback\n",
    "            try:\n",
    "                result = json.loads(content)\n",
    "            except json.JSONDecodeError:\n",
    "                import re\n",
    "                json_match = re.search(r'\\{[^{}]*\"rank\"[^{}]*\\}', content, re.DOTALL)\n",
    "                if json_match:\n",
    "                    result = json.loads(json_match.group())\n",
    "                else:\n",
    "                    raise ValueError(f\"Could not parse JSON from response: {content[:200]}\")\n",
    "            # After successfully parsing JSON, add validation:\n",
    "            rank = int(result.get(\"rank\", 0))\n",
    "            sources = result.get(\"sources\", [])\n",
    "            reasoning = result.get(\"reasoning\", \"\")\n",
    "            \n",
    "            # Validate that sources are provided for non-zero ranks\n",
    "            if rank > 0 and len(sources) == 0:\n",
    "                if debug:\n",
    "                    print(f\"WARNING: Rank {rank} assigned but no sources provided. Lowering rank to 0.\")\n",
    "                rank = 0\n",
    "                reasoning += \" [Note: Rank lowered to 0 due to lack of verifiable sources]\"\n",
    "            \n",
    "            # Warn if sources look generic or empty\n",
    "            if len(sources) > 0:\n",
    "                empty_sources = [s for s in sources if len(s.strip()) < 10]\n",
    "                if len(empty_sources) > 0 and debug:\n",
    "                    print(f\"WARNING: {len(empty_sources)} sources appear to be empty or too short\")\n",
    "            \n",
    "            return AssessmentResult(\n",
    "                software=software,\n",
    "                method=method,\n",
    "                rank=rank,\n",
    "                reasoning=reasoning,\n",
    "                sources=sources,\n",
    "                llm_provider=f\"claude_{model}\",\n",
    "                input_tokens=response.usage.input_tokens,\n",
    "                output_tokens=response.usage.output_tokens\n",
    "            )\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with Claude assessment: {e}\")\n",
    "            return AssessmentResult(\n",
    "                software=software,\n",
    "                method=method,\n",
    "                rank=0,\n",
    "                reasoning=f\"Error: {str(e)}\",\n",
    "                sources=[],\n",
    "                llm_provider=f\"claude_{model}\"\n",
    "            )\n",
    "\n",
    "    def assess_with_google(self, \n",
    "                        software: str, \n",
    "                        method: str, \n",
    "                        model: str = \"models/gemini-2.5-flash\",  # ← UPDATED DEFAULT\n",
    "                        debug: bool = False) -> AssessmentResult:\n",
    "        \"\"\"\n",
    "        Assess using Google Gemini with credit tracking\n",
    "\n",
    "        Args:\n",
    "            software: Software name\n",
    "            method: Method to assess\n",
    "            model: Gemini model to use (default: models/gemini-2.5-flash)\n",
    "            debug: If True, print detailed debugging information\n",
    "        \"\"\"\n",
    "        if not self.google_enabled:\n",
    "            raise ValueError(\"Google API key not provided\")\n",
    "        \n",
    "        try:\n",
    "            prompt = self.create_assessment_prompt(software, method)\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(f\"DEBUG: Google Gemini Request for {software} - {method}\")\n",
    "                print(f\"{'='*60}\")\n",
    "                print(f\"Model: {model}\")\n",
    "\n",
    "            # Ensure model has correct prefix\n",
    "            if not model.startswith('models/'):\n",
    "                model = f\"models/{model}\"\n",
    "            \n",
    "            # Create the model\n",
    "            gemini_model = genai.GenerativeModel(\n",
    "                model_name=model,\n",
    "                generation_config={\n",
    "                    \"temperature\": 0.3,\n",
    "                    \"max_output_tokens\": 2048,\n",
    "                },\n",
    "                system_instruction=self.system_prompt\n",
    "            )\n",
    "            \n",
    "            # Generate response\n",
    "            response = gemini_model.generate_content(\n",
    "                prompt,\n",
    "                generation_config={\n",
    "                    \"temperature\": 0.3,\n",
    "                    \"max_output_tokens\": 2048,\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"DEBUG: Successfully used model: {model}\")\n",
    "            \n",
    "            # Extract token counts\n",
    "            try:\n",
    "                input_tokens = response.usage_metadata.prompt_token_count\n",
    "                output_tokens = response.usage_metadata.candidates_token_count\n",
    "            except AttributeError:\n",
    "                # Fallback if usage_metadata is not available\n",
    "                input_tokens = int(len(prompt.split()) * 1.3)\n",
    "                output_tokens = int(len(response.text.split()) * 1.3)\n",
    "                if debug:\n",
    "                    print(\"DEBUG: Token usage not available, using approximation\")\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"DEBUG: Tokens - Input: {input_tokens}, Output: {output_tokens}\")\n",
    "            \n",
    "            # Track usage\n",
    "            self.credit_tracker.update(\n",
    "                model=model,\n",
    "                input_tokens=int(input_tokens),\n",
    "                output_tokens=int(output_tokens)\n",
    "            )\n",
    "            \n",
    "            content = response.text\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"\\nDEBUG: Raw Gemini Response:\")\n",
    "                print(f\"{'-'*60}\")\n",
    "                print(content[:500])  # Print first 500 chars\n",
    "                print(f\"{'-'*60}\")\n",
    "            \n",
    "            # Clean and parse JSON (same as Claude method)\n",
    "            content = content.strip()\n",
    "            if content.startswith('```'):\n",
    "                if debug:\n",
    "                    print(f\"\\nDEBUG: Detected markdown code block, cleaning...\")\n",
    "                lines = content.split('\\n')\n",
    "                start_idx = 0\n",
    "                end_idx = len(lines)\n",
    "                for i, line in enumerate(lines):\n",
    "                    if line.strip().startswith('```'):\n",
    "                        if start_idx == 0:\n",
    "                            start_idx = i + 1\n",
    "                        else:\n",
    "                            end_idx = i\n",
    "                            break\n",
    "                content = '\\n'.join(lines[start_idx:end_idx])\n",
    "            \n",
    "            # Try to parse JSON\n",
    "            result = None\n",
    "            try:\n",
    "                result = json.loads(content)\n",
    "                if debug:\n",
    "                    print(f\"\\nDEBUG: Successfully parsed JSON\")\n",
    "                    print(f\"DEBUG: Rank: {result.get('rank', 'NOT FOUND')}\")\n",
    "            except json.JSONDecodeError as je:\n",
    "                if debug:\n",
    "                    print(f\"\\nDEBUG: JSON parsing failed: {je}\")\n",
    "                    print(f\"DEBUG: Attempting regex extraction...\")\n",
    "                \n",
    "                import re\n",
    "                json_match = re.search(r'\\{[^{}]*\"rank\"[^{}]*\\}', content, re.DOTALL)\n",
    "                if json_match:\n",
    "                    json_str = json_match.group()\n",
    "                    result = json.loads(json_str)\n",
    "                else:\n",
    "                    raise ValueError(f\"Could not parse JSON from response: {content[:200]}\")\n",
    "            \n",
    "            # Validate sources\n",
    "            rank = int(result.get(\"rank\", 0))\n",
    "            sources = result.get(\"sources\", [])\n",
    "            reasoning = result.get(\"reasoning\", \"\")\n",
    "            \n",
    "            if rank > 0 and len(sources) == 0:\n",
    "                if debug:\n",
    "                    print(f\"WARNING: Rank {rank} assigned but no sources provided. Lowering rank to 0.\")\n",
    "                rank = 0\n",
    "                reasoning += \" [Note: Rank lowered to 0 due to lack of verifiable sources]\"\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"\\nDEBUG: Final rank: {rank}\")\n",
    "                print(f\"DEBUG: Number of sources: {len(sources)}\")\n",
    "                print(f\"{'='*60}\\n\")\n",
    "\n",
    "            return AssessmentResult(\n",
    "                software=software,\n",
    "                method=method,\n",
    "                rank=rank,\n",
    "                reasoning=reasoning,\n",
    "                sources=sources,\n",
    "                llm_provider=f\"google_{model.replace('models/', '')}\",\n",
    "                input_tokens=int(input_tokens),\n",
    "                output_tokens=int(output_tokens)\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with Google assessment: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return AssessmentResult(\n",
    "                software=software,\n",
    "                method=method,\n",
    "                rank=0,\n",
    "                reasoning=f\"Error: {str(e)}\",\n",
    "                sources=[],\n",
    "                llm_provider=f\"google_{model.replace('models/', '')}\"\n",
    "            )\n",
    "\n",
    "# compare different results within same run\n",
    "    def calculate_confidence(self, ranks: List[int]) -> Tuple[float, str]:\n",
    "        \"\"\"\n",
    "        Calculate confidence based on agreement between LLM assessments\n",
    "\n",
    "        Returns:\n",
    "            confidence score (0-1) and agreement level description\n",
    "        \"\"\"\n",
    "        if len(ranks) < 2:\n",
    "            return 0.5, \"single_assessment\"\n",
    "\n",
    "        rank_counts = Counter(ranks)\n",
    "        most_common_count = rank_counts.most_common(1)[0][1]\n",
    "        total_assessments = len(ranks)\n",
    "\n",
    "        agreement = most_common_count / total_assessments\n",
    "        variance = np.var(ranks)\n",
    "\n",
    "        # Combined confidence metric\n",
    "        confidence = agreement * (1 - variance / 4)\n",
    "\n",
    "        if agreement == 1.0:\n",
    "            level = \"perfect_agreement\"\n",
    "        elif agreement >= 0.67:\n",
    "            level = \"strong_agreement\"\n",
    "        elif agreement >= 0.5:\n",
    "            level = \"moderate_agreement\"\n",
    "        else:\n",
    "            level = \"low_agreement\"\n",
    "\n",
    "        return confidence, level\n",
    "\n",
    "    def assess_software_method(self, \n",
    "                               software: str, \n",
    "                               method: str,\n",
    "                               use_openai: bool = True,\n",
    "                               use_claude: bool = True,\n",
    "                               use_google:bool = True,\n",
    "                               openai_model: str = None,\n",
    "                               claude_model: str = \"claude-sonnet-4-20250514\",\n",
    "                               google_model: str = \"models/gemini-2.5-flash\") -> ConsensusResult:\n",
    "        \"\"\"\n",
    "        Assess a single software-method combination using multiple LLMs\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        total_tokens = 0\n",
    "\n",
    "        if use_openai and self.openai_client:\n",
    "            print(f\"Assessing {software} - {method} with OpenAI...\")\n",
    "            result = self.assess_with_openai(software, method, openai_model)\n",
    "            results.append(result)\n",
    "            total_tokens += result.input_tokens + result.output_tokens\n",
    "            time.sleep(1)\n",
    "\n",
    "        if use_claude and self.anthropic_client:\n",
    "            print(f\"Assessing {software} - {method} with Claude...\")\n",
    "            result = self.assess_with_claude(software, method, claude_model)\n",
    "            results.append(result)\n",
    "            total_tokens += result.input_tokens + result.output_tokens\n",
    "            time.sleep(1)\n",
    "        if use_google and self.google_enabled:  # ← ADD THIS BLOCK\n",
    "            print(f\"Assessing {software} - {method} with Google Gemini...\")\n",
    "            result = self.assess_with_google(software, method, google_model)\n",
    "            results.append(result)\n",
    "            total_tokens += result.input_tokens + result.output_tokens\n",
    "            time.sleep(1)\n",
    "        \n",
    "        ranks = [r.rank for r in results]\n",
    "        confidence, agreement_level = self.calculate_confidence(ranks)\n",
    "\n",
    "        rank_counts = Counter(ranks)\n",
    "        final_rank = rank_counts.most_common(1)[0][0]\n",
    "\n",
    "        individual_ranks = {r.llm_provider: r.rank for r in results}\n",
    "        individual_reasoning = {r.llm_provider: r.reasoning for r in results}\n",
    "        individual_sources = {r.llm_provider: r.sources for r in results} \n",
    "\n",
    "        # Calculate total cost\n",
    "        total_cost = sum([\n",
    "            self.credit_tracker.PRICING.get(\n",
    "                r.llm_provider.replace('openai_', '').replace('claude_', '').replace('google_', ''),\n",
    "                {'input': 0, 'output': 0}\n",
    "            )['input'] * r.input_tokens / 1_000_000 +\n",
    "            self.credit_tracker.PRICING.get(\n",
    "                r.llm_provider.replace('openai_', '').replace('claude_', '').replace('google_', ''),\n",
    "                {'input': 0, 'output': 0}\n",
    "            )['output'] * r.output_tokens / 1_000_000\n",
    "            for r in results\n",
    "        ])\n",
    "\n",
    "        return ConsensusResult(\n",
    "            software=software,\n",
    "            method=method,\n",
    "            final_rank=final_rank,\n",
    "            confidence=confidence,\n",
    "            individual_ranks=individual_ranks,\n",
    "            individual_reasoning=individual_reasoning,\n",
    "            individual_sources=individual_sources, \n",
    "            agreement_level=agreement_level,\n",
    "            total_tokens=total_tokens,\n",
    "            total_cost=total_cost\n",
    "        )\n",
    "\n",
    "    def assess_multiple(self,\n",
    "                       software_list: List[str],\n",
    "                       method_list: List[str],\n",
    "                       use_openai: bool = True,\n",
    "                       use_claude: bool = True,\n",
    "                       use_google: bool = True,\n",
    "                       openai_model: str = None,\n",
    "                       claude_model: str = \"claude-sonnet-4-20250514\",\n",
    "                       google_model: str = \"models/gemini-2.5-flash\") -> List[ConsensusResult]:\n",
    "        \"\"\"\n",
    "        Assess multiple software-method combinations with progress tracking\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        total = len(software_list) * len(method_list)\n",
    "        current = 0\n",
    "\n",
    "        print(f\"\\nStarting assessment of {total} combinations...\")\n",
    "        print(f\"Using OpenAI: {use_openai}, Using Claude: {use_claude}, Using Google: {use_google}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        for software in software_list:\n",
    "            for method in method_list:\n",
    "                current += 1\n",
    "                print(f\"\\n[{current}/{total}] Processing: {software} - {method}\")\n",
    "\n",
    "                result = self.assess_software_method(\n",
    "                    software=software,\n",
    "                    method=method,\n",
    "                    use_openai=use_openai,\n",
    "                    use_claude=use_claude,\n",
    "                    openai_model=openai_model,\n",
    "                    claude_model=claude_model,\n",
    "                    google_model=google_model\n",
    "                )\n",
    "                results.append(result)\n",
    "\n",
    "                # Show interim cost\n",
    "                stats = self.credit_tracker.get_stats()\n",
    "                print(f\"  Rank: {result.final_rank}, Confidence: {result.confidence:.2%}\")\n",
    "                print(f\"  Running cost: ${stats['total_cost']:.4f} ({stats['total_tokens']:,} tokens)\")\n",
    "\n",
    "                time.sleep(2)  # Rate limiting\n",
    "\n",
    "        return results\n",
    "\n",
    "    def print_detailed_results(self, results: List[ConsensusResult], min_confidence: float = 0.0):\n",
    "        \"\"\" Print detailed results including sources\n",
    "            Args:\n",
    "            results: List of consensus results\n",
    "            min_confidence: Minimum confidence threshold to display (0.0 to 1.0)\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"DETAILED ASSESSMENT RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for result in results:\n",
    "            if result.confidence >= min_confidence:\n",
    "                print(f\"\\n{'─'*80}\")\n",
    "                print(f\"Software: {result.software}\")\n",
    "                print(f\"Method: {result.method}\")\n",
    "                print(f\"Final Rank: {result.final_rank}\")\n",
    "                print(f\"Confidence: {result.confidence:.2%} ({result.agreement_level})\")\n",
    "                print(f\"Cost: ${result.total_cost:.4f}\")\n",
    "                \n",
    "                print(f\"\\n{'─'*40}\")\n",
    "                print(\"Individual Assessments:\")\n",
    "                print(f\"{'─'*40}\")\n",
    "                \n",
    "                for provider, rank in result.individual_ranks.items():\n",
    "                    print(f\"\\n{provider}:\")\n",
    "                    print(f\"  Rank: {rank}\")\n",
    "                    \n",
    "                    # Print reasoning (truncated)\n",
    "                    reasoning = result.individual_reasoning.get(provider, \"N/A\")\n",
    "                    if len(reasoning) > 200:\n",
    "                        print(f\"  Reasoning: {reasoning[:200]}...\")\n",
    "                    else:\n",
    "                        print(f\"  Reasoning: {reasoning}\")\n",
    "                    \n",
    "                    # Print sources\n",
    "                    sources = result.individual_sources.get(provider, [])\n",
    "                    if sources:\n",
    "                        print(f\"  Sources ({len(sources)}):\")\n",
    "                        for i, source in enumerate(sources, 1):\n",
    "                            print(f\"    [{i}] {source}\")\n",
    "                    else:\n",
    "                        print(f\"  Sources: None provided\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "\n",
    "        \"\"\"Create summary statistics from results\"\"\"\n",
    "        total = len(results)\n",
    "        \n",
    "        confidence_levels = {\n",
    "            \"high_confidence\": sum(1 for r in results if r.confidence >= 0.8),\n",
    "            \"medium_confidence\": sum(1 for r in results if 0.5 <= r.confidence < 0.8),\n",
    "            \"low_confidence\": sum(1 for r in results if r.confidence < 0.5)\n",
    "        }\n",
    "        \n",
    "        agreement_levels = Counter([r.agreement_level for r in results])\n",
    "        \n",
    "        rank_distribution = Counter([r.final_rank for r in results])\n",
    "        \n",
    "        return {\n",
    "            \"total_assessments\": total,\n",
    "            \"confidence_distribution\": confidence_levels,\n",
    "            \"agreement_distribution\": dict(agreement_levels),\n",
    "            \"rank_distribution\": dict(rank_distribution),\n",
    "            \"average_confidence\": np.mean([r.confidence for r in results])\n",
    "        }\n",
    "\n",
    "    def create_sources_summary(self, results: List[ConsensusResult]) -> Dict:\n",
    "        \"\"\"Create summary of sources across all assessments\"\"\"\n",
    "        total_sources = 0\n",
    "        assessments_with_sources = 0\n",
    "        assessments_without_sources = 0\n",
    "        sources_by_provider = {}\n",
    "        \n",
    "        for result in results:\n",
    "            has_sources = False\n",
    "            for provider, sources in result.individual_sources.items():\n",
    "                if provider not in sources_by_provider:\n",
    "                    sources_by_provider[provider] = {\n",
    "                        'total_sources': 0,\n",
    "                        'assessments_with_sources': 0,\n",
    "                        'assessments_without_sources': 0\n",
    "                    }\n",
    "                \n",
    "                if sources and len(sources) > 0:\n",
    "                    total_sources += len(sources)\n",
    "                    sources_by_provider[provider]['total_sources'] += len(sources)\n",
    "                    sources_by_provider[provider]['assessments_with_sources'] += 1\n",
    "                    has_sources = True\n",
    "                else:\n",
    "                    sources_by_provider[provider]['assessments_without_sources'] += 1\n",
    "            \n",
    "            if has_sources:\n",
    "                assessments_with_sources += 1\n",
    "            else:\n",
    "                assessments_without_sources += 1\n",
    "        \n",
    "        return {\n",
    "            'total_sources': total_sources,\n",
    "            'assessments_with_sources': assessments_with_sources,\n",
    "            'assessments_without_sources': assessments_without_sources,\n",
    "            'average_sources_per_assessment': total_sources / max(len(results), 1),\n",
    "            'by_provider': sources_by_provider\n",
    "        }\n",
    "\n",
    "# export and save\n",
    "    def export_results(self, results: List[ConsensusResult], filename: str):\n",
    "        \"\"\"Export results to JSON file\"\"\"\n",
    "        export_data = [asdict(r) for r in results]\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(export_data, f, indent=2)\n",
    "        print(f\"\\nResults exported to {filename}\")\n",
    "# create the comparison reesults report\n",
    "    def create_summary_report(self, results: List[ConsensusResult]) -> Dict:\n",
    "        \"\"\"Create summary statistics from results\"\"\"\n",
    "        total = len(results)\n",
    "\n",
    "        confidence_levels = {\n",
    "            \"high_confidence\": sum(1 for r in results if r.confidence >= 0.8),\n",
    "            \"medium_confidence\": sum(1 for r in results if 0.5 <= r.confidence < 0.8),\n",
    "            \"low_confidence\": sum(1 for r in results if r.confidence < 0.5)\n",
    "        }\n",
    "\n",
    "        agreement_levels = Counter([r.agreement_level for r in results])\n",
    "        rank_distribution = Counter([r.final_rank for r in results])\n",
    "\n",
    "        return {\n",
    "            \"total_assessments\": total,\n",
    "            \"confidence_distribution\": confidence_levels,\n",
    "            \"agreement_distribution\": dict(agreement_levels),\n",
    "            \"rank_distribution\": dict(rank_distribution),\n",
    "            \"average_confidence\": np.mean([r.confidence for r in results]),\n",
    "            \"total_cost\": sum(r.total_cost for r in results),\n",
    "            \"total_tokens\": sum(r.total_tokens for r in results)\n",
    "        }\n",
    "\n",
    "    def merge_assessment_results(self, \n",
    "                                 *result_files: str,\n",
    "                                 output_file: str = \"merged_assessment.json\",\n",
    "                                 merge_strategy: str = \"union\",\n",
    "                                 prefer_higher_confidence: bool = True) -> List[ConsensusResult]:\n",
    "        \"\"\"\n",
    "        Merge multiple assessment result JSON files\n",
    "        \n",
    "        Args:\n",
    "            *result_files: Variable number of JSON file paths to merge\n",
    "            output_file: Output file for merged results\n",
    "            merge_strategy: How to handle duplicates:\n",
    "                - \"union\": Keep all assessments from all LLMs (default)\n",
    "                - \"replace\": Later files replace earlier ones completely\n",
    "                - \"best\": Keep only the highest confidence assessment\n",
    "            prefer_higher_confidence: If True, prefer assessments with more LLM opinions\n",
    "        \n",
    "        Returns:\n",
    "            List of merged ConsensusResult objects\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"MERGING ASSESSMENT RESULTS\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Strategy: {merge_strategy}\")\n",
    "        print(f\"Input files: {len(result_files)}\")\n",
    "        \n",
    "        # Dictionary to store merged results: (software, method) -> result data\n",
    "        merged_data = {}\n",
    "        \n",
    "        # Track which LLMs assessed each combination\n",
    "        assessment_tracking = {}\n",
    "        \n",
    "        # Load and process each file\n",
    "        for file_idx, file_path in enumerate(result_files, 1):\n",
    "            print(f\"\\nProcessing file {file_idx}/{len(result_files)}: {file_path}\")\n",
    "            \n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    results = json.load(f)\n",
    "                \n",
    "                print(f\"  Loaded {len(results)} assessments\")\n",
    "                \n",
    "                for result in results:\n",
    "                    software = result['software']\n",
    "                    method = result['method']\n",
    "                    key = (software, method)\n",
    "                    \n",
    "                    # Track this assessment\n",
    "                    if key not in assessment_tracking:\n",
    "                        assessment_tracking[key] = {\n",
    "                            'files': [],\n",
    "                            'llm_providers': set()\n",
    "                        }\n",
    "                    \n",
    "                    assessment_tracking[key]['files'].append(file_path)\n",
    "                    assessment_tracking[key]['llm_providers'].update(result['individual_ranks'].keys())\n",
    "                    \n",
    "                    if key not in merged_data:\n",
    "                        # First time seeing this combination\n",
    "                        merged_data[key] = result\n",
    "                    else:\n",
    "                        # Combination exists, apply merge strategy\n",
    "                        if merge_strategy == \"union\":\n",
    "                            merged_data[key] = self._merge_union(merged_data[key], result)\n",
    "                        elif merge_strategy == \"replace\":\n",
    "                            merged_data[key] = result  # Later file wins\n",
    "                        elif merge_strategy == \"best\":\n",
    "                            merged_data[key] = self._merge_best(merged_data[key], result, prefer_higher_confidence)\n",
    "                        else:\n",
    "                            raise ValueError(f\"Unknown merge strategy: {merge_strategy}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR loading {file_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Print merge summary\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"MERGE SUMMARY\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Total unique combinations: {len(merged_data)}\")\n",
    "        \n",
    "        # Show combinations by number of LLMs\n",
    "        llm_count_distribution = {}\n",
    "        for key, tracking in assessment_tracking.items():\n",
    "            num_llms = len(tracking['llm_providers'])\n",
    "            if num_llms not in llm_count_distribution:\n",
    "                llm_count_distribution[num_llms] = []\n",
    "            llm_count_distribution[num_llms].append(key)\n",
    "        \n",
    "        print(f\"\\nAssessments by number of LLMs:\")\n",
    "        for num_llms in sorted(llm_count_distribution.keys(), reverse=True):\n",
    "            combos = llm_count_distribution[num_llms]\n",
    "            print(f\"  {num_llms} LLMs: {len(combos)} combinations\")\n",
    "        \n",
    "        # Convert back to list of results\n",
    "        merged_results = list(merged_data.values())\n",
    "        \n",
    "        # Recalculate consensus for merged results\n",
    "        print(f\"\\nRecalculating consensus for merged results...\")\n",
    "        merged_results = self._recalculate_consensus(merged_results)\n",
    "        \n",
    "        # Export merged results\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(merged_results, f, indent=2)\n",
    "        \n",
    "        print(f\"\\n✓ Merged results saved to: {output_file}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        # Convert to ConsensusResult objects\n",
    "        consensus_results = []\n",
    "        for result in merged_results:\n",
    "            consensus_results.append(ConsensusResult(\n",
    "                software=result['software'],\n",
    "                method=result['method'],\n",
    "                final_rank=result['final_rank'],\n",
    "                confidence=result['confidence'],\n",
    "                individual_ranks=result['individual_ranks'],\n",
    "                individual_reasoning=result['individual_reasoning'],\n",
    "                individual_sources=result['individual_sources'],\n",
    "                agreement_level=result['agreement_level'],\n",
    "                total_tokens=result['total_tokens'],\n",
    "                total_cost=result['total_cost']\n",
    "            ))\n",
    "        \n",
    "        return consensus_results\n",
    "    \n",
    "    \n",
    "    def _merge_union(self, existing: Dict, new: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Merge strategy: Union - Combine all LLM assessments\n",
    "        \"\"\"\n",
    "        # Combine individual assessments from all LLMs\n",
    "        merged = existing.copy()\n",
    "        \n",
    "        # Merge individual ranks\n",
    "        merged['individual_ranks'].update(new['individual_ranks'])\n",
    "        \n",
    "        # Merge individual reasoning\n",
    "        merged['individual_reasoning'].update(new['individual_reasoning'])\n",
    "        \n",
    "        # Merge individual sources\n",
    "        merged['individual_sources'].update(new['individual_sources'])\n",
    "        \n",
    "        # Sum tokens and costs\n",
    "        merged['total_tokens'] = existing['total_tokens'] + new['total_tokens']\n",
    "        merged['total_cost'] = existing['total_cost'] + new['total_cost']\n",
    "        \n",
    "        # Note: final_rank, confidence, and agreement_level will be recalculated\n",
    "        \n",
    "        return merged\n",
    "    \n",
    "    \n",
    "    def _merge_best(self, existing: Dict, new: Dict, prefer_higher_confidence: bool) -> Dict:\n",
    "        \"\"\"\n",
    "        Merge strategy: Best - Keep the assessment with best confidence/most LLMs\n",
    "        \"\"\"\n",
    "        existing_llm_count = len(existing['individual_ranks'])\n",
    "        new_llm_count = len(new['individual_ranks'])\n",
    "        \n",
    "        if prefer_higher_confidence:\n",
    "            # Prefer more LLM assessments (higher confidence)\n",
    "            if new_llm_count > existing_llm_count:\n",
    "                return new\n",
    "            elif new_llm_count < existing_llm_count:\n",
    "                return existing\n",
    "            else:\n",
    "                # Same number of LLMs, use confidence score\n",
    "                if new['confidence'] > existing['confidence']:\n",
    "                    return new\n",
    "                else:\n",
    "                    return existing\n",
    "        else:\n",
    "            # Just use confidence score\n",
    "            if new['confidence'] > existing['confidence']:\n",
    "                return new\n",
    "            else:\n",
    "                return existing\n",
    "    \n",
    "    \n",
    "    def _recalculate_consensus(self, results: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Recalculate final_rank, confidence, and agreement_level for merged results\n",
    "        \"\"\"\n",
    "        for result in results:\n",
    "            # Get all ranks\n",
    "            ranks = list(result['individual_ranks'].values())\n",
    "            \n",
    "            if len(ranks) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Calculate new consensus\n",
    "            confidence, agreement_level = self.calculate_confidence(ranks)\n",
    "            \n",
    "            # Determine final rank (most common)\n",
    "            rank_counts = Counter(ranks)\n",
    "            final_rank = rank_counts.most_common(1)[0][0]\n",
    "            \n",
    "            # Update result\n",
    "            result['final_rank'] = final_rank\n",
    "            result['confidence'] = confidence\n",
    "            result['agreement_level'] = agreement_level\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    \n",
    "    def compare_assessment_files(self, file1: str, file2: str):\n",
    "        \"\"\"\n",
    "        Compare two assessment files and show differences\n",
    "        \n",
    "        Args:\n",
    "            file1: First JSON file path\n",
    "            file2: Second JSON file path\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"COMPARING ASSESSMENT FILES\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"File 1: {file1}\")\n",
    "        print(f\"File 2: {file2}\")\n",
    "        \n",
    "        with open(file1, 'r') as f:\n",
    "            results1 = json.load(f)\n",
    "        with open(file2, 'r') as f:\n",
    "            results2 = json.load(f)\n",
    "        \n",
    "        # Create lookup dictionaries\n",
    "        lookup1 = {(r['software'], r['method']): r for r in results1}\n",
    "        lookup2 = {(r['software'], r['method']): r for r in results2}\n",
    "        \n",
    "        all_keys = set(lookup1.keys()) | set(lookup2.keys())\n",
    "        \n",
    "        only_in_file1 = set(lookup1.keys()) - set(lookup2.keys())\n",
    "        only_in_file2 = set(lookup2.keys()) - set(lookup1.keys())\n",
    "        in_both = set(lookup1.keys()) & set(lookup2.keys())\n",
    "        \n",
    "        print(f\"\\nCombinations only in File 1: {len(only_in_file1)}\")\n",
    "        for key in sorted(only_in_file1):\n",
    "            print(f\"  - {key[0]} / {key[1]}\")\n",
    "        \n",
    "        print(f\"\\nCombinations only in File 2: {len(only_in_file2)}\")\n",
    "        for key in sorted(only_in_file2):\n",
    "            print(f\"  - {key[0]} / {key[1]}\")\n",
    "        \n",
    "        print(f\"\\nCombinations in both files: {len(in_both)}\")\n",
    "        \n",
    "        # Compare assessments in both files\n",
    "        differences = []\n",
    "        for key in sorted(in_both):\n",
    "            r1 = lookup1[key]\n",
    "            r2 = lookup2[key]\n",
    "            \n",
    "            llms1 = set(r1['individual_ranks'].keys())\n",
    "            llms2 = set(r2['individual_ranks'].keys())\n",
    "            \n",
    "            if llms1 != llms2:\n",
    "                differences.append({\n",
    "                    'key': key,\n",
    "                    'type': 'different_llms',\n",
    "                    'file1_llms': llms1,\n",
    "                    'file2_llms': llms2\n",
    "                })\n",
    "            elif r1['final_rank'] != r2['final_rank']:\n",
    "                differences.append({\n",
    "                    'key': key,\n",
    "                    'type': 'different_rank',\n",
    "                    'file1_rank': r1['final_rank'],\n",
    "                    'file2_rank': r2['final_rank']\n",
    "                })\n",
    "        \n",
    "        if differences:\n",
    "            print(f\"\\nDifferences found: {len(differences)}\")\n",
    "            for diff in differences:\n",
    "                print(f\"\\n  {diff['key'][0]} / {diff['key'][1]}\")\n",
    "                if diff['type'] == 'different_llms':\n",
    "                    print(f\"    File 1 LLMs: {diff['file1_llms']}\")\n",
    "                    print(f\"    File 2 LLMs: {diff['file2_llms']}\")\n",
    "                elif diff['type'] == 'different_rank':\n",
    "                    print(f\"    File 1 Rank: {diff['file1_rank']}\")\n",
    "                    print(f\"    File 2 Rank: {diff['file2_rank']}\")\n",
    "        else:\n",
    "            print(\"\\n✓ No differences found in overlapping assessments\")\n",
    "        \n",
    "        print(f\"{'='*70}\\n\")\n",
    "\n",
    "\n",
    "    def filter_results(self, \n",
    "                      results: List[ConsensusResult],\n",
    "                      min_confidence: float = None,\n",
    "                      min_llms: int = None,\n",
    "                      specific_software: List[str] = None,\n",
    "                      specific_methods: List[str] = None,\n",
    "                      output_file: str = None) -> List[ConsensusResult]:\n",
    "        \"\"\"\n",
    "        Filter results based on various criteria\n",
    "        \n",
    "        Args:\n",
    "            results: List of ConsensusResult objects\n",
    "            min_confidence: Minimum confidence threshold (0.0-1.0)\n",
    "            min_llms: Minimum number of LLMs that assessed\n",
    "            specific_software: List of software names to include\n",
    "            specific_methods: List of methods to include\n",
    "            output_file: Optional output file for filtered results\n",
    "        \n",
    "        Returns:\n",
    "            Filtered list of ConsensusResult objects\n",
    "        \"\"\"\n",
    "        filtered = results\n",
    "        \n",
    "        if min_confidence is not None:\n",
    "            filtered = [r for r in filtered if r.confidence >= min_confidence]\n",
    "            print(f\"After confidence filter (>={min_confidence}): {len(filtered)} results\")\n",
    "        \n",
    "        if min_llms is not None:\n",
    "            filtered = [r for r in filtered if len(r.individual_ranks) >= min_llms]\n",
    "            print(f\"After LLM count filter (>={min_llms}): {len(filtered)} results\")\n",
    "        \n",
    "        if specific_software is not None:\n",
    "            filtered = [r for r in filtered if r.software in specific_software]\n",
    "            print(f\"After software filter: {len(filtered)} results\")\n",
    "        \n",
    "        if specific_methods is not None:\n",
    "            filtered = [r for r in filtered if r.method in specific_methods]\n",
    "            print(f\"After method filter: {len(filtered)} results\")\n",
    "        \n",
    "        if output_file:\n",
    "            self.export_results(filtered, output_file)\n",
    "        \n",
    "        return filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5c7037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting assessment of 4 combinations...\n",
      "Using OpenAI: False, Using Claude: False, Using Google: True\n",
      "------------------------------------------------------------\n",
      "\n",
      "[1/4] Processing: PSS/E - Optimal Power Flow (OPF)\n",
      "Assessing PSS/E - Optimal Power Flow (OPF) with Google Gemini...\n",
      "  Rank: 3, Confidence: 50.00%\n",
      "  Running cost: $0.0000 (1,225 tokens)\n",
      "\n",
      "[2/4] Processing: PSS/E - State Estimation\n",
      "Assessing PSS/E - State Estimation with Google Gemini...\n",
      "  Rank: 3, Confidence: 50.00%\n",
      "  Running cost: $0.0000 (2,489 tokens)\n",
      "\n",
      "[3/4] Processing: PowerWorld - Optimal Power Flow (OPF)\n",
      "Assessing PowerWorld - Optimal Power Flow (OPF) with Google Gemini...\n",
      "  Rank: 3, Confidence: 50.00%\n",
      "  Running cost: $0.0000 (3,631 tokens)\n",
      "\n",
      "[4/4] Processing: PowerWorld - State Estimation\n",
      "Assessing PowerWorld - State Estimation with Google Gemini...\n",
      "  Rank: 3, Confidence: 50.00%\n",
      "  Running cost: $0.0000 (4,829 tokens)\n",
      "\n",
      "Results exported to software_method_assessment.json\n",
      "\n",
      "============================================================\n",
      "ASSESSMENT SUMMARY\n",
      "============================================================\n",
      "{\n",
      "  \"total_assessments\": 4,\n",
      "  \"confidence_distribution\": {\n",
      "    \"high_confidence\": 0,\n",
      "    \"medium_confidence\": 4,\n",
      "    \"low_confidence\": 0\n",
      "  },\n",
      "  \"agreement_distribution\": {\n",
      "    \"single_assessment\": 4\n",
      "  },\n",
      "  \"rank_distribution\": {\n",
      "    \"3\": 4\n",
      "  },\n",
      "  \"average_confidence\": 0.5,\n",
      "  \"total_cost\": 0.0,\n",
      "  \"total_tokens\": 4829\n",
      "}\n",
      "\n",
      "============================================================\n",
      "SOURCES SUMMARY\n",
      "============================================================\n",
      "{\n",
      "  \"total_sources\": 16,\n",
      "  \"assessments_with_sources\": 4,\n",
      "  \"assessments_without_sources\": 0,\n",
      "  \"average_sources_per_assessment\": 4.0,\n",
      "  \"by_provider\": {\n",
      "    \"google_gemini-2.0-flash-exp\": {\n",
      "      \"total_sources\": 16,\n",
      "      \"assessments_with_sources\": 4,\n",
      "      \"assessments_without_sources\": 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "============================================================\n",
      "API USAGE SUMMARY\n",
      "============================================================\n",
      "Total API Calls: 4\n",
      "Total Tokens: 4,829\n",
      "  - Input: 3,280\n",
      "  - Output: 1,549\n",
      "  - Cached: 0\n",
      "\n",
      "Total Cost: $0.0000\n",
      "Average Cost per Call: $0.0000\n",
      "\n",
      "Breakdown by Model:\n",
      "------------------------------------------------------------\n",
      "  models/gemini-2.0-flash-exp:\n",
      "    Calls: 4\n",
      "    Tokens: 4,829\n",
      "    Cost: $0.0000\n",
      "============================================================\n",
      "\n",
      "================================================================================\n",
      "DETAILED ASSESSMENT RESULTS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize assessor\n",
    "assessor = SoftwareMethodAssessor(use_config=True)\n",
    "\n",
    "# Define software and methods to assess\n",
    "software_list_all = [\n",
    "    \"PSS/E\",\n",
    "    \"PowerWorld\",\n",
    "]\n",
    "\n",
    "method_list_all = [\n",
    "    \"Optimal Power Flow (OPF)\",\n",
    "    \"State Estimation\",\n",
    "]\n",
    "\n",
    "# Run assessments\n",
    "results = assessor.assess_multiple(\n",
    "software_list=software_list_all,\n",
    "method_list=method_list_all,\n",
    "use_openai=False,\n",
    "use_claude=False,\n",
    "use_google=True,  # ← Enable third LLM\n",
    "openai_model=\"gpt-4o\",\n",
    "claude_model=\"claude-sonnet-4-20250514\",\n",
    "google_model=\"models/gemini-2.0-flash-exp\"  # Use flash for cost savings\n",
    ")\n",
    "\n",
    "# Export results (sources will be included automatically via asdict)\n",
    "assessor.export_results(results, \"software_method_assessment.json\")\n",
    "\n",
    "# Create and print summary report\n",
    "summary = assessor.create_summary_report(results)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ASSESSMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(json.dumps(summary, indent=2))\n",
    "\n",
    "# Print sources summary\n",
    "sources_summary = assessor.create_sources_summary(results)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SOURCES SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(json.dumps(sources_summary, indent=2))\n",
    "\n",
    "# Print credit tracker summary\n",
    "assessor.credit_tracker.print_summary()\n",
    "\n",
    "# Print detailed results with sources (only high confidence)\n",
    "assessor.print_detailed_results(results, min_confidence=0.8)\n",
    "\n",
    "# Or print all results with sources:\n",
    "# assessor.print_detailed_results(results, min_confidence=0.0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "literature-search-and-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
