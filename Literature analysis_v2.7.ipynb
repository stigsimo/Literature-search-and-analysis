{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33fc68ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "# Cell 1: Imports and Setup\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import configparser\n",
    "import tiktoken\n",
    "import logging\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Phrases\n",
    "import openai\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "SAVE_DIR = \"Saved_files_new\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "156bf246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Cell 2: OpenAI Setup and Utility (Updated for gpt-5-nano)\n",
    "class CreditTracker:\n",
    "    def __init__(self):\n",
    "        self.total_tokens = 0\n",
    "        self.total_cost = 0\n",
    "        self.cost_per_1k_tokens = 0.00015\n",
    "    def update(self, tokens):\n",
    "        self.total_tokens += tokens\n",
    "        self.total_cost += (tokens / 1000) * self.cost_per_1k_tokens\n",
    "    def get_stats(self):\n",
    "        return {\"total_tokens\": self.total_tokens, \"total_cost\": round(self.total_cost, 4)}\n",
    "\n",
    "def initialize_openai():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('config_LLM.txt')\n",
    "    api_key = config['LLM'].get('OPENAI_API_KEY')\n",
    "    model_type = config['LLM'].get('MODEL_TYPE')\n",
    "    client = openai.OpenAI(api_key=api_key)\n",
    "    return client, model_type\n",
    "\n",
    "def num_tokens_from_string(string: str, model_name: str) -> int:\n",
    "    \"\"\"Get token count with fallback for unsupported models like gpt-5-nano\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model_name)\n",
    "        return len(encoding.encode(string))\n",
    "    except KeyError:\n",
    "        # Fallback for unsupported models like gpt-5-nano\n",
    "        if model_name.startswith('gpt-5-nano'):\n",
    "            # Use o200k_base encoding as fallback for gpt-5-nano\n",
    "            encoding = tiktoken.get_encoding(\"o200k_base\")\n",
    "            return len(encoding.encode(string))\n",
    "        else:\n",
    "            # For other unsupported models, use a reasonable approximation\n",
    "            return len(string) // 4  # Rough approximation: 4 chars per token\n",
    "\n",
    "client, model_type = initialize_openai()\n",
    "credit_tracker = CreditTracker()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "145755ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Cell 3: Data Preprocessing Utilities\n",
    "\n",
    "def extract_keywords_from_filename(filename):\n",
    "    base = os.path.splitext(os.path.basename(filename))[0]\n",
    "    parts = base.split('_')\n",
    "    return [part for i, part in enumerate(parts) if i > 2 and part != 'results' and not part.isdigit()]\n",
    "\n",
    "def get_custom_stop_words(search_keywords=None):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words_to_keep = set()\n",
    "    if search_keywords:\n",
    "        for keyword in search_keywords:\n",
    "            keyword = keyword.lower()\n",
    "            words_to_keep.add(keyword)\n",
    "            for word in keyword.split():\n",
    "                words_to_keep.add(word)\n",
    "    stop_words = stop_words - words_to_keep\n",
    "    scientific_terms = {'et', 'al', 'ref', 'reference', 'references', 'cited', 'cite',\n",
    "        'fig', 'figure', 'figures', 'table', 'tables', 'chart', 'charts',\n",
    "        'published', 'journal', 'conference', 'proceedings', 'vol', 'volume', 'pp', 'page', 'pages', 'doi'}\n",
    "    return stop_words.union(scientific_terms)\n",
    "\n",
    "def preprocess_text(text, search_keywords=None, min_word_length=2, remove_numbers=True):\n",
    "    if not isinstance(text, (str, int, float)):\n",
    "        return ''\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    if remove_numbers:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s-]', '', text)\n",
    "    text = re.sub(r'--+', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = get_custom_stop_words(search_keywords)\n",
    "    tokens = [t for t in tokens if len(t) >= min_word_length and t not in stop_words and len(t) > 1 and not t.isdigit()]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    try:\n",
    "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    except:\n",
    "        pass\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess_dataframe(df, text_col, search_keywords, processed_col='processed_text'):\n",
    "    df[text_col] = df[text_col].fillna('').astype(str)\n",
    "    df[processed_col] = df[text_col].apply(lambda x: preprocess_text(x, search_keywords))\n",
    "    return df[df[processed_col].str.strip() != '']\n",
    "\n",
    "def clean_fields_of_study(s):\n",
    "    valid_fields = ['Computer Science', 'Economics', 'Engineering', 'Physics', 'Mathematics',\n",
    "        'Medicine','Business','Environmental Science','Chemistry','Materials Science',\n",
    "        'Geography','Biology','Geology','Political Science','Psychology','Com']\n",
    "    if pd.isna(s) or s == '[]':\n",
    "        return [\"Unknown\"]\n",
    "    if isinstance(s, str):\n",
    "        fields = [field.strip().strip(\"'\\\"\") for field in s.strip('[]').split(',')]\n",
    "        return [f if f in valid_fields else \"Unknown\" for f in fields] or [\"Unknown\"]\n",
    "    return [\"Unknown\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98a22896",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 15:52:23,604 - INFO - Loaded and preprocessed 28934 papers\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Cell 4: Data Loading & Cleaning\n",
    "\n",
    "filename = \"semantic_scholar_2025_02_14_reliability_resilience_power_systems_results.csv\"\n",
    "filepath = os.path.join(\"Saved_files\", filename)\n",
    "df = pd.read_csv(filepath, sep=\";\")\n",
    "df['text'] = df['title'].fillna('') + ' ' + df['abstract'].fillna('')\n",
    "search_keywords = extract_keywords_from_filename(filename)\n",
    "df = preprocess_dataframe(df, text_col='text', search_keywords=search_keywords)\n",
    "df['fieldsOfStudy'] = df['fieldsOfStudy'].apply(clean_fields_of_study)\n",
    "logger.info(f\"Loaded and preprocessed {len(df)} papers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feef6bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Cell 5: Enhanced Method Detection Functions (Updated for gpt-5-nano)\n",
    "\n",
    "def extract_candidate_terms(df, text_col='processed_text', max_features=20000):\n",
    "    vectorizer = CountVectorizer(\n",
    "        ngram_range=(1, 4), max_df=0.95, min_df=2, max_features=max_features, token_pattern=r'\\b[\\w-]+\\b'\n",
    "    )\n",
    "    matrix = vectorizer.fit_transform(df[text_col].fillna(''))\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    freqs = matrix.sum(axis=0).A1\n",
    "    return [term for term, freq in sorted(zip(terms, freqs), key=lambda x: x[1], reverse=True)]\n",
    "\n",
    "def parse_llm_python_list(output_text):\n",
    "    \"\"\"Improved parsing function for LLM outputs\"\"\"\n",
    "    import re\n",
    "    import ast\n",
    "    \n",
    "    # Clean the output\n",
    "    content = output_text.strip()\n",
    "    \n",
    "    # Remove markdown code blocks\n",
    "    content = re.sub(r'```(?:python|json)?\\n?', '', content)\n",
    "    content = re.sub(r'```', '', content)\n",
    "    \n",
    "    # Look for list patterns\n",
    "    list_patterns = [\n",
    "        r'\\[([^\\]]+)\\]',  # Standard list format\n",
    "        r'List:\\s*\\[([^\\]]+)\\]',  # List: [items]\n",
    "        r'Result:\\s*\\[([^\\]]+)\\]'  # Result: [items]\n",
    "    ]\n",
    "    \n",
    "    for pattern in list_patterns:\n",
    "        match = re.search(pattern, content, re.DOTALL | re.IGNORECASE)\n",
    "        if match:\n",
    "            try:\n",
    "                # Try to parse as literal\n",
    "                return ast.literal_eval('[' + match.group(1) + ']')\n",
    "            except:\n",
    "                # Fallback: split by comma and clean\n",
    "                items = [item.strip().strip(\"'\\\"\") for item in match.group(1).split(',')]\n",
    "                return [item for item in items if item.strip()]\n",
    "    \n",
    "    # If no list found, try line by line\n",
    "    lines = content.split('\\n')\n",
    "    items = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line and not line.startswith('#') and not line.startswith('//'):\n",
    "            # Remove leading numbers, dashes, etc.\n",
    "            line = re.sub(r'^\\d+\\.?\\s*[-*]?\\s*', '', line)\n",
    "            line = line.strip(\"'\\\"\")\n",
    "            if line:\n",
    "                items.append(line)\n",
    "    \n",
    "    return items[:1500]  # Limit to prevent huge lists\n",
    "\n",
    "def get_method_phrases_enhanced(\n",
    "    corpus_terms,\n",
    "    client,\n",
    "    model_type,\n",
    "    credit_tracker,\n",
    "    n_runs=3,\n",
    "    temp=0.1,\n",
    "    top_p=0.9,\n",
    "    show_progress=True,\n",
    "    batch_size=500 \n",
    "):\n",
    "    \"\"\"\n",
    "    Enhanced method extraction with batching, for gpt-5-nano and others.\n",
    "    \"\"\"\n",
    "    import collections\n",
    "    from math import ceil\n",
    "\n",
    "    all_phrases_sets = []\n",
    "\n",
    "    n_batches = ceil(len(corpus_terms) / batch_size)\n",
    "    for batch_idx in range(n_batches):\n",
    "        batch_terms = corpus_terms[batch_idx * batch_size : (batch_idx + 1) * batch_size]\n",
    "\n",
    "        # Compose prompt for just this batch\n",
    "        prompt = f\"\"\"You are analyzing scientific papers about power systems reliability and resilience.\n",
    "\n",
    "From these terms: {batch_terms}\n",
    "\n",
    "Extract ALL terms that represent:\n",
    "1. Specific algorithms (e.g., genetic algorithm, particle swarm optimization)\n",
    "2. Mathematical methods (e.g., monte carlo simulation, linear programming)\n",
    "3. Analysis techniques (e.g., fault tree analysis, load flow analysis)\n",
    "4. Optimization methods (e.g., unit commitment, optimal power flow)\n",
    "5. Modeling approaches (e.g., neural network, markov chain)\n",
    "6. Simulation methods (e.g., time series analysis, stochastic programming)\n",
    "DO include: e.g. 'monte carlo simulation', 'unit commitment', 'load flow analysis', 'genetic algorithm', 'neural network', 'stochastic optimization', 'reinforcement learning', 'fault tree analysis'.\\n\n",
    "DO NOT include generic terms like 'framework', 'analysis', 'system', 'method', 'procedure', 'approach', 'application', 'performance', 'review', 'assesment', by themselves or in combination with only other generic terms\n",
    "INCLUDE abbreviations, variants (e.g., OPF/optimal power flow) and compounds.\n",
    "\n",
    "Return as a Python list, one method per item.\n",
    "\"\"\"\n",
    "\n",
    "        for i in range(n_runs):\n",
    "            try:\n",
    "                api_params = {\n",
    "                    \"model\": model_type,\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                }\n",
    "                if model_type.startswith('gpt-5-nano'):\n",
    "                    api_params[\"max_completion_tokens\"] = 5000\n",
    "                else:\n",
    "                    api_params[\"temperature\"] = temp\n",
    "                    api_params[\"top_p\"] = top_p\n",
    "                    api_params[\"max_tokens\"] = 5000\n",
    "\n",
    "                response = client.chat.completions.create(**api_params)\n",
    "                content = response.choices[0].message.content\n",
    "                phrases = parse_llm_python_list(content)\n",
    "                phrases = [p.lower().strip() for p in phrases if p.strip() and len(p.strip()) > 2]\n",
    "                all_phrases_sets.append(set(phrases))\n",
    "                credit_tracker.update(num_tokens_from_string(content, model_type))\n",
    "                if show_progress:\n",
    "                    print(f\"BATCH {batch_idx+1}/{n_batches}, run {i+1}: found {len(phrases)}\")\n",
    "                    print(f\"  Sample: {phrases[:10]}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in LLM call for batch {batch_idx+1}, run {i+1}: {e}\")\n",
    "                all_phrases_sets.append(set())\n",
    "\n",
    "    # Combine and count results\n",
    "    all_flat = [p for s in all_phrases_sets for p in s]\n",
    "    counts = collections.Counter(all_flat)\n",
    "    sorted_methods = sorted(counts.keys(), key=lambda x: (-counts[x], x))\n",
    "    print(f\"\\nTotal unique phrases: {len(counts)}\")\n",
    "    print(f\"Most frequent (top 10): {sorted_methods[:10]}\")\n",
    "    return sorted_methods, counts\n",
    "\n",
    "\n",
    "def load_method_phrases_from_csv(filename=\"extracted_method_phrases.csv\"):\n",
    "    path = os.path.join(SAVE_DIR, filename)\n",
    "    if os.path.exists(path):\n",
    "        with open(path, encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            method_phrases = []\n",
    "            method_counts = []\n",
    "            for row in reader:\n",
    "                method_phrases.append(row[\"Method Phrase\"])\n",
    "                method_counts.append(int(row[\"Count\"]))\n",
    "            print(f\"✓ Loaded {len(method_phrases)} method phrases from {path}\")\n",
    "            return method_phrases, method_counts\n",
    "    else:\n",
    "        logger.warning(f\"File {path} not found.\")\n",
    "        return None, None\n",
    "\n",
    "def build_method_variant_groups(method_list, client, model_type, credit_tracker, batch_size=50):\n",
    "    \"\"\"Groups method variants together while keeping them as separate entries\"\"\"\n",
    "    variant_groups = {}\n",
    "    processed_methods = set()\n",
    "    \n",
    "    for i in range(0, len(method_list), batch_size):\n",
    "        batch = method_list[i:i + batch_size]\n",
    "        batch = [m for m in batch if m not in processed_methods]\n",
    "        \n",
    "        if not batch:\n",
    "            continue\n",
    "            \n",
    "        prompt = f\"\"\"Group these method terms by their semantic similarity. Methods that refer to the same core technique should be grouped together, but keep specific variants separate when they represent different approaches.\n",
    "\n",
    "Methods: {batch}\n",
    "\n",
    "For each group, identify:\n",
    "1. A canonical name (most complete/descriptive)\n",
    "2. All variants and abbreviations\n",
    "\n",
    "Example format:\n",
    "{{\n",
    "  \"optimal power flow\": [\"optimal power flow\", \"opf\", \"ac opf\", \"dc opf\"],\n",
    "  \"security constrained optimal power flow\": [\"security constrained optimal power flow\", \"scopf\", \"scuc\"],\n",
    "  \"monte carlo simulation\": [\"monte carlo simulation\", \"monte carlo\", \"mc simulation\"]\n",
    "}}\n",
    "\n",
    "Return as a Python dictionary.\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Build API parameters for gpt-5-nano compatibility\n",
    "            api_params = {\n",
    "                \"model\": model_type,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a scientific method classification expert.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            # gpt-5-nano specific parameters\n",
    "            if model_type.startswith('gpt-5-nano'):\n",
    "                api_params[\"max_completion_tokens\"] = 1500\n",
    "            else:\n",
    "                api_params[\"temperature\"] = 0.1\n",
    "                api_params[\"top_p\"] = 0.9\n",
    "                api_params[\"max_tokens\"] = 1500\n",
    "            \n",
    "            response = client.chat.completions.create(**api_params)\n",
    "            content = response.choices[0].message.content\n",
    "            credit_tracker.update(num_tokens_from_string(content, model_type))\n",
    "            \n",
    "            # Parse the dictionary response\n",
    "            try:\n",
    "                # Clean the response\n",
    "                content = content.strip()\n",
    "                if content.startswith('```'):\t\n",
    "                    content = re.sub(r'```(?:python|json)?\\n?', '', content)\n",
    "                    content = re.sub(r'```', '', content)\t\n",
    "                \n",
    "                # Find dictionary pattern\n",
    "                dict_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "                if dict_match:\n",
    "                    groups = ast.literal_eval(dict_match.group(0))\n",
    "                    variant_groups.update(groups)\n",
    "                    processed_methods.update(batch)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to parse variant groups from batch: {e}\")\n",
    "                # Fallback: treat each method as its own group\n",
    "                for method in batch:\n",
    "                    variant_groups[method] = [method]\n",
    "                processed_methods.update(batch)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in variant grouping: {e}\")\n",
    "            # Fallback: treat each method as its own group\n",
    "            for method in batch:\n",
    "                variant_groups[method] = [method]\n",
    "            processed_methods.update(batch)\n",
    "    \n",
    "    logger.info(f\"Created {len(variant_groups)} method variant groups\")\n",
    "    return variant_groups\n",
    "\n",
    "def create_variant_mapping(variant_groups):\n",
    "    \"\"\"Create mapping from any variant to its canonical form\"\"\"\n",
    "    variant_to_canonical = {}\n",
    "    canonical_to_variants = {}\n",
    "    \n",
    "    for canonical, variants in variant_groups.items():\n",
    "        canonical_to_variants[canonical] = variants\n",
    "        for variant in variants:\n",
    "            variant_to_canonical[variant.lower()] = canonical\n",
    "    \n",
    "    return variant_to_canonical, canonical_to_variants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "35425ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Cell 6: Method Scoring Functions (Enhanced)\n",
    "\n",
    "def compute_enhanced_tfidf_scores(processed_texts, method_variants_dict, ngram_range=(1, 4), min_df=1, max_df=0.95):\n",
    "    \"\"\"Compute TF-IDF scores for all method variants\"\"\"\n",
    "    # Get all variants\n",
    "    all_variants = []\n",
    "    for variants in method_variants_dict.values():\n",
    "        all_variants.extend(variants)\n",
    "    \n",
    "    # Create vocabulary from actual variants that exist in corpus\n",
    "    existing_variants = []\n",
    "    for variant in all_variants:\n",
    "        # Check if variant appears in any document\n",
    "        variant_pattern = r'\\b' + re.escape(variant.lower()) + r'\\b'\n",
    "        found = False\n",
    "        for text in processed_texts[:100]:  # Sample check for efficiency\n",
    "            if re.search(variant_pattern, text.lower()):\n",
    "                existing_variants.append(variant)\n",
    "                found = True\n",
    "                break\n",
    "        if not found and len(existing_variants) < 1000:  # Keep checking if we don't have too many\n",
    "            for text in processed_texts:\n",
    "                if re.search(variant_pattern, text.lower()):\n",
    "                    existing_variants.append(variant)\n",
    "                    break\n",
    "    \n",
    "    print(f\"Found {len(existing_variants)} variants that exist in corpus out of {len(all_variants)} total\")\n",
    "    \n",
    "    if not existing_variants:\n",
    "        logger.warning(\"No method variants found in corpus!\")\n",
    "        return np.zeros((len(processed_texts), 1)), ['no_methods_found']\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        vocabulary=existing_variants,\n",
    "        ngram_range=ngram_range,\n",
    "        min_df=min_df,\n",
    "        max_df=max_df,\n",
    "        norm='l2',\n",
    "        token_pattern=r'\\b[\\w-]+\\b'\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(processed_texts)\n",
    "        scores = tfidf_matrix.toarray()\n",
    "        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "        return scores, feature_names\n",
    "    except Exception as e:\n",
    "        logger.error(f\"TF-IDF computation failed: {e}\")\n",
    "        return np.zeros((len(processed_texts), len(existing_variants))), existing_variants\n",
    "\n",
    "def compute_enhanced_lda_scores(processed_texts, method_variants_dict, ngram_range=(1, 3), n_topics=None, max_iter=20):\n",
    "    \"\"\"Compute LDA scores for method variants.\"\"\"\n",
    "    all_variants = []\n",
    "    for variants in method_variants_dict.values():\n",
    "        all_variants.extend(variants)\n",
    "\n",
    "    if n_topics is None:\n",
    "        n_topics = min(len(all_variants), 100)\n",
    "\n",
    "    vectorizer = CountVectorizer(\n",
    "        vocabulary=all_variants,\n",
    "        ngram_range=ngram_range,\n",
    "        token_pattern=r'\\b[\\w-]+\\b'\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        doc_term_matrix = vectorizer.fit_transform(processed_texts)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "        if n_topics >= 2 and doc_term_matrix.shape[1] > 0:\n",
    "            lda = LatentDirichletAllocation(\n",
    "                n_components=min(n_topics, doc_term_matrix.shape[1]),\n",
    "                learning_method='batch',\n",
    "                random_state=42,\n",
    "                max_iter=max_iter\n",
    "            )\n",
    "            lda_matrix = lda.fit_transform(doc_term_matrix)\n",
    "        else:\n",
    "            lda_matrix = np.zeros((doc_term_matrix.shape, len(all_variants)))\n",
    "\n",
    "        return lda_matrix, feature_names\n",
    "    except Exception as e:\n",
    "        logger.error(f\"LDA computation failed: {e}\")\n",
    "        return np.zeros((len(processed_texts), len(all_variants))), all_variants\n",
    "    \n",
    "def compute_enhanced_compound_scores(df, method_variants_dict, processed_col='processed_text', window=150):\n",
    "    \"\"\"Enhanced compound scoring that handles variants\"\"\"\n",
    "    n_docs = len(df)\n",
    "    all_variants = []\n",
    "    for variants in method_variants_dict.values():\n",
    "        all_variants.extend(variants)\n",
    "    \n",
    "    n_methods = len(all_variants)\n",
    "    scores = np.zeros((n_docs, n_methods), dtype=np.float32)\n",
    "    docs = df[processed_col].fillna('').str.lower().tolist()\n",
    "    \n",
    "    for j, variant in enumerate(all_variants):\n",
    "        variant_l = variant.lower()\n",
    "        \n",
    "        for i, text in enumerate(docs):\n",
    "            # Full phrase match\n",
    "            if variant_l in text:\n",
    "                scores[i, j] = 1.0\n",
    "            # Partial word match for compound terms\n",
    "            elif len(variant_l.split()) > 1:\n",
    "                words = variant_l.split()\n",
    "                if all(word in text for word in words):\n",
    "                    scores[i, j] = 0.7\n",
    "            # Abbreviation handling\n",
    "            elif len(variant_l) <= 5 and variant_l.upper() in text.upper():\n",
    "                scores[i, j] = 0.8\n",
    "    \n",
    "    return scores, all_variants\n",
    "\n",
    "def aggregate_variant_scores_to_canonical(scores, variant_names, variant_to_canonical):\n",
    "    \"\"\"Aggregate variant scores back to canonical method names\"\"\"\n",
    "    canonical_methods = list(set(variant_to_canonical.values()))\n",
    "    canonical_scores = np.zeros((scores.shape, len(canonical_methods)))\n",
    "    \n",
    "    canonical_to_idx = {method: i for i, method in enumerate(canonical_methods)}\n",
    "    \n",
    "    for j, variant in enumerate(variant_names):\n",
    "        canonical = variant_to_canonical.get(variant.lower(), variant)\n",
    "        if canonical in canonical_to_idx:\n",
    "            canonical_idx = canonical_to_idx[canonical]\n",
    "            canonical_scores[:, canonical_idx] += scores[:, j]  # Sum scores for variants\n",
    "    \n",
    "    return canonical_scores, canonical_methods\n",
    "\n",
    "def assign_top_methods_enhanced(\n",
    "    df, canonical_scores, canonical_methods, variant_scores, variant_names,\n",
    "    top_n=5, min_score=0.01\n",
    "):\n",
    "    \"\"\"Enhanced method assignment with granular variant tracking\"\"\"\n",
    "    \n",
    "    # Assign top canonical methods\n",
    "    for rank in range(top_n):\n",
    "        top_method = []\n",
    "        top_score = []\n",
    "        top_variants = []\n",
    "        confidence = []\n",
    "\n",
    "        for i, row in enumerate(canonical_scores):\n",
    "            if np.allclose(row, row):  # All equal\n",
    "                top_method.append(\"\")\n",
    "                top_score.append(0.0)\n",
    "                top_variants.append(\"\")\n",
    "                confidence.append(\"\")\n",
    "                continue\n",
    "\n",
    "            idxs = np.argsort(row)[::-1]\n",
    "            if rank < len(idxs):\n",
    "                method_idx = idxs[rank]\n",
    "                method = canonical_methods[method_idx]\n",
    "                score = row[method_idx]\n",
    "                \n",
    "                if score >= min_score:\n",
    "                    # Find contributing variants\n",
    "                    variant_contributions = []\n",
    "                    for v_idx, variant in enumerate(variant_names):\n",
    "                        if variant_scores[i, v_idx] > 0:\n",
    "                            # Check if this variant belongs to the current canonical method\n",
    "                            variant_canonical = variant_to_canonical.get(variant.lower(), variant)\n",
    "                            if variant_canonical == method:\n",
    "                                variant_contributions.append(f\"{variant}({variant_scores[i, v_idx]:.2f})\")\n",
    "                    \n",
    "                    top_method.append(method)\n",
    "                    top_score.append(score)\n",
    "                    top_variants.append(\"; \".join(variant_contributions[:3]))  # Top 3 variants\n",
    "                    confidence.append(\"confident\" if score > min_score * 2 else \"low_confidence\")\n",
    "                else:\n",
    "                    top_method.append(\"\")\n",
    "                    top_score.append(0.0)\n",
    "                    top_variants.append(\"\")\n",
    "                    confidence.append(\"\")\n",
    "            else:\n",
    "                top_method.append(\"\")\n",
    "                top_score.append(0.0)\n",
    "                top_variants.append(\"\")\n",
    "                confidence.append(\"\")\n",
    "\n",
    "        df[f'Top_{rank+1}_Method'] = top_method\n",
    "        df[f'Top_{rank+1}_Score'] = top_score\n",
    "        df[f'Top_{rank+1}_Variants'] = top_variants\n",
    "        df[f'Top_{rank+1}_Confidence'] = confidence\n",
    "\n",
    "    # Set primary columns\n",
    "    df['Primary_Method'] = df['Top_1_Method']\n",
    "    df['Primary_Method_Score'] = df['Top_1_Score']\n",
    "    df['Primary_Method_Variants'] = df['Top_1_Variants']\n",
    "    df['Method_Confidence'] = df['Top_1_Confidence']\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86210a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Cell 7: c(Keep your existing functions)\n",
    "\n",
    "def run_lda_topic_modeling(df, num_topics=10, num_words=25):\n",
    "    tokenized_texts = df['processed_text'].apply(lambda x: x.split()).tolist()\n",
    "    bigram = Phrases(tokenized_texts, min_count=10, threshold=50, delimiter='_')\n",
    "    trigram = Phrases(bigram[tokenized_texts], threshold=50, delimiter='_')\n",
    "    phrased = []\n",
    "    for doc in tokenized_texts:\n",
    "        bigrams_ = [w for w in bigram[doc] if '_' in w]\n",
    "        trigrams_ = [w for w in trigram[bigram[doc]] if '_' in w]\n",
    "        combined = doc + bigrams_ + trigrams_\n",
    "        phrased.append(' '.join(combined))\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 1), token_pattern=r'\\b[\\w_-]+\\b', max_df=0.95, min_df=2, max_features=10000)\n",
    "    doc_term_matrix = vectorizer.fit_transform(phrased)\n",
    "    lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "    topic_distributions = lda_model.fit_transform(doc_term_matrix)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topic_keywords = {}\n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        top_indices = topic.argsort()[:-num_words-1:-1]\n",
    "        top_words = [feature_names[i] for i in top_indices]\n",
    "        topic_keywords[topic_idx] = {'top_words': top_words}\n",
    "    return lda_model, vectorizer, topic_distributions, topic_keywords\n",
    "\n",
    "def assign_papers_to_topics(topic_distributions):\n",
    "    paper_classifications = []\n",
    "    for idx, dist in enumerate(topic_distributions):\n",
    "        top_2_topics = np.argsort(dist)[-2:][::-1]\n",
    "        primary_score = dist[top_2_topics]\n",
    "        other_topics_sum = sum(dist) - primary_score\n",
    "        dominance_ratio = primary_score / (other_topics_sum + 1e-10)\n",
    "        paper_classifications.append({\n",
    "            'paper_idx': idx,\n",
    "            'primary_topic': top_2_topics[0],\n",
    "            'secondary_topic': top_2_topics[1],\n",
    "            'primary_score': primary_score,\n",
    "            'dominance_ratio': dominance_ratio\n",
    "        })\n",
    "    return paper_classifications\n",
    "\n",
    "def string_similarity(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def topic_name_llm_robust(\n",
    "    lda_keywords, tfidf_ngrams, top_titles,\n",
    "    client, model_type, credit_tracker,\n",
    "    initial_iterations=3, max_iterations=10, similarity_threshold=0.7,\n",
    "    temp=0, top_p=1.0\n",
    "):\n",
    "    prompt = (\n",
    "        \"Based on the following keywords and n-grams from LDA and TF-IDF, plus top paper titles, provide a concise topic name \"\n",
    "        \"(bigram or trigram, single word if very specific):\\n\"\n",
    "        f\"LDA: {', '.join(lda_keywords)}\\n\"\n",
    "        f\"TFIDF: {', '.join(tfidf_ngrams)}\\n\"\n",
    "        f\"TITLES: {', '.join(top_titles)}\\n\"\n",
    "        \"Return ONLY the topic name.\"\n",
    "    )\n",
    "    iterations = initial_iterations\n",
    "    from collections import Counter\n",
    "    while iterations <= max_iterations:\n",
    "        generated_names = []\n",
    "        for _ in range(iterations):\n",
    "            response = client.chat.completions.create(\n",
    "                model=model_type,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a science topic-naming assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=temp,\n",
    "                top_p=top_p\n",
    "            )\n",
    "            content = response.choices[0].message.content.strip()\n",
    "            if content:\n",
    "                generated_names.append(content)\n",
    "        for i, name in enumerate(generated_names):\n",
    "            matches = [other for j, other in enumerate(generated_names)\n",
    "                       if i != j and string_similarity(name, other) >= similarity_threshold]\n",
    "            if len(matches) >= len(generated_names)//2:\n",
    "                print(f\"Topic name stabilized after {iterations} iterations: {name}\")\n",
    "                return name\n",
    "        iterations += 2\n",
    "        print(f\"No majority topic name found, increasing iterations to {iterations}.\")\n",
    "    most_common = Counter(generated_names).most_common(1)\n",
    "    print(f\"Returning most common topic name after {max_iterations} iterations: {most_common}\")\n",
    "    return most_common\n",
    "\n",
    "def get_top_titles_for_topic(df, paper_classifications, topic_idx, n_titles=10):\n",
    "    dominant_papers = [p for p in paper_classifications if p['primary_topic'] == topic_idx]\n",
    "    paper_infos = [\n",
    "        (df.iloc[p['paper_idx']]['citationCount'] if 'citationCount' in df.columns else 0, df.iloc[p['paper_idx']]['title'])\n",
    "        for p in dominant_papers if not pd.isna(df.iloc[p['paper_idx']]['title'])\n",
    "    ]\n",
    "    # Correctly sort by citation count (descending)\n",
    "    top_titles = [title for _, title in sorted(paper_infos, key=lambda x: -x[0])[:n_titles]]\n",
    "    return top_titles\n",
    "\n",
    "\n",
    "def get_top_tfidf_ngrams_per_topic(df, tfidf_matrix, feature_names, topic_col='Primary_Topic_Index', top_k=10):\n",
    "    tfidf_ngrams = {}\n",
    "    for topic_idx in df[topic_col].dropna().unique():\n",
    "        topic_idx = int(topic_idx)\n",
    "        doc_indices = df[df[topic_col] == topic_idx].index\n",
    "        if len(doc_indices) == 0:\n",
    "            continue\n",
    "        topic_tfidf = np.asarray(tfidf_matrix[doc_indices].mean(axis=0)).ravel()\n",
    "        top_indices = topic_tfidf.argsort()[-top_k:][::-1]\n",
    "        top_terms = [(feature_names[i], topic_tfidf[i]) for i in top_indices if topic_tfidf[i] > 0]\n",
    "        tfidf_ngrams[topic_idx] = top_terms\n",
    "    return tfidf_ngrams\n",
    "\n",
    "def get_author_stats(paper_classifications, df_field, n_top=5):\n",
    "    top_papers = {}\n",
    "    author_topic_stats = {}\n",
    "    \n",
    "    for topic in set(p['primary_topic'] for p in paper_classifications):\n",
    "        topic_papers = [p for p in paper_classifications if p['primary_topic'] == topic]\n",
    "        \n",
    "        # Fix: Handle various numpy array cases for dominance_ratio\n",
    "        for p in topic_papers:\n",
    "            dominance_ratio = p['dominance_ratio']\n",
    "            \n",
    "            if isinstance(dominance_ratio, np.ndarray):\n",
    "                if dominance_ratio.size == 1:\n",
    "                    p['dominance_ratio'] = float(dominance_ratio.item())\n",
    "                else:\n",
    "                    # Take the first element if it's a multi-element array\n",
    "                    p['dominance_ratio'] = float(dominance_ratio.flat[0])\n",
    "            elif hasattr(dominance_ratio, 'item'):\n",
    "                p['dominance_ratio'] = float(dominance_ratio.item())\n",
    "            else:\n",
    "                p['dominance_ratio'] = float(dominance_ratio)\n",
    "            \n",
    "            # Also fix primary_score if needed\n",
    "            primary_score = p['primary_score']\n",
    "            if isinstance(primary_score, np.ndarray):\n",
    "                if primary_score.size == 1:\n",
    "                    p['primary_score'] = float(primary_score.item())\n",
    "                else:\n",
    "                    p['primary_score'] = float(primary_score.flat[0])\n",
    "            elif hasattr(primary_score, 'item'):\n",
    "                p['primary_score'] = float(primary_score.item())\n",
    "            else:\n",
    "                p['primary_score'] = float(primary_score)\n",
    "        \n",
    "        topic_papers.sort(key=lambda x: x['dominance_ratio'], reverse=True)\n",
    "        top_papers[topic] = []\n",
    "        \n",
    "        for p in topic_papers[:n_top]:\n",
    "            paper_idx = p['paper_idx']\n",
    "            try:\n",
    "                authors = df_field.iloc[paper_idx]['authors']\n",
    "                if isinstance(authors, str):\n",
    "                    try: \n",
    "                        authors = ast.literal_eval(authors)\n",
    "                    except (ValueError, SyntaxError): \n",
    "                        authors = []\n",
    "                if isinstance(authors, list):\n",
    "                    author_list = []\n",
    "                    for author in authors:\n",
    "                        if isinstance(author, dict):\n",
    "                            author_list.append({'name': author.get('name', 'Unknown'), 'id': author.get('authorId', 'Unknown')})\n",
    "                else: \n",
    "                    author_list = []\n",
    "                    \n",
    "                top_papers[topic].append({\n",
    "                    'paperId': df_field.iloc[paper_idx].get('paperId',''),\n",
    "                    'title': df_field.iloc[paper_idx].get('title',''),\n",
    "                    'authors': author_list,\n",
    "                    'score': float(p['primary_score']),\n",
    "                    'dominance_ratio': float(p['dominance_ratio'])\n",
    "                })\n",
    "            except Exception as e: \n",
    "                continue\n",
    "                \n",
    "    return top_papers, author_topic_stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfa79b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Cell 8: Utility Functions for Saving\n",
    "\n",
    "def save_term_frequencies(df, suffix_string, save_dir=SAVE_DIR, max_keywords=5000):\n",
    "    \"\"\"Save .json containing keywords, bigrams, trigrams with their counts for later visualization.\"\"\"\n",
    "    freq_data = {}\n",
    "    processed_text = df['processed_text'].fillna('').astype(str)\n",
    "    \n",
    "    for n in range(1, 4):\n",
    "        vectorizer = CountVectorizer(ngram_range=(n, n), stop_words='english', max_features=max_keywords)\n",
    "        matrix = vectorizer.fit_transform(processed_text)\n",
    "        terms = vectorizer.get_feature_names_out()\n",
    "        freqs = matrix.sum(axis=0).A1\n",
    "        \n",
    "        # Fix: Access the frequency (x[1]) for sorting, not the whole tuple (x)\n",
    "        freq_dict = {term: int(freq) for term, freq in sorted(zip(terms, freqs), key=lambda x: -x[1])}\n",
    "        \n",
    "        if n == 1: \n",
    "            freq_data['keywords'] = freq_dict\n",
    "        elif n == 2: \n",
    "            freq_data['bigrams'] = freq_dict\n",
    "        elif n == 3: \n",
    "            freq_data['trigrams'] = freq_dict\n",
    "    \n",
    "    out_fn = os.path.join(save_dir, f'term_frequencies_{suffix_string}.json')\n",
    "    with open(out_fn, 'w', encoding='utf-8') as f:\n",
    "        json.dump(freq_data, f, indent=2)\n",
    "    print(f\"✓ Saved term frequency summary to {out_fn}\")\n",
    "    return out_fn\n",
    "\n",
    "\n",
    "def save_author_and_venue_frequencies(df, suffix_string, save_dir=SAVE_DIR):\n",
    "    if 'authors' in df.columns:\n",
    "        authors_all = []\n",
    "        for item in df['authors']:\n",
    "            if isinstance(item, str) and item.strip():\n",
    "                try:\n",
    "                    obj = eval(item) if (item.strip().startswith(\"[\") or item.strip().startswith(\"{\")) else item.strip()\n",
    "                except Exception:\n",
    "                    obj = item.strip()\n",
    "            else:\n",
    "                obj = item\n",
    "            if isinstance(obj, list):\n",
    "                for author in obj:\n",
    "                    if isinstance(author, dict) and 'name' in author:\n",
    "                        authors_all.append(author['name'])\n",
    "                    elif isinstance(author, str):\n",
    "                        authors_all.append(author)\n",
    "            elif isinstance(obj, dict) and 'name' in obj:\n",
    "                authors_all.append(obj['name'])\n",
    "            elif isinstance(obj, str):\n",
    "                authors_all.append(obj)\n",
    "        author_counts = pd.Series(authors_all).value_counts().reset_index()\n",
    "        author_counts.columns = ['Author', 'Frequency']\n",
    "        author_fn = os.path.join(save_dir, f\"semantic_scholar_{suffix_string}_author_analysis.csv\")\n",
    "        author_counts.to_csv(author_fn, sep=';', encoding='utf-8', index=False)\n",
    "        print(f\"✓ Saved author frequencies: {author_fn}\")\n",
    "    else:\n",
    "        print(\"No 'authors' column found in DF: skipping author frequencies.\")\n",
    "        \n",
    "    if 'venue' in df.columns:\n",
    "        venue_counts = df['venue'].value_counts().reset_index()\n",
    "        venue_counts.columns = ['Venue', 'Frequency']\n",
    "        venue_fn = os.path.join(save_dir, f\"semantic_scholar_{suffix_string}_venue_frequencies.csv\")\n",
    "        venue_counts.to_csv(venue_fn, sep=';', encoding='utf-8', index=False)\n",
    "        print(f\"✓ Saved venue frequencies: {venue_fn}\")\n",
    "    else:\n",
    "        print(\"No 'venue' column found in DF: skipping venue frequencies.\")\n",
    "\n",
    "def save_topic_analysis_outputs(\n",
    "    df, lda_model, lda_vectorizer, topic_distributions, topic_keywords, topic_names, topic_ngrams,\n",
    "    author_stats, top_papers, tfidf_ngrams, suffix_string\n",
    "):\n",
    "    topic_metadata = {\n",
    "        \"topics\": {int(k): v for k,v in topic_keywords.items()},\n",
    "        \"topic_names\": {int(k): v for k,v in topic_names.items()},\n",
    "        \"topic_ngrams\": {int(k): v for k,v in topic_ngrams.items()},\n",
    "    }\n",
    "    with open(os.path.join(SAVE_DIR, f\"topics_{suffix_string}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(topic_metadata, f, indent=2)\n",
    "    with open(os.path.join(SAVE_DIR, f\"topic_names_{suffix_string}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({int(k):v for k,v in topic_names.items()}, f, indent=2)\n",
    "    np.save(os.path.join(SAVE_DIR, f\"topic_distributions_{suffix_string}.npy\"), topic_distributions)\n",
    "    import joblib\n",
    "    joblib.dump(lda_model, os.path.join(SAVE_DIR, f\"lda_model_{suffix_string}.joblib\"))\n",
    "    joblib.dump(lda_vectorizer, os.path.join(SAVE_DIR, f\"lda_vectorizer_{suffix_string}.joblib\"))\n",
    "    with open(os.path.join(SAVE_DIR, f\"top_papers_{suffix_string}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({int(k): v for k, v in top_papers.items()}, f, ensure_ascii=False, indent=2, default=str)\n",
    "    pd.DataFrame.from_dict(author_stats, orient='index').to_csv(\n",
    "        os.path.join(SAVE_DIR, f\"author_stats_{suffix_string}.csv\"))\n",
    "    with open(os.path.join(SAVE_DIR, f\"topic_specific_tfidf_ngrams_{suffix_string}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({int(k):[(term,float(score)) for term,score in v] for k,v in topic_ngrams.items()}, f, indent=2)\n",
    "\n",
    "def diagnostics_enhanced(df, canonical_scores, variant_scores, canonical_methods, variant_names):\n",
    "    n_docs, n_canonical = canonical_scores.shape\n",
    "    n_variants = variant_scores.shape[1]\n",
    "    \n",
    "    print(\"=== ENHANCED DIAGNOSTICS ===\")\n",
    "    print(f\"Total documents: {n_docs}\")\n",
    "    print(f\"Canonical methods: {n_canonical}\")\n",
    "    print(f\"Method variants: {n_variants}\")\n",
    "    print(f\"Canonical coverage: {(canonical_scores > 0).any(axis=1).sum()}/{n_docs} ({100*(canonical_scores>0).any(axis=1).mean():.1f}%)\")\n",
    "    print(f\"Variant coverage: {(variant_scores > 0).any(axis=1).sum()}/{n_docs} ({100*(variant_scores>0).any(axis=1).mean():.1f}%)\")\n",
    "    \n",
    "    if 'Primary_Method' in df.columns:\n",
    "        print(\"\\nMethod distribution (top 10):\")\n",
    "        method_dist = df['Primary_Method'].value_counts().head(10)\n",
    "        for method, count in method_dist.items():\n",
    "            if method:  # Skip empty strings\n",
    "                print(f\"  {method}: {count}\")\n",
    "    \n",
    "    if 'Method_Confidence' in df.columns:\n",
    "        print(\"\\nConfidence distribution:\")\n",
    "        conf_dist = df['Method_Confidence'].value_counts()\n",
    "        for conf, count in conf_dist.items():\n",
    "            if conf:  # Skip empty strings\n",
    "                print(f\"  {conf}: {count}\")\n",
    "    \n",
    "    print(f\"\\nCanonical methods sample: {canonical_methods[:5]}\")\n",
    "    print(f\"Variant methods sample: {variant_names[:10]}\")\n",
    "    print(f\"\\nCanonical scores stats: mean={canonical_scores.mean():.3f}, std={canonical_scores.std():.3f}\")\n",
    "    print(f\"Variant scores stats: mean={variant_scores.mean():.3f}, std={variant_scores.std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f31ba70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample topics and names:\n",
      "{0: 'Photovoltaic Power Systems', 1: 'Optical Wireless Communications', 2: 'Sensor Networks', 3: 'Smart Grid Sensor Networks', 4: 'Power system control'}\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Cell 9: Topic Analysis Workflow\n",
    "\"\"\"\n",
    "NUM_TOPICS = 12\n",
    "NUM_TOPIC_WORDS = 15\n",
    "TOPIC_LLM_ITER_INIT = 3\n",
    "TOPIC_LLM_ITER_MAX = 9\n",
    "TOPIC_LLM_SIM_THRESH = 0.72\n",
    "TOPIC_LLM_TEMP = 1\n",
    "TOPIC_LLM_TOP_P = 1.0\n",
    "\n",
    "current_date = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "keyword_str = '_'.join(extract_keywords_from_filename(filename)) if 'filename' in locals() else \"\"\n",
    "suffix_string = f\"{current_date}_{keyword_str}\"\n",
    "save_term_frequencies(df, suffix_string)\n",
    "save_author_and_venue_frequencies(df, suffix_string)\n",
    "\n",
    "logger.info(\"Starting topic modeling workflow...\")  \n",
    "lda_model, lda_vectorizer, topic_distributions, topic_keywords = run_lda_topic_modeling(\n",
    "    df, num_topics=NUM_TOPICS, num_words=NUM_TOPIC_WORDS)\n",
    "logger.info(\"✓ LDA topic modeling completed.\")\n",
    "\n",
    "paper_classifications = assign_papers_to_topics(topic_distributions)\n",
    "df['Primary_Topic_Index'] = [int(p['primary_topic']) for p in paper_classifications]\n",
    "df['Primary_Score'] = [p['primary_score'] for p in paper_classifications]\n",
    "df['Dominance_Ratio'] = [p['dominance_ratio'] for p in paper_classifications]\n",
    "\n",
    "logger.info(\"✓ Papers assigned to topics based on LDA distributions.\")\n",
    "\n",
    "topic_tfidf_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 3), min_df=2, max_df=0.95, token_pattern=r'\\b[\\w_-]+\\b'\n",
    ")\n",
    "topic_tfidf_matrix = topic_tfidf_vectorizer.fit_transform(df['processed_text'])\n",
    "topic_tfidf_feature_names = topic_tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "topic_ngrams = get_top_tfidf_ngrams_per_topic(\n",
    "    df, topic_tfidf_matrix, topic_tfidf_feature_names, topic_col='Primary_Topic_Index', top_k=10)\n",
    "\n",
    "logger.info(\"✓ Extracted topic-specific TF-IDF n-grams for naming.\")\n",
    "\n",
    "topic_names = {}\n",
    "for topic_idx, keywords in topic_keywords.items():\n",
    "    lda_ngrams = keywords['top_words'][:NUM_TOPIC_WORDS]\n",
    "    tfidf_ng = [ngram for ngram, _ in topic_ngrams.get(topic_idx, [])][:NUM_TOPIC_WORDS]\n",
    "    top_titles = get_top_titles_for_topic(df, paper_classifications, topic_idx, n_titles=10)\n",
    "    topic_name = topic_name_llm_robust(\n",
    "        lda_ngrams, tfidf_ng, top_titles,\n",
    "        client, model_type, credit_tracker,\n",
    "        initial_iterations=TOPIC_LLM_ITER_INIT,\n",
    "        max_iterations=TOPIC_LLM_ITER_MAX,\n",
    "        similarity_threshold=TOPIC_LLM_SIM_THRESH,\n",
    "        temp=TOPIC_LLM_TEMP, top_p=TOPIC_LLM_TOP_P\n",
    "    )\n",
    "    topic_names[topic_idx] = topic_name\n",
    "    logger.info(f\"Topic {topic_idx}: {topic_name if topic_name else 'Unnamed'}\")\n",
    "\n",
    "df['Primary_Topic'] = df['Primary_Topic_Index'].map(lambda x: topic_names.get(x, f\"Topic_{x}\"))\n",
    "logger.info(\"✓ Topic naming and assignment completed.\")\n",
    "\"\"\"\n",
    "top_papers, author_stats = get_author_stats(paper_classifications, df, n_top=5)\n",
    "\n",
    "save_topic_analysis_outputs(df, lda_model, lda_vectorizer, topic_distributions, topic_keywords, topic_names, topic_ngrams, author_stats, top_papers, topic_ngrams, suffix_string)\n",
    "print(\"\\nSample topics and names:\")\n",
    "print({k: topic_names[k] for k in list(topic_names)[:5]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3bb61ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 11:57:07,199 - INFO - === Starting Enhanced Method Detection Pipeline ===\n",
      "2025-08-23 11:57:07,201 - INFO - Step 1: Extracting candidate terms...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (method_phrases \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(method_phrases) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# 1. Extract candidate terms\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep 1: Extracting candidate terms...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m     candidate_terms \u001b[38;5;241m=\u001b[39m \u001b[43mextract_candidate_terms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprocessed_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_FEATURES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Extracted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(candidate_terms)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m candidate terms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample candidate terms: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcandidate_terms[:\u001b[38;5;241m10\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[68], line 8\u001b[0m, in \u001b[0;36mextract_candidate_terms\u001b[1;34m(df, text_col, max_features)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_candidate_terms\u001b[39m(df, text_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m, max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20000\u001b[39m):\n\u001b[0;32m      5\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(\n\u001b[0;32m      6\u001b[0m         ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m), max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m, min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, max_features\u001b[38;5;241m=\u001b[39mmax_features, token_pattern\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw-]+\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m     )\n\u001b[1;32m----> 8\u001b[0m     matrix \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext_col\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     terms \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[0;32m     10\u001b[0m     freqs \u001b[38;5;241m=\u001b[39m matrix\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mA1\n",
      "File \u001b[1;32mc:\\git_repos\\Literature-search-and-analysis\\.venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\git_repos\\Literature-search-and-analysis\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1368\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1369\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1370\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1373\u001b[0m             )\n\u001b[0;32m   1374\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1376\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1379\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\git_repos\\Literature-search-and-analysis\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Cell 10: Enhanced Method Extraction and Assignment Workflow (Updated for gpt-5-nano)\n",
    "\n",
    "# Parameters\n",
    "MAX_FEATURES = 15000\n",
    "TFIDF_WEIGHT = 0.4\n",
    "LDA_WEIGHT = 0.3\n",
    "COMPOUND_WEIGHT = 0.3\n",
    "TOP_METHODS_PER_PAPER = 5\n",
    "MIN_ASSIGN_SCORE = 0.01\n",
    "BATCH_SIZE = 2000\n",
    "\n",
    "# LLM parameters (gpt-5-nano compatible)\n",
    "METHOD_LLM_N_RUNS = 3\n",
    "\n",
    "logger.info(\"=== Starting Enhanced Method Detection Pipeline ===\")\n",
    "\n",
    "\n",
    "method_phrases, method_counts = load_method_phrases_from_csv(filename=os.path.join(SAVE_DIR, \"Saved_files_new\\extracted_method_phrases.csv\"))\n",
    "\n",
    "if (method_phrases is None) or (len(method_phrases) < 3):\n",
    "    # 1. Extract candidate terms\n",
    "    logger.info(\"Step 1: Extracting candidate terms...\")\n",
    "    candidate_terms = extract_candidate_terms(df, text_col='processed_text', max_features=MAX_FEATURES)\n",
    "    logger.info(f\"✓ Extracted {len(candidate_terms)} candidate terms\")\n",
    "    print(f\"Sample candidate terms: {candidate_terms[:10]}\")\n",
    "    # 2. Enhanced LLM method extraction (gpt-5-nano compatible)\n",
    "    logger.info(\"Step 2: Enhanced LLM method extraction...\")\n",
    "    method_phrases, method_counts = get_method_phrases_enhanced(\n",
    "    candidate_terms,\n",
    "    client,\n",
    "    model_type,\n",
    "    credit_tracker,\n",
    "    n_runs=METHOD_LLM_N_RUNS,\n",
    "    batch_size=BATCH_SIZE\n",
    "    )\n",
    "    save_method_phrases_to_csv(method_phrases, method_counts)\n",
    "else:\n",
    "    logger.info(f\"Loaded {len(method_phrases)} method phrases from existing CSV\")\n",
    "\n",
    "\n",
    "if not method_phrases:\n",
    "    logger.error(\"No method phrases extracted! Check your LLM configuration and prompts.\")\n",
    "    raise RuntimeError(\"Method extraction failed - no phrases found\")\n",
    "\n",
    "logger.info(f\"✓ Extracted {len(method_phrases)} method phrases\")\n",
    "print(f\"Sample methods: {method_phrases[:10]}\")\n",
    "\n",
    "# 3. Build variant groups while keeping granular methods\n",
    "logger.info(\"Step 3: Building method variant groups...\")\n",
    "variant_groups = build_method_variant_groups(\n",
    "    method_phrases, client, model_type, credit_tracker, batch_size=50\n",
    ")\n",
    "\n",
    "# Create mappings\n",
    "variant_to_canonical, canonical_to_variants = create_variant_mapping(variant_groups)\n",
    "logger.info(f\"✓ Created {len(canonical_to_variants)} canonical methods with {len(variant_to_canonical)} total variants\")\n",
    "\n",
    "print(\"\\nSample variant groups:\")\n",
    "for canonical, variants in list(canonical_to_variants.items())[:5]:\n",
    "    print(f\"  {canonical}: {variants}\")\n",
    "\n",
    "# 4. Compute scores for all variants\n",
    "logger.info(\"Step 4: Computing enhanced scoring matrices...\")\n",
    "\n",
    "logger.info(\"  4a: TF-IDF scoring...\")\n",
    "tfidf_scores, tfidf_feature_names = compute_enhanced_tfidf_scores(\n",
    "    df['processed_text'], canonical_to_variants\n",
    ")\n",
    "logger.info(f\"  ✓ TF-IDF: {tfidf_scores.shape}\")\n",
    "\n",
    "logger.info(\"  4b: LDA scoring...\")  \n",
    "lda_scores, lda_feature_names = compute_enhanced_lda_scores(\n",
    "    df['processed_text'], canonical_to_variants, n_topics=50\n",
    ")\n",
    "logger.info(f\"  ✓ LDA: {lda_scores.shape}\")\n",
    "\n",
    "logger.info(\"  4c: Compound scoring...\")\n",
    "compound_scores, compound_feature_names = compute_enhanced_compound_scores(\n",
    "    df, canonical_to_variants\n",
    ")\n",
    "logger.info(f\"  ✓ Compound: {compound_scores.shape}\")\n",
    "\n",
    "# 5. Combine variant scores\n",
    "logger.info(\"Step 5: Combining variant scores...\")\n",
    "# Ensure all score matrices have the same feature order\n",
    "common_features = list(set(tfidf_feature_names) & set(lda_feature_names) & set(compound_feature_names))\n",
    "logger.info(f\"  Common features across all methods: {len(common_features)}\")\n",
    "\n",
    "if len(common_features) == 0:\n",
    "    logger.warning(\"No common features found - using TF-IDF features as reference\")\n",
    "    common_features = tfidf_feature_names\n",
    "\n",
    "# Align score matrices\n",
    "def align_scores(scores, current_features, target_features):\n",
    "    if list(current_features) == list(target_features):\n",
    "        return scores\n",
    "    \n",
    "    aligned_scores = np.zeros((scores.shape[0], len(target_features)))  # Fixed shape access\n",
    "    current_to_idx = {feat: i for i, feat in enumerate(current_features)}\n",
    "    \n",
    "    for j, feat in enumerate(target_features):\n",
    "        if feat in current_to_idx:\n",
    "            aligned_scores[:, j] = scores[:, current_to_idx[feat]]\n",
    "    \n",
    "    return aligned_scores\n",
    "\n",
    "tfidf_aligned = align_scores(tfidf_scores, tfidf_feature_names, common_features)\n",
    "lda_aligned = align_scores(lda_scores, lda_feature_names, common_features)\n",
    "compound_aligned = align_scores(compound_scores, compound_feature_names, common_features)\n",
    "\n",
    "# Combined variant scores\n",
    "combined_variant_scores = (TFIDF_WEIGHT * tfidf_aligned + \n",
    "                          LDA_WEIGHT * lda_aligned + \n",
    "                          COMPOUND_WEIGHT * compound_aligned)\n",
    "\n",
    "logger.info(f\"  ✓ Combined variant scores: {combined_variant_scores.shape}\")\n",
    "\n",
    "# 6. Aggregate to canonical methods\n",
    "logger.info(\"Step 6: Aggregating to canonical methods...\")\n",
    "canonical_scores, canonical_methods = aggregate_variant_scores_to_canonical(\n",
    "    combined_variant_scores, common_features, variant_to_canonical\n",
    ")\n",
    "logger.info(f\"✓ Canonical scores: {canonical_scores.shape} for {len(canonical_methods)} methods\")\n",
    "\n",
    "# 7. Assign methods to papers\n",
    "logger.info(\"Step 7: Assigning methods to papers...\")\n",
    "df = assign_top_methods_enhanced(\n",
    "    df, canonical_scores, canonical_methods, \n",
    "    combined_variant_scores, common_features,\n",
    "    top_n=TOP_METHODS_PER_PAPER, min_score=MIN_ASSIGN_SCORE\n",
    ")\n",
    "logger.info(\"✓ Method assignment completed\")\n",
    "\n",
    "# 8. Save results\n",
    "logger.info(\"Step 8: Saving results...\")\n",
    "\n",
    "# Save variant groups\n",
    "with open(os.path.join(SAVE_DIR, f\"method_variant_groups_{suffix_string}.json\"), 'w') as f:\n",
    "    json.dump(canonical_to_variants, f, indent=2)\n",
    "\n",
    "# Save score matrices\n",
    "pd.DataFrame(canonical_scores, columns=canonical_methods).to_csv(\n",
    "    os.path.join(SAVE_DIR, f\"canonical_method_scores_{suffix_string}.csv\")\n",
    ")\n",
    "\n",
    "pd.DataFrame(combined_variant_scores, columns=common_features).to_csv(\n",
    "    os.path.join(SAVE_DIR, f\"variant_method_scores_{suffix_string}.csv\")\n",
    ")\n",
    "\n",
    "# Save final dataframe\n",
    "df.to_csv(os.path.join(SAVE_DIR, f\"enhanced_method_analysis_{suffix_string}.csv\"), index=False)\n",
    "\n",
    "# 9. Diagnostics\n",
    "logger.info(\"Step 9: Running diagnostics...\")\n",
    "diagnostics_enhanced(df, canonical_scores, combined_variant_scores, canonical_methods, common_features)\n",
    "\n",
    "# 10. Display results\n",
    "print(\"\\n=== SAMPLE RESULTS ===\")\n",
    "sample_cols = ['Primary_Method', 'Primary_Method_Score', 'Primary_Method_Variants', 'Method_Confidence']\n",
    "available_cols = [col for col in sample_cols if col in df.columns]\n",
    "print(df[available_cols].head(10))\n",
    "\n",
    "print(f\"\\n=== METHOD STATISTICS ===\")\n",
    "if 'Primary_Method' in df.columns:\n",
    "    method_stats = df['Primary_Method'].value_counts()\n",
    "    print(f\"Papers with methods assigned: {(df['Primary_Method'] != '').sum()}/{len(df)} ({100*(df['Primary_Method'] != '').mean():.1f}%)\")\n",
    "    print(f\"Top 10 methods:\")\n",
    "    for method, count in method_stats.head(10).items():\n",
    "        if method:\n",
    "            print(f\"  {method}: {count}\")\n",
    "\n",
    "print(f\"\\n✓ Enhanced method detection pipeline completed!\")\n",
    "print(f\"✓ Results saved to {SAVE_DIR}\")\n",
    "logger.info(\"Enhanced method detection pipeline completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2882cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved method phrases to extracted_method_phrases.csv\n"
     ]
    }
   ],
   "source": [
    "#save method phrases from list to csv\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "def save_method_phrases_to_csv(method_phrases, method_counts, filename=\"extracted_method_phrases.csv\"):\n",
    "    \"\"\"Save method phrases and their counts to a CSV file.\"\"\"\n",
    "    #save to Save directory\n",
    "    filename = os.path.join(SAVE_DIR, filename)\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Method Phrase\", \"Count\"])\n",
    "        for phrase, count in zip(method_phrases, method_counts):\n",
    "            # Clean phrase to remove newlines and excessive whitespace\n",
    "            clean_phrase = re.sub(r'\\s+', ' ', phrase.replace('\\n', ' ')).strip()\n",
    "            writer.writerow([clean_phrase, count])  \n",
    "    print(f\"✓ Saved method phrases to {filename}\")\n",
    "#usage\n",
    "save_method_phrases_to_csv(method_phrases, method_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "literature-search-and-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
