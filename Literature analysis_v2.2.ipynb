{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82082ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import configparser\n",
    "import tiktoken\n",
    "import logging\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Phrases\n",
    "\n",
    "import openai\n",
    "\n",
    "SAVE_DIR = \"Saved_files_new\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17a679a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreditTracker:\n",
    "    def __init__(self):\n",
    "        self.total_tokens = 0\n",
    "        self.total_cost = 0\n",
    "        self.cost_per_1k_tokens = 0.00015\n",
    "    def update(self, tokens):\n",
    "        self.total_tokens += tokens\n",
    "        self.total_cost += (tokens / 1000) * self.cost_per_1k_tokens\n",
    "    def get_stats(self):\n",
    "        return {\"total_tokens\": self.total_tokens, \"total_cost\": round(self.total_cost, 4)}\n",
    "\n",
    "def initialize_openai():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('config_LLM.txt')\n",
    "    api_key = config['LLM'].get('OPENAI_API_KEY')\n",
    "    model_type = config['LLM'].get('MODEL_TYPE')\n",
    "    client = openai.OpenAI(api_key=api_key)\n",
    "    return client, model_type\n",
    "\n",
    "client, model_type = initialize_openai()\n",
    "credit_tracker = CreditTracker()\n",
    "\n",
    "def num_tokens_from_string(string: str, model_name: str) -> int:\n",
    "    encoding = tiktoken.encoding_for_model(model_name)\n",
    "    return len(encoding.encode(string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0508b72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def extract_keywords_from_filename(filename):\n",
    "    base = os.path.splitext(os.path.basename(filename))[0]\n",
    "    parts = base.split('_')\n",
    "    keywords = [part for i, part in enumerate(parts) if i > 2 and part != 'results' and not part.isdigit()]\n",
    "    return keywords\n",
    "\n",
    "def keywords_to_filename_part(keywords):\n",
    "    return '_'.join([kw.lower().replace(' ', '_') for kw in keywords])\n",
    "\n",
    "def get_custom_stop_words(search_keywords=None):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words_to_keep = set()\n",
    "    if search_keywords:\n",
    "        for keyword in search_keywords:\n",
    "            keyword = keyword.lower()\n",
    "            words_to_keep.add(keyword)\n",
    "            for word in keyword.split():\n",
    "                words_to_keep.add(word)\n",
    "    stop_words = stop_words - words_to_keep\n",
    "    scientific_terms = {'et', 'al','ref','reference','references','cited','cite',\n",
    "        'fig','figure','figures','table','tables','chart','charts',\n",
    "        'published','journal','conference','proceedings','vol','volume','pp','page','pages','doi'}\n",
    "    stop_words = stop_words.union(scientific_terms)\n",
    "    return stop_words\n",
    "\n",
    "def preprocess_text(text, search_keywords=None, min_word_length=2, remove_numbers=False):\n",
    "    if not isinstance(text, (str, int, float)):\n",
    "        return ''\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    if remove_numbers:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s-]', '', text)\n",
    "    text = re.sub(r'--+', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = get_custom_stop_words(search_keywords)\n",
    "    tokens = [t for t in tokens if len(t) >= min_word_length and t not in stop_words and len(t) > 1 and not t.isdigit()]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    try:\n",
    "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    except:\n",
    "        pass\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess_dataframe(df, text_col, search_keywords, processed_col='processed_text'):\n",
    "    df[text_col] = df[text_col].fillna('').astype(str)\n",
    "    df[processed_col] = df[text_col].apply(lambda x: preprocess_text(x, search_keywords))\n",
    "    return df[df[processed_col].str.strip() != '']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "303e546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "filename = \"semantic_scholar_2025_02_14_reliability_resilience_power_systems_results.csv\"\n",
    "filepath = os.path.join(\"Saved_files\", filename)\n",
    "df = pd.read_csv(filepath, sep=\";\")\n",
    "df['text'] = df['title'].fillna('') + ' ' + df['abstract'].fillna('')\n",
    "search_keywords = extract_keywords_from_filename(filename)\n",
    "df = preprocess_dataframe(df, text_col='text', search_keywords=search_keywords)\n",
    "\n",
    "def clean_fields_of_study(s):\n",
    "    valid_fields = ['Computer Science', 'Economics', 'Engineering', 'Physics', 'Mathematics','Medicine','Business','Environmental Science','Chemistry','Materials Science','Geography','Biology','Geology','Political Science','Psychology','Com']\n",
    "    if pd.isna(s) or s == '[]':\n",
    "        return [\"Unknown\"]\n",
    "    if isinstance(s, str):\n",
    "        fields = [field.strip().strip(\"'\\\"\") for field in s.strip('[]').split(',')]\n",
    "        return [f if f in valid_fields else \"Unknown\" for f in fields] or [\"Unknown\"]\n",
    "    return [\"Unknown\"]\n",
    "df['fieldsOfStudy'] = df['fieldsOfStudy'].apply(clean_fields_of_study)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afad3587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 11:34:01,842 - INFO - collecting all words and their counts\n",
      "2025-08-19 11:34:01,845 - INFO - PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2025-08-19 11:34:03,737 - INFO - PROGRESS: at sentence #10000, processed 1528760 words and 873903 word types\n",
      "2025-08-19 11:34:05,685 - INFO - PROGRESS: at sentence #20000, processed 3012255 words and 1486274 word types\n",
      "2025-08-19 11:34:07,222 - INFO - collected 1928697 token types (unigram + bigrams) from a corpus of 4300658 words and 28934 sentences\n",
      "2025-08-19 11:34:07,223 - INFO - merged Phrases<1928697 vocab, min_count=10, threshold=50, max_vocab_size=40000000>\n",
      "2025-08-19 11:34:07,225 - INFO - Phrases lifecycle event {'msg': 'built Phrases<1928697 vocab, min_count=10, threshold=50, max_vocab_size=40000000> in 5.38s', 'datetime': '2025-08-19T11:34:07.225234', 'gensim': '4.3.2', 'python': '3.11.13 (main, Jun 12 2025, 12:41:34) [MSC v.1943 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'created'}\n",
      "2025-08-19 11:34:07,226 - INFO - collecting all words and their counts\n",
      "2025-08-19 11:34:07,227 - INFO - PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2025-08-19 11:34:11,239 - INFO - PROGRESS: at sentence #10000, processed 1474857 words and 896239 word types\n",
      "2025-08-19 11:34:15,314 - INFO - PROGRESS: at sentence #20000, processed 2897746 words and 1537900 word types\n",
      "2025-08-19 11:34:18,860 - INFO - collected 2008055 token types (unigram + bigrams) from a corpus of 4127557 words and 28934 sentences\n",
      "2025-08-19 11:34:18,861 - INFO - merged Phrases<2008055 vocab, min_count=5, threshold=50, max_vocab_size=40000000>\n",
      "2025-08-19 11:34:18,862 - INFO - Phrases lifecycle event {'msg': 'built Phrases<2008055 vocab, min_count=5, threshold=50, max_vocab_size=40000000> in 11.64s', 'datetime': '2025-08-19T11:34:18.862552', 'gensim': '4.3.2', 'python': '3.11.13 (main, Jun 12 2025, 12:41:34) [MSC v.1943 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def model_topics(df, num_topics=10, num_words=25):\n",
    "    tokenized_texts = df['processed_text'].apply(lambda x: x.split()).tolist()\n",
    "    bigram = Phrases(tokenized_texts, min_count=10, threshold=50, delimiter='_')\n",
    "    trigram = Phrases(bigram[tokenized_texts], threshold=50, delimiter='_')\n",
    "    phrased = []\n",
    "    for doc in tokenized_texts:\n",
    "        bigrams_ = [w for w in bigram[doc] if '_' in w]\n",
    "        trigrams_ = [w for w in trigram[bigram[doc]] if '_' in w]\n",
    "        combined = doc + bigrams_ + trigrams_\n",
    "        phrased.append(' '.join(combined))\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 1), token_pattern=r'\\b[\\w_-]+\\b', max_df=0.95, min_df=2, max_features=10000)\n",
    "    doc_term_matrix = vectorizer.fit_transform(phrased)\n",
    "    lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "    topic_distributions = lda_model.fit_transform(doc_term_matrix)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topic_keywords = {}\n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        top_indices = topic.argsort()[:-num_words-1:-1]\n",
    "        top_words = [feature_names[i] for i in top_indices]\n",
    "        word_weights = [(feature_names[i], topic[i]) for i in top_indices]\n",
    "        topic_keywords[topic_idx] = {'top_words': top_words, 'word_weights': word_weights}\n",
    "    return lda_model, vectorizer, topic_distributions, df, topic_keywords\n",
    "\n",
    "lda_model, vectorizer, topic_distributions, df_topic, topic_keywords = model_topics(df, num_topics=10, num_words=25)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db0e5141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate_terms(df, text_col='processed_text', max_features=1000):\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 3), max_df=0.95, min_df=2, max_features=max_features, token_pattern=r'\\b[\\w-]+\\b')\n",
    "    matrix = vectorizer.fit_transform(df[text_col].fillna(''))\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    freqs = matrix.sum(axis=0).A1\n",
    "    return [term for term, freq in sorted(zip(terms, freqs), key=lambda x: x[1], reverse=True)]\n",
    "\n",
    "def get_method_phrases(corpus_terms, client, model_type, credit_tracker):\n",
    "    sample_terms = ', '.join(corpus_terms[:50])\n",
    "    prompt = f\"\"\"Here are the most frequent terms from a corpus of scientific papers:\n",
    "{sample_terms}\n",
    "From the full list: {', '.join(corpus_terms)}\n",
    "Extract ONLY the terms that represent specific methodologies, techniques, or named approaches that would actually appear in this type of engineering research. Focus on:\n",
    "- Power system analysis methods\n",
    "- Reliability analysis techniques  \n",
    "- Engineering design approaches\n",
    "- Computational methods used in power/electrical engineering\n",
    "- Statistical methods for engineering\n",
    "\n",
    "Do NOT include: generic words like \"analysis\", \"method\", \"approach\", \"design\", \"system\" by themselves.\n",
    "DO include: specific named methods like \"monte carlo simulation\", \"load flow analysis\", \"reliability assessment\", loss of load probability, probabilitstic methods, etc.\n",
    "\n",
    "Return as a simple Python list of strings, no code blocks or formatting.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_type,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    try:\n",
    "        return ast.literal_eval(response.choices[0].message.content)\n",
    "    except:\n",
    "        content = response.choices.message.content\n",
    "        content = content.replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", '')\n",
    "        return [term.strip() for term in content.split(',') if len(term.strip()) > 3]\n",
    "\n",
    "def clean_method_phrases_fixed(method_phrases):\n",
    "    cleaned_phrases = []\n",
    "    for phrase in method_phrases:\n",
    "        cleaned = phrase.strip().replace('``````','').replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", '').replace('\\n', ' ')\n",
    "        cleaned = ' '.join(cleaned.split())\n",
    "        if len(cleaned) > 2:\n",
    "            cleaned_phrases.append(cleaned.lower())\n",
    "    return list(set(cleaned_phrases))\n",
    "\n",
    "# Normalize and augment method vocabulary\n",
    "def stable_normalize_augment_vocab(method_phrases):\n",
    "    base = [p.lower().strip() for p in method_phrases if isinstance(p, str) and p.strip()]\n",
    "    variants = set()\n",
    "    for p in base:\n",
    "        variants.add(p)\n",
    "        variants.add(p.replace('-', ' '))\n",
    "        variants.add(p.replace('_', ' '))\n",
    "        if ' ' in p:\n",
    "            variants.add(p.replace(' ', '_'))\n",
    "        # canonical variants (extend as needed)\n",
    "        if p == 'optimal power flow':\n",
    "            variants.update(['opf','security-constrained opf','scopf'])\n",
    "        if p == 'monte carlo simulation':\n",
    "            variants.update(['mc simulation'])\n",
    "        if p == 'load flow analysis':\n",
    "            variants.update(['power flow','ac power flow','dc power flow'])\n",
    "        if p == 'state estimation':\n",
    "            variants.update(['wls state estimation','kalman filter','extended kalman','unscented kalman'])\n",
    "        if p == 'contingency analysis':\n",
    "            variants.update(['n-1 security','n-1 contingency'])\n",
    "        if p == 'unit commitment':\n",
    "            variants.update(['economic dispatch','security-constrained unit commitment'])\n",
    "    # Create stable, reproducible order\n",
    "    vocab = sorted(set(variants))\n",
    "    return vocab\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7718855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidate terms: 15000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 11:37:30,351 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented (stable) vocab size: 93\n",
      "Top method phrases: ['ac power flow', 'backward/forward compatibility', 'backward/forward_compatibility', 'constraint optimization', 'constraint_optimization', 'cost benefit analysis', 'cost-benefit analysis', 'cost-benefit_analysis', 'data driven approach', 'data-driven approach', 'data-driven_approach', 'dc power flow', 'design for reliability', 'design of experiments', 'design_for_reliability']\n"
     ]
    }
   ],
   "source": [
    "#Run thorugh method extraction\n",
    "candidate_terms = extract_candidate_terms(df, text_col='processed_text', max_features=15000)\n",
    "print(\"Number of candidate terms:\", len(candidate_terms))\n",
    "method_phrases = get_method_phrases(candidate_terms, client, model_type, credit_tracker)\n",
    "method_phrases = clean_method_phrases_fixed(method_phrases)\n",
    "method_phrases_aug = stable_normalize_augment_vocab(method_phrases)\n",
    "print(\"Augmented (stable) vocab size:\", len(method_phrases_aug))\n",
    "print(\"Top method phrases:\", method_phrases_aug[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0440dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Methods for method assignment with TF-IDF and LDA\n",
    "\n",
    "# --- TF-IDF Method Assignment ---\n",
    "def tfidf_method_assignment(df, method_phrases, processed_col='processed_text', min_score=0.005):\n",
    "    logger.info(f\"Assigning primary methods using TF-IDF with {len(method_phrases)} phrases.\")\n",
    "    # Use augmented vocab\n",
    "    vocab = method_phrases\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        vocabulary=vocab,\n",
    "        ngram_range=(1, 3),\n",
    "        min_df=1,\n",
    "        max_df=0.999,               # allow very common terms if they’re in vocabulary\n",
    "        norm='l2',\n",
    "        token_pattern=r'\\b[\\w_-]+\\b'\n",
    "    )\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[processed_col])\n",
    "    method_scores = tfidf_matrix.max(axis=1).toarray().ravel()\n",
    "    argmax_indices = tfidf_matrix.argmax(axis=1).A1\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    assigned = []\n",
    "    for i, score in zip(argmax_indices, method_scores):\n",
    "        if score >= min_score:\n",
    "            assigned.append(feature_names[i])\n",
    "        else:\n",
    "            assigned.append('LowConfidence')\n",
    "\n",
    "    df['Primary_Method_TFIDF'] = assigned\n",
    "    df['Method_TFIDF_Score'] = method_scores\n",
    "\n",
    "    # Optional: store top-k labels\n",
    "    topk = 3\n",
    "    mat = tfidf_matrix.toarray()\n",
    "    top_idx = np.argsort(mat, axis=1)[:, -topk:]\n",
    "    for k in range(1, topk+1):\n",
    "        col_name = f'Top_{k}_TFIDF_Methods'\n",
    "        df[col_name] = [[feature_names[j] for j in idxs[::-1][:k]] for idxs in top_idx]\n",
    "\n",
    "    # Diagnostics\n",
    "    nonzero_docs = int((method_scores > 0).sum())\n",
    "    logger.info(f\"TF-IDF nonzero docs: {nonzero_docs}/{len(df)} ({100*nonzero_docs/len(df):.1f}%)\")\n",
    "    return df\n",
    "\n",
    "def compute_tfidf_matrix(df, vocab, processed_col='processed_text'):\n",
    "    vect = TfidfVectorizer(\n",
    "        vocabulary=vocab,              # stable order retained\n",
    "        ngram_range=(1,3),\n",
    "        min_df=1,\n",
    "        max_df=0.999,\n",
    "        norm='l2',\n",
    "        token_pattern=r'\\b[\\w_-]+\\b'\n",
    "    )\n",
    "    X = vect.fit_transform(df[processed_col])\n",
    "    feature_names = vect.get_feature_names_out().tolist()  # stable order matches vocab order\n",
    "    # Normalize row-wise to [0,1] for combining (optional)\n",
    "    max_per_doc = X.max(axis=1).toarray().ravel()\n",
    "    # Avoid division by zero\n",
    "    max_per_doc[max_per_doc == 0] = 1.0\n",
    "    X_norm = X.multiply(1.0 / max_per_doc.reshape(-1,1))\n",
    "    return X, X_norm, feature_names\n",
    "\n",
    "\n",
    "# --- LDA-based Method Assignment ---\n",
    "def lda_method_assignment(\n",
    "    df, method_phrases, processed_col='processed_text',\n",
    "    max_method_topics=20,          # keep topics moderate\n",
    "    min_papers_per_topic=2,        # allow rarer topics\n",
    "    extra_gate=0.00                # was +0.03, now relaxed\n",
    "):\n",
    "    vocab = method_phrases\n",
    "    vectorizer = CountVectorizer(\n",
    "        vocabulary=vocab,\n",
    "        ngram_range=(1, 3),\n",
    "        token_pattern=r'\\b[\\w_-]+\\b'\n",
    "    )\n",
    "    doc_term_matrix = vectorizer.fit_transform(df[processed_col])\n",
    "    n_method_topics = len(vocab)\n",
    "    if n_method_topics < 2:\n",
    "        logger.warning(\"Not enough method phrases for LDA method assignment. Skipping.\")\n",
    "        df['Primary_Method_LDA'] = 'No_Method_Found'\n",
    "        df['Method_LDA_Score'] = 0.0\n",
    "        return df\n",
    "\n",
    "    n_components = min(max_method_topics, n_method_topics)\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=n_components,\n",
    "        learning_method='batch',\n",
    "        random_state=42,\n",
    "        max_iter=30\n",
    "    )\n",
    "    doc_topic_dist = lda.fit_transform(doc_term_matrix)\n",
    "    # Map topics to vocabulary indices via component highest-weight terms (approximate label)\n",
    "    vocab_list = list(vectorizer.vocabulary_.keys())\n",
    "    # Label each LDA topic by its top token in vocab_list\n",
    "    topic_labels = []\n",
    "    for t in range(n_components):\n",
    "        comp = lda.components_[t]\n",
    "        top_idx = comp.argmax()\n",
    "        # We need the term string at index top_idx in the vectorizer feature order\n",
    "        feat_names = vectorizer.get_feature_names_out()\n",
    "        topic_labels.append(feat_names[top_idx])\n",
    "\n",
    "    best_topic_idx = doc_topic_dist.argmax(axis=1)\n",
    "    best_topic_val = doc_topic_dist[np.arange(len(df)), best_topic_idx]\n",
    "    gate = (1.0 / n_components) + extra_gate\n",
    "\n",
    "    # Count topic assignments\n",
    "    topic_assignment_counts = pd.Series(best_topic_idx).value_counts()\n",
    "    rare_topics = set(topic_assignment_counts[topic_assignment_counts < min_papers_per_topic].index.tolist())\n",
    "\n",
    "    assigned_methods = []\n",
    "    for j, i in enumerate(best_topic_idx):\n",
    "        label = topic_labels[i]\n",
    "        if best_topic_val[j] > gate and (i not in rare_topics):\n",
    "            assigned_methods.append(label)\n",
    "        else:\n",
    "            assigned_methods.append('LowConfidence')\n",
    "\n",
    "    df['Primary_Method_LDA'] = assigned_methods\n",
    "    df['Method_LDA_Score'] = best_topic_val\n",
    "\n",
    "    # Store top-3 topic labels and scores per doc (optional)\n",
    "    topk = 3\n",
    "    top_idx = np.argsort(doc_topic_dist, axis=1)[:, -topk:]\n",
    "    df['Top_3_Methods_LDA'] = [[topic_labels[i] for i in idxs[::-1]] for idxs in top_idx]\n",
    "    df['Top_3_Methods_LDA_Scores'] = [\n",
    "        [float(doc_topic_dist[row_i, i]) for i in idxs[::-1]] for row_i, idxs in enumerate(top_idx)\n",
    "    ]\n",
    "\n",
    "    # Diagnostics\n",
    "    non_low = (df['Primary_Method_LDA'] != 'LowConfidence').sum()\n",
    "    logger.info(f\"LDA confident assignments: {non_low}/{len(df)} ({100*non_low/len(df):.1f}%) (gate={gate:.4f}, rare<{min_papers_per_topic})\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_compound_matrix(df, vocab, processed_col='processed_text', window=300, ratio_thresh=0.5):\n",
    "    # Scoring: exact match -> 1.0\n",
    "    # Else if compound: (#sig words present / #sig words) if any two consecutive words appear within window -> >=0.5 typical\n",
    "    n_docs = len(df)\n",
    "    n_terms = len(vocab)\n",
    "    scores = np.zeros((n_docs, n_terms), dtype=np.float32)\n",
    "\n",
    "    docs = df[processed_col].fillna('').str.lower().tolist()\n",
    "\n",
    "    for j, phrase in enumerate(vocab):\n",
    "        phrase_l = phrase.lower()\n",
    "        phrase_words = [w for w in phrase_l.split() if len(w) > 0]\n",
    "        sig_words = [w for w in phrase_words if len(w) > 3]\n",
    "        for i, text in enumerate(docs):\n",
    "            if phrase_l in text:\n",
    "                scores[i, j] = 1.0\n",
    "                continue\n",
    "            if len(phrase_words) > 1:\n",
    "                # coverage score\n",
    "                present = sum(1 for w in sig_words if w in text) if sig_words else 0\n",
    "                if sig_words:\n",
    "                    coverage = present / len(sig_words)\n",
    "                else:\n",
    "                    coverage = 0.0\n",
    "                # proximity check on consecutive words\n",
    "                prox_hit = False\n",
    "                for k in range(len(phrase_words)-1):\n",
    "                    w1 = phrase_words[k]\n",
    "                    w2 = phrase_words[k+1]\n",
    "                    pos = text.find(w1)\n",
    "                    if pos >= 0:\n",
    "                        nearby = text[pos:pos+window]\n",
    "                        if w2 in nearby:\n",
    "                            prox_hit = True\n",
    "                            break\n",
    "                if coverage >= ratio_thresh or prox_hit:\n",
    "                    scores[i, j] = max(scores[i, j], min(1.0, 0.6 + 0.4*coverage))  # lift to 0.6..1.0\n",
    "            else:\n",
    "                # single word long technical token\n",
    "                if len(phrase_l) > 6 and phrase_l in text:\n",
    "                    scores[i, j] = max(scores[i, j], 0.7)\n",
    "    # Row-wise normalization to [0,1] for combining\n",
    "    row_max = scores.max(axis=1, keepdims=True)\n",
    "    row_max[row_max == 0] = 1.0\n",
    "    scores_norm = scores / row_max\n",
    "    return scores, scores_norm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cb53e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods for compound phrases\n",
    "def assign_methods_compound(df, method_phrases, processed_col='processed_text', min_confidence=0.05):\n",
    "    all_text = ' '.join(df[processed_col]).lower()\n",
    "    validated_phrases = []\n",
    "\n",
    "    def strict_phrase_validation(phrase, all_text):\n",
    "        phrase_lower = phrase.lower()\n",
    "        if phrase_lower in all_text:\n",
    "            return True, 'exact'\n",
    "        words = phrase_lower.split()\n",
    "        # Lower threshold from 0.6 to 0.5\n",
    "        if len(words) > 1:\n",
    "            sig_words = [w for w in words if len(w) > 3]\n",
    "            if not sig_words:\n",
    "                return False, 'none'\n",
    "            word_matches = sum(1 for w in sig_words if w in all_text)\n",
    "            if word_matches >= max(1, int(len(sig_words) * 0.5)):\n",
    "                return True, 'compound'\n",
    "        if len(words) == 1 and len(phrase_lower) > 6:\n",
    "            generic_terms = {'analysis','method','approach','technique','system','design','study','evaluation','assessment','processing'}\n",
    "            if phrase_lower not in generic_terms and phrase_lower in all_text:\n",
    "                return True, 'technical'\n",
    "        return False, 'none'\n",
    "\n",
    "    for phrase in method_phrases:\n",
    "        is_valid, _ = strict_phrase_validation(phrase, all_text)\n",
    "        if is_valid:\n",
    "            validated_phrases.append(phrase)\n",
    "    logger.info(f\"Validated {len(validated_phrases)} method phrases (compound).\")\n",
    "\n",
    "    def targeted_contains_method(text, validated_phrases):\n",
    "        text_lower = text.lower()\n",
    "        exact_matches = [phrase for phrase in validated_phrases if phrase.lower() in text_lower]\n",
    "        if exact_matches:\n",
    "            return True, exact_matches\n",
    "        # Wider proximity window: 300 chars\n",
    "        for phrase in validated_phrases:\n",
    "            words = phrase.lower().split()\n",
    "            if len(words) > 1:\n",
    "                for i, word in enumerate(words[:-1]):\n",
    "                    if word in text_lower:\n",
    "                        word_pos = text_lower.find(word)\n",
    "                        next_word = words[i+1]\n",
    "                        nearby_text = text_lower[word_pos:word_pos+300]\n",
    "                        if next_word in nearby_text:\n",
    "                            return True, [phrase]\n",
    "        return False, []\n",
    "\n",
    "    classification_data = []\n",
    "    for idx, row in df.iterrows():\n",
    "        has_method, found_phrases = targeted_contains_method(row[processed_col], validated_phrases)\n",
    "        classification_data.append({'index': idx, 'has_method': has_method, 'found_phrases': found_phrases})\n",
    "    docs_to_classify_indices = [d['index'] for d in classification_data if d['has_method']]\n",
    "    docs_to_classify = df.loc[docs_to_classify_indices].copy()\n",
    "    logger.info(f\"Compound: docs to classify: {len(docs_to_classify)}\")\n",
    "\n",
    "    if len(docs_to_classify) > 0 and len(validated_phrases) > 0:\n",
    "        method_tfidf_vectorizer = TfidfVectorizer(\n",
    "            vocabulary=validated_phrases,\n",
    "            ngram_range=(1, 3),\n",
    "            min_df=1,\n",
    "            max_df=0.999,\n",
    "            sublinear_tf=True,\n",
    "            norm='l2',\n",
    "            token_pattern=r'\\b[\\w_-]+\\b'\n",
    "        )\n",
    "        method_tfidf_matrix = method_tfidf_vectorizer.fit_transform(docs_to_classify[processed_col])\n",
    "        method_scores_matrix = method_tfidf_matrix.toarray()\n",
    "        feature_names = method_tfidf_vectorizer.get_feature_names_out()\n",
    "        method_scores_max = method_scores_matrix.max(axis=1)\n",
    "        argmax_indices = method_scores_matrix.argmax(axis=1)\n",
    "        primary_methods = [feature_names[i] for i in argmax_indices]\n",
    "\n",
    "        final_methods = []\n",
    "        confidence_flags = []\n",
    "        for method, score in zip(primary_methods, method_scores_max):\n",
    "            final_methods.append(method)\n",
    "            if score == 0.0:\n",
    "                confidence_flags.append(\"NoEvidence\")\n",
    "            elif score < min_confidence:\n",
    "                confidence_flags.append(\"Low\")\n",
    "            else:\n",
    "                confidence_flags.append(\"High\")\n",
    "\n",
    "        df.loc[docs_to_classify.index, 'Method_Detected_Compound'] = final_methods\n",
    "        df.loc[docs_to_classify.index, 'Method_Compound_Score'] = method_scores_max\n",
    "        df.loc[docs_to_classify.index, 'Method_Compound_Confidence'] = confidence_flags\n",
    "    else:\n",
    "        logger.warning(\"No documents or validated phrases for compound classification.\")\n",
    "\n",
    "    unclassified_idx = df.index.difference(docs_to_classify.index)\n",
    "    df.loc[unclassified_idx, 'Method_Detected_Compound'] = 'No_Method_Found'\n",
    "    df.loc[unclassified_idx, 'Method_Compound_Score'] = 0.0\n",
    "    df.loc[unclassified_idx, 'Method_Compound_Confidence'] = 'NotClassified'\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1da1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def combined_method_assignment_with_compound(df):\n",
    "    final_method = []\n",
    "    final_confidence = []\n",
    "    for i, row in df.iterrows():\n",
    "        compound_conf = row.get('Method_Compound_Confidence', 'NotClassified')\n",
    "        compound_method = row.get('Method_Detected_Compound', 'No_Method_Found')\n",
    "        tfidf_method = row.get('Primary_Method_TFIDF', 'LowConfidence')\n",
    "        tfidf_score = row.get('Method_TFIDF_Score', 0.0)\n",
    "        lda_method = row.get('Primary_Method_LDA', 'LowConfidence')\n",
    "\n",
    "        if compound_conf == \"High\":\n",
    "            final_method.append(compound_method)\n",
    "            final_confidence.append('compound_high')\n",
    "        elif tfidf_method != 'LowConfidence':\n",
    "            final_method.append(tfidf_method)\n",
    "            final_confidence.append('tfidf_confident')\n",
    "        elif lda_method != 'LowConfidence':\n",
    "            final_method.append(lda_method)\n",
    "            final_confidence.append('lda_confident')\n",
    "        elif compound_conf == \"Low\" and (pd.isna(tfidf_score) or float(tfidf_score) == 0.0):\n",
    "            # Prefer compound_low if TF-IDF score is zero\n",
    "            final_method.append(compound_method)\n",
    "            final_confidence.append('compound_low')\n",
    "        else:\n",
    "            final_method.append('LowConfidence')\n",
    "            final_confidence.append('low')\n",
    "\n",
    "    df['Final_Method_Label'] = final_method\n",
    "    df['Final_Method_Confidence'] = final_confidence\n",
    "    return df\n",
    "\n",
    "def combine_scores_to_confidence(tfidf_norm, compound_norm, feature_names,\n",
    "                                 w_tfidf=0.6, w_comp=0.4,\n",
    "                                 th_super=0.85, th_high=0.6, th_low=0.2):\n",
    "    # combined = w_tfidf*tfidf + w_comp*compound\n",
    "    Xc = tfidf_norm.multiply(w_tfidf).toarray() + (compound_norm * w_comp)\n",
    "    # Confidence tiers\n",
    "    # super_high >= th_super, high >= th_high, low >= th_low, else not_detected\n",
    "    n_docs, n_terms = Xc.shape\n",
    "    labels = np.full((n_docs, n_terms), 'not_detected', dtype=object)\n",
    "    labels[Xc >= th_low] = 'low'\n",
    "    labels[Xc >= th_high] = 'high'\n",
    "    labels[Xc >= th_super] = 'super_high'\n",
    "    return Xc, labels  # combined score, confidence label matrix (same shape as Xc)\n",
    "\n",
    "def attach_multilabel_to_df(df, feature_names, combined_scores, confidence_labels, topk=40):\n",
    "    n_docs = combined_scores.shape[0]\n",
    "    top_methods = []\n",
    "    top_conf = []\n",
    "    top_scores = []\n",
    "    for i in range(n_docs):\n",
    "        row = combined_scores[i]\n",
    "        idxs = np.argsort(row)[-topk:][::-1]\n",
    "        methods_i = [feature_names[j] for j in idxs]\n",
    "        conf_i = [confidence_labels[i, j] for j in idxs]\n",
    "        scores_i = [float(row[j]) for j in idxs]\n",
    "        top_methods.append(methods_i)\n",
    "        top_conf.append(conf_i)\n",
    "        top_scores.append(scores_i)\n",
    "    df['ML_TopMethods'] = top_methods\n",
    "    df['ML_TopConf'] = top_conf\n",
    "    df['ML_TopScores'] = top_scores\n",
    "    return df\n",
    "\n",
    "def stable_single_label_from_multilabel(df, feature_names, combined_scores, confidence_labels):\n",
    "    \"\"\"\n",
    "    Pick a single deterministic method label per document from multi-label matrices.\n",
    "    - combined_scores: np.ndarray (n_docs x n_terms), higher is better\n",
    "    - confidence_labels: np.ndarray (n_docs x n_terms) in {super_high, high, low, not_detected}\n",
    "    Deterministic tie-breaking: score desc, tier desc, method name asc.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # Shape guards\n",
    "    if not isinstance(combined_scores, np.ndarray):\n",
    "        combined_scores = np.asarray(combined_scores)\n",
    "    if not isinstance(confidence_labels, np.ndarray):\n",
    "        confidence_labels = np.asarray(confidence_labels)\n",
    "\n",
    "    if combined_scores.ndim != 2:\n",
    "        raise ValueError(f\"combined_scores must be 2D, got shape {combined_scores.shape}\")\n",
    "    if confidence_labels.ndim != 2:\n",
    "        raise ValueError(f\"confidence_labels must be 2D, got shape {confidence_labels.shape}\")\n",
    "    if combined_scores.shape != confidence_labels.shape:\n",
    "        raise ValueError(f\"Shape mismatch: combined_scores {combined_scores.shape} vs confidence_labels {confidence_labels.shape}\")\n",
    "\n",
    "    n_docs, n_terms = combined_scores.shape\n",
    "    if n_terms != len(feature_names):\n",
    "        raise ValueError(f\"Feature names length {len(feature_names)} != number of terms {n_terms}\")\n",
    "\n",
    "    # Confidence tier ranking (higher is better)\n",
    "    tier_rank = {'super_high': 3, 'high': 2, 'low': 1, 'not_detected': 0}\n",
    "\n",
    "    final_label = []\n",
    "    final_conf = []\n",
    "\n",
    "    for i in range(n_docs):\n",
    "        row_scores = combined_scores[i]\n",
    "        row_labels = confidence_labels[i]\n",
    "\n",
    "        # Build tuples: (method_name, score, confidence)\n",
    "        tuples = [(feature_names[j], float(row_scores[j]), row_labels[j]) for j in range(n_terms)]\n",
    "\n",
    "        # Sort by: score desc, confidence tier desc, method name asc\n",
    "        # IMPORTANT: tier uses x[2] (confidence), not x[1]\n",
    "        tuples_sorted = sorted(\n",
    "            tuples,\n",
    "            key=lambda x: (-x[1], -tier_rank.get(x[2], 0), x)\n",
    "        )\n",
    "\n",
    "        # Take the first tuple ONLY, then unpack it\n",
    "        top_method, top_score, top_conf = tuples_sorted[0]\n",
    "\n",
    "        if top_conf != 'not_detected':\n",
    "            final_label.append(top_method)\n",
    "            final_conf.append(top_conf)\n",
    "        else:\n",
    "            final_label.append('LowConfidence')\n",
    "            final_conf.append('low')\n",
    "\n",
    "    df['Final_Method_Label'] = final_label\n",
    "    df['Final_Method_Confidence'] = final_conf\n",
    "    return df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b485f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 11:37:30,490 - INFO - Assigning primary methods using TF-IDF with 93 phrases.\n",
      "2025-08-19 11:37:36,782 - INFO - TF-IDF nonzero docs: 4361/28934 (15.1%)\n",
      "2025-08-19 11:38:10,899 - INFO - LDA confident assignments: 4360/28934 (15.1%) (gate=0.0200, rare<3)\n",
      "2025-08-19 11:38:12,910 - INFO - Validated 55 method phrases (compound).\n",
      "2025-08-19 11:38:17,724 - INFO - Compound: docs to classify: 22062\n"
     ]
    }
   ],
   "source": [
    "#Workflow to run through method assignment with TF-IDF and LDA and compound methods\n",
    "\n",
    "#Run through method assignment with TF-IDF and LDA\n",
    "df = tfidf_method_assignment(df, method_phrases_aug, processed_col='processed_text', min_score=0.01)\n",
    "df = lda_method_assignment(df, method_phrases_aug, processed_col='processed_text', max_method_topics=50, min_papers_per_topic=3)\n",
    "\n",
    "# Run after TFIDF/LDA assignment:\n",
    "df = assign_methods_compound(df, method_phrases_aug, processed_col='processed_text', min_confidence=0.005)\n",
    "#Run after TFIDF/LDA assignment with compound methods\n",
    "df = combined_method_assignment_with_compound(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8eccdb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deterministic seeds\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Build matrices\n",
    "X_tfidf, X_tfidf_norm, feat = compute_tfidf_matrix(df, method_phrases_aug, processed_col='processed_text')\n",
    "X_compound, X_compound_norm = compute_compound_matrix(df, method_phrases_aug, processed_col='processed_text', window=300, ratio_thresh=0.5)\n",
    "\n",
    "# Combine and label\n",
    "X_combined, conf_labels = combine_scores_to_confidence(\n",
    "    X_tfidf_norm, X_compound_norm, feat,\n",
    "    w_tfidf=0.6, w_comp=0.4,\n",
    "    th_super=0.85, th_high=0.6, th_low=0.2\n",
    ")\n",
    "\n",
    "# Attach multi-label summaries and deterministic single-label\n",
    "df = attach_multilabel_to_df(df, feat, X_combined, conf_labels, topk=5)\n",
    "df = stable_single_label_from_multilabel(df, feat, X_combined, conf_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f523416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF nonzero docs: 4361/28934 (15.1%)\n",
      "Compound nonzero docs: 28349/28934 (98.0%)\n",
      "Confidence label counts (matrix-level): {'high': 578, 'low': 303355, 'not_detected': 2382173, 'super_high': 4756}\n",
      "Final confidence distribution (per doc):\n",
      "Final_Method_Confidence\n",
      "low           24573\n",
      "super_high     4361\n",
      "Name: count, dtype: int64\n",
      "Top methods by avg combined score:\n",
      "  power flow: 0.344\n",
      "  dc power flow: 0.323\n",
      "  ac power flow: 0.323\n",
      "  design for reliability: 0.255\n",
      "  reliability optimization: 0.232\n",
      "  reliability assessment: 0.220\n",
      "  safety analysis: 0.107\n",
      "  proportional-integral control: 0.101\n",
      "  load forecasting: 0.100\n",
      "  design of experiments: 0.099\n",
      "  sensitivity analysis: 0.099\n",
      "  mc simulation: 0.093\n",
      "  real-time simulation: 0.092\n",
      "  regression analysis: 0.091\n",
      "  cost-benefit analysis: 0.090\n",
      "  optimal power flow: 0.088\n",
      "  data-driven approach: 0.076\n",
      "  detection algorithm: 0.075\n",
      "  constraint optimization: 0.073\n",
      "  real time simulation: 0.070\n"
     ]
    }
   ],
   "source": [
    "def stability_diagnostics(df, feat, X_tfidf, X_compound, X_combined, conf_labels):\n",
    "    total = X_combined.shape[0]\n",
    "    tfidf_nonzero = int((X_tfidf.max(axis=1).toarray().ravel() > 0).sum())\n",
    "    print(f\"TF-IDF nonzero docs: {tfidf_nonzero}/{total} ({100*tfidf_nonzero/total:.1f}%)\")\n",
    "    cmp_nonzero = int((X_compound.max(axis=1) > 0).sum())\n",
    "    print(f\"Compound nonzero docs: {cmp_nonzero}/{total} ({100*cmp_nonzero/total:.1f}%)\")\n",
    "\n",
    "    # Confidence distribution (flattened)\n",
    "    unique, counts = np.unique(conf_labels, return_counts=True)\n",
    "    dist = dict(zip(unique, counts))\n",
    "    print(\"Confidence label counts (matrix-level):\", dist)\n",
    "\n",
    "    if 'Final_Method_Confidence' in df.columns:\n",
    "        print(\"Final confidence distribution (per doc):\")\n",
    "        print(df['Final_Method_Confidence'].value_counts())\n",
    "\n",
    "    # Top-20 methods by average combined score\n",
    "    avg_scores = X_combined.mean(axis=0)\n",
    "    top_idx = np.argsort(avg_scores)[-20:][::-1]\n",
    "    top_methods = [(feat[j], float(avg_scores[j])) for j in top_idx]\n",
    "    print(\"Top methods by avg combined score:\")\n",
    "    for m, s in top_methods:\n",
    "        print(f\"  {m}: {s:.3f}\")\n",
    "\n",
    "stability_diagnostics(df, feat, X_tfidf, X_compound, X_combined, conf_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13ac8473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Diagnostics ===\n",
      "TF-IDF: nonzero-score docs: 4361/28934 (15.1%)\n",
      "LDA: confident assignments: 4360/28934 (15.1%)\n",
      "Compound confidence counts:\n",
      "Method_Compound_Confidence\n",
      "NoEvidence       17701\n",
      "NotClassified     6872\n",
      "High              4361\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final confidence distribution:\n",
      "Final_Method_Confidence\n",
      "low           24573\n",
      "super_high     4361\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top final methods:\n",
      "Final_Method_Label\n",
      "ac power flow                     7367\n",
      "design for reliability            4435\n",
      "mc simulation                     3508\n",
      "cost benefit analysis             1262\n",
      "energy management optimization    1001\n",
      "power flow                         842\n",
      "data driven approach               763\n",
      "optimal power flow                 732\n",
      "reliability assessment             697\n",
      "real time simulation               663\n",
      "machine learning                   641\n",
      "constraint optimization            628\n",
      "LowConfidence                      585\n",
      "reliability optimization           441\n",
      "particle swarm optimization        437\n",
      "statistical process control        370\n",
      "load flow analysis                 313\n",
      "monte carlo simulation             296\n",
      "detection algorithm                282\n",
      "time series analysis               280\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Docs with TF-IDF score==0 but Compound detected: 0\n",
      "Empty DataFrame\n",
      "Columns: [Final_Method_Label, Final_Method_Confidence, Primary_Method_TFIDF, Primary_Method_LDA, Method_Detected_Compound, Method_Compound_Confidence]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Diagnostics & coverage report\n",
    "def method_diagnostics_report(df):\n",
    "    total = len(df)\n",
    "    print(\"=== Diagnostics ===\")\n",
    "    # TF-IDF\n",
    "    tfidf_nonzero = int((df['Method_TFIDF_Score'] > 0).sum()) if 'Method_TFIDF_Score' in df.columns else 0\n",
    "    print(f\"TF-IDF: nonzero-score docs: {tfidf_nonzero}/{total} ({100*tfidf_nonzero/total:.1f}%)\")\n",
    "\n",
    "    # LDA\n",
    "    lda_confident = int((df['Primary_Method_LDA'] != 'LowConfidence').sum()) if 'Primary_Method_LDA' in df.columns else 0\n",
    "    print(f\"LDA: confident assignments: {lda_confident}/{total} ({100*lda_confident/total:.1f}%)\")\n",
    "\n",
    "    # Compound\n",
    "    cmp_col = 'Method_Compound_Confidence'\n",
    "    if cmp_col in df.columns:\n",
    "        cmp_counts = df[cmp_col].value_counts()\n",
    "        print(\"Compound confidence counts:\")\n",
    "        print(cmp_counts)\n",
    "\n",
    "    # Final\n",
    "    if 'Final_Method_Confidence' in df.columns:\n",
    "        print(\"\\nFinal confidence distribution:\")\n",
    "        print(df['Final_Method_Confidence'].value_counts())\n",
    "    if 'Final_Method_Label' in df.columns:\n",
    "        print(\"\\nTop final methods:\")\n",
    "        print(df['Final_Method_Label'].value_counts().head(20))\n",
    "\n",
    "    # Sample spot checks where TF-IDF score == 0 but compound found something\n",
    "    if 'Method_TFIDF_Score' in df.columns and 'Method_Compound_Confidence' in df.columns:\n",
    "        mask = (df['Method_TFIDF_Score'] == 0) & (df['Method_Compound_Confidence'].isin(['High','Low']))\n",
    "        print(f\"\\nDocs with TF-IDF score==0 but Compound detected: {mask.sum()}\")\n",
    "        print(df.loc[mask, ['Final_Method_Label','Final_Method_Confidence','Primary_Method_TFIDF','Primary_Method_LDA','Method_Detected_Compound','Method_Compound_Confidence']].head(5))\n",
    "\n",
    "method_diagnostics_report(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b209c504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to Saved_files_new\\semantic_scholar_2025_08_19_final_combined_methods.csv\n",
      "Final method label distribution:\n",
      "Final_Method_Label\n",
      "ac power flow                       7367\n",
      "design for reliability              4435\n",
      "mc simulation                       3508\n",
      "cost benefit analysis               1262\n",
      "energy management optimization      1001\n",
      "power flow                           842\n",
      "data driven approach                 763\n",
      "optimal power flow                   732\n",
      "reliability assessment               697\n",
      "real time simulation                 663\n",
      "machine learning                     641\n",
      "constraint optimization              628\n",
      "LowConfidence                        585\n",
      "reliability optimization             441\n",
      "particle swarm optimization          437\n",
      "statistical process control          370\n",
      "load flow analysis                   313\n",
      "monte carlo simulation               296\n",
      "detection algorithm                  282\n",
      "time series analysis                 280\n",
      "sensitivity analysis                 271\n",
      "multi objective optimization         265\n",
      "fuzzy logic                          240\n",
      "load forecasting                     200\n",
      "safety analysis                      196\n",
      "multi-objective optimization         181\n",
      "state estimation                     175\n",
      "markov decision process              163\n",
      "risk assessment                      159\n",
      "dynamic programming                  157\n",
      "cost-benefit analysis                154\n",
      "data-driven approach                 129\n",
      "sequential monte carlo               115\n",
      "feature extraction                   100\n",
      "quantitative risk analysis            95\n",
      "mixed integer linear programming      93\n",
      "mixed-integer linear programming      92\n",
      "game theory analysis                  79\n",
      "thermal modeling                      75\n",
      "regression analysis                   73\n",
      "proportional integral control         69\n",
      "opf                                   61\n",
      "empirical modeling                    59\n",
      "kalman filter                         57\n",
      "real-time simulation                  35\n",
      "proportional-integral control         27\n",
      "extended kalman                       20\n",
      "dc power flow                         17\n",
      "regression modeling                   10\n",
      "unscented kalman                       9\n",
      "security-constrained opf               5\n",
      "probabilistic methods                  4\n",
      "scopf                                  3\n",
      "backward/forward compatibility         2\n",
      "design of experiments                  1\n",
      "Name: count, dtype: int64\n",
      "Confidence breakdown:\n",
      " Final_Method_Confidence\n",
      "low           24573\n",
      "super_high     4361\n",
      "Name: count, dtype: int64\n",
      "First 3 sample assigned methods:\n",
      "            Final_Method_Label Final_Method_Confidence Primary_Method_TFIDF  \\\n",
      "0       design for reliability                     low        LowConfidence   \n",
      "1       design for reliability                     low        LowConfidence   \n",
      "2                ac power flow                     low        LowConfidence   \n",
      "3  statistical process control                     low        LowConfidence   \n",
      "4                ac power flow                     low        LowConfidence   \n",
      "\n",
      "  Primary_Method_LDA Method_Detected_Compound  \n",
      "0      LowConfidence            ac power flow  \n",
      "1      LowConfidence          No_Method_Found  \n",
      "2      LowConfidence            ac power flow  \n",
      "3      LowConfidence          No_Method_Found  \n",
      "4      LowConfidence            ac power flow  \n",
      "API token usage and cost: {'total_tokens': 0, 'total_cost': 0}\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "current_date = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "output_filename = os.path.join(SAVE_DIR, f\"semantic_scholar_{current_date}_final_combined_methods.csv\")\n",
    "df.to_csv(output_filename, sep=';', encoding='utf-8', quoting=csv.QUOTE_NONNUMERIC, escapechar='\\\\')\n",
    "print(f\"Results saved to {output_filename}\")\n",
    "\n",
    "print(\"Final method label distribution:\")\n",
    "print(df['Final_Method_Label'].value_counts())\n",
    "print(\"Confidence breakdown:\\n\", df['Final_Method_Confidence'].value_counts())\n",
    "print(\"First 3 sample assigned methods:\")\n",
    "print(df[['Final_Method_Label', 'Final_Method_Confidence', 'Primary_Method_TFIDF', 'Primary_Method_LDA', 'Method_Detected_Compound']].head())\n",
    "\n",
    "print(\"API token usage and cost:\", credit_tracker.get_stats())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcaaf48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "literature-search-and-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
