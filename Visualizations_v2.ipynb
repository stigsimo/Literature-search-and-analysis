{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18555e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime\n",
    "\n",
    "SAVE_DIR = \"Saved_files_new\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8f5a2c",
   "metadata": {},
   "source": [
    "# ===== TOPIC VISUALIZATION FUNCTIONS ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba14c46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_topic_name(topic_idx, topic_names):\n",
    "    \"\"\"Fetch topic name using string keys and clean up quotes.\"\"\"\n",
    "    names = topic_names.get('all', topic_names)\n",
    "    name = names.get(str(topic_idx), f\"Topic {topic_idx}\")\n",
    "    return name.strip('\"')\n",
    "\n",
    "def plot_topic_keywords_with_names(topic_keywords, topic_names, top_words=10):\n",
    "    \"\"\"Plot keyword weights for each topic with generated names\"\"\"\n",
    "    num_topics = len(topic_keywords)\n",
    "    fig, axes = plt.subplots(num_topics, 1, figsize=(14, 5*num_topics))\n",
    "    \n",
    "    if num_topics == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (topic_idx, keywords) in enumerate(topic_keywords.items()):\n",
    "        words, weights = zip(*keywords['word_weights'][:top_words])\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        bars = ax.barh(words, weights, color='steelblue', alpha=0.7)\n",
    "        topic_name = get_topic_name(topic_idx, topic_names)\n",
    "        ax.set_title(f'Topic {topic_idx}: {topic_name}', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Word Weight', fontsize=12)\n",
    "        ax.invert_yaxis()\n",
    "        \n",
    "        for bar, weight in zip(bars, weights):\n",
    "            ax.text(weight + 0.001, bar.get_y() + bar.get_height()/2, \n",
    "                   f'{weight:.3f}', va='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_topic_importance_with_names(df_analyzed, topic_names):\n",
    "    \"\"\"Plot topic importance using actual topic assignments\"\"\"\n",
    "    topic_counts = df_analyzed['Primary_Topic'].value_counts()\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bars = plt.bar(range(len(topic_counts)), topic_counts.values, \n",
    "                   color='lightcoral', alpha=0.7)\n",
    "    \n",
    "    plt.xticks(range(len(topic_counts)), topic_counts.index, rotation=45, ha='right')\n",
    "    plt.title('Topic Importance (Number of Papers per Topic)', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Topics', fontsize=12)\n",
    "    plt.ylabel('Number of Papers', fontsize=12)\n",
    "    \n",
    "    for bar, count in zip(bars, topic_counts.values):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                str(count), ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_topic_trends_over_time(df, topic_col='Primary_Topic', year_col='year', top_n=10):\n",
    "    \"\"\"Plot topic trends over time\"\"\"\n",
    "    topic_year = df.groupby([year_col, topic_col]).size().reset_index(name='count')\n",
    "    top_topics = topic_year.groupby(topic_col)['count'].sum().nlargest(top_n).index\n",
    "    topic_year = topic_year[topic_year[topic_col].isin(top_topics)]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=topic_year, x=year_col, y='count', hue=topic_col, marker='o')\n",
    "    plt.title('Topic Trends Over Time')\n",
    "    plt.ylabel('Number of Papers')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb201706",
   "metadata": {},
   "source": [
    "# ===== METHOD VISUALIZATION FUNCTIONS ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c8ed12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_method_importance(df_analyzed, method_col='Method_Detected'):\n",
    "    \"\"\"Plot the number of papers assigned to each method.\"\"\"\n",
    "    # Filter out 'No_Method_Found' for cleaner visualization\n",
    "    df_methods = df_analyzed[df_analyzed[method_col] != 'No_Method_Found']\n",
    "    \n",
    "    if df_methods.empty:\n",
    "        print(\"No methods found for visualization\")\n",
    "        return\n",
    "    \n",
    "    method_counts = df_methods[method_col].value_counts().sort_values(ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bars = plt.bar(range(len(method_counts)), method_counts.values, \n",
    "                   color='mediumseagreen', alpha=0.7)\n",
    "    plt.xticks(range(len(method_counts)), method_counts.index, rotation=45, ha='right')\n",
    "    plt.title('Method Importance (Number of Papers per Method)', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Methods', fontsize=12)\n",
    "    plt.ylabel('Number of Papers', fontsize=12)\n",
    "    \n",
    "    for bar, count in zip(bars, method_counts.values):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                str(count), ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_method_trends_over_time(df_analyzed, method_col='Method_Detected', year_col='year', top_n=8):\n",
    "    \"\"\"Plot method trends over time\"\"\"\n",
    "    # Filter out 'No_Method_Found'\n",
    "    df_methods = df_analyzed[df_analyzed[method_col] != 'No_Method_Found']\n",
    "    \n",
    "    if df_methods.empty:\n",
    "        print(\"No methods found for visualization\")\n",
    "        return\n",
    "    \n",
    "    method_year = df_methods.groupby([year_col, method_col]).size().reset_index(name='count')\n",
    "    top_methods = method_year.groupby(method_col)['count'].sum().nlargest(top_n).index\n",
    "    method_year = method_year[method_year[method_col].isin(top_methods)]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=method_year, x=year_col, y='count', hue=method_col, marker='o')\n",
    "    plt.title('Method Trends Over Time')\n",
    "    plt.ylabel('Number of Papers')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_method_development(df_analyzed):\n",
    "    \"\"\"Create visualizations showing the development of identified methods over years\"\"\"\n",
    "    # Filter out documents without methods\n",
    "    df_methods = df_analyzed[df_analyzed['Method_Detected'] != 'No_Method_Found'].copy()\n",
    "    \n",
    "    if df_methods.empty:\n",
    "        print(\"No methods found for visualization\")\n",
    "        return None\n",
    "    \n",
    "    # Ensure we have publication year data\n",
    "    if 'publication_year' not in df_methods.columns:\n",
    "        if 'year' in df_methods.columns:\n",
    "            df_methods['publication_year'] = df_methods['year']\n",
    "        else:\n",
    "            print(\"No publication year data available\")\n",
    "            return None\n",
    "    \n",
    "    # Clean and prepare data\n",
    "    df_methods = df_methods.dropna(subset=['publication_year', 'Method_Detected'])\n",
    "    df_methods['publication_year'] = pd.to_numeric(df_methods['publication_year'], errors='coerce')\n",
    "    df_methods = df_methods.dropna(subset=['publication_year'])\n",
    "    \n",
    "    print(f\"Visualizing {len(df_methods)} papers with methods across {df_methods['publication_year'].nunique()} years\")\n",
    "    \n",
    "    # Group by year and method\n",
    "    method_year_counts = df_methods.groupby(['publication_year', 'Method_Detected']).size().reset_index(name='count')\n",
    "    \n",
    "    # Get top methods for better visualization\n",
    "    top_methods = df_methods['Method_Detected'].value_counts().head(10).index.tolist()\n",
    "    method_year_filtered = method_year_counts[method_year_counts['Method_Detected'].isin(top_methods)]\n",
    "    \n",
    "    return method_year_counts, method_year_filtered, top_methods\n",
    "\n",
    "def create_method_timeline_plots(df_analyzed, output_dir=None):\n",
    "    \"\"\"Create multiple visualizations for method development over time\"\"\"\n",
    "    result = visualize_method_development(df_analyzed)\n",
    "    if result is None:\n",
    "        return\n",
    "    \n",
    "    method_year_counts, method_year_filtered, top_methods = result\n",
    "    \n",
    "    # 1. Interactive Line Chart\n",
    "    fig_line = px.line(\n",
    "        method_year_filtered, \n",
    "        x='publication_year', \n",
    "        y='count',\n",
    "        color='Method_Detected',\n",
    "        title='Development of Research Methods Over Time',\n",
    "        labels={\n",
    "            'publication_year': 'Publication Year',\n",
    "            'count': 'Number of Papers',\n",
    "            'Method_Detected': 'Research Method'\n",
    "        },\n",
    "        markers=True\n",
    "    )\n",
    "    \n",
    "    fig_line.update_layout(width=1200, height=600, hovermode='x unified')\n",
    "    \n",
    "    # 2. Method Emergence Timeline\n",
    "    method_first_appearance = method_year_counts.groupby('Method_Detected')['publication_year'].min().reset_index()\n",
    "    method_first_appearance = method_first_appearance.sort_values('publication_year')\n",
    "    \n",
    "    fig_emergence = px.scatter(\n",
    "        method_first_appearance,\n",
    "        x='publication_year',\n",
    "        y='Method_Detected',\n",
    "        size=[10] * len(method_first_appearance),\n",
    "        title='Method Emergence Timeline',\n",
    "        labels={'publication_year': 'First Appearance Year', 'Method_Detected': 'Research Method'}\n",
    "    )\n",
    "    \n",
    "    fig_emergence.update_layout(width=1200, height=800)\n",
    "    \n",
    "    # Display plots\n",
    "    print(\"=== METHOD DEVELOPMENT VISUALIZATIONS ===\")\n",
    "    fig_line.show()\n",
    "    fig_emergence.show()\n",
    "    \n",
    "    # Save plots if output directory specified\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        fig_line.write_html(os.path.join(output_dir, \"method_timeline.html\"))\n",
    "        fig_emergence.write_html(os.path.join(output_dir, \"method_emergence.html\"))\n",
    "        print(f\"✓ Visualizations saved to {output_dir}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n=== METHOD DEVELOPMENT SUMMARY ===\")\n",
    "    total_methods = len(method_year_counts['Method_Detected'].unique())\n",
    "    year_range = f\"{method_year_counts['publication_year'].min():.0f}-{method_year_counts['publication_year'].max():.0f}\"\n",
    "    \n",
    "    print(f\"Total unique methods identified: {total_methods}\")\n",
    "    print(f\"Year range: {year_range}\")\n",
    "    print(\"Most common methods:\")\n",
    "    \n",
    "    method_totals = method_year_counts.groupby('Method_Detected')['count'].sum().sort_values(ascending=False)\n",
    "    for method, count in method_totals.head(5).items():\n",
    "        print(f\"  {method}: {count} papers\")\n",
    "    \n",
    "    return fig_line, fig_emergence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2f418b",
   "metadata": {},
   "source": [
    "# ===== AUTHOR AND VENUE VISUALIZATION ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e3fc97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_frequencies(file_prefix, n_items=10, save=False):\n",
    "    \"\"\"Plot author and venue frequencies\"\"\"\n",
    "    try:\n",
    "        file_prefix = file_prefix.replace('.csv', '')\n",
    "        author_file = os.path.join(SAVE_DIR, f\"{file_prefix}_author_analysis.csv\")\n",
    "        venue_file = os.path.join(SAVE_DIR, f\"{file_prefix}_venue_frequencies.csv\")\n",
    "        \n",
    "        try:\n",
    "            authors_df = pd.read_csv(author_file, sep=';', encoding='utf-8')\n",
    "            venues_df = pd.read_csv(venue_file, sep=';', encoding='utf-8')\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"File not found: {e}\")\n",
    "            return\n",
    "        \n",
    "        top_authors = authors_df.nlargest(n_items, ['Frequency'])\n",
    "        top_venues = venues_df.nlargest(n_items, ['Frequency'])\n",
    "      \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "        \n",
    "        # Plot authors\n",
    "        sns.barplot(data=top_authors, x='Author', y='Frequency', ax=ax1)\n",
    "        ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right', fontsize=14)\n",
    "        ax1.set_title(f\"Top {n_items} Authors\")\n",
    "        ax1.set_xlabel('')\n",
    "        ax1.set_ylabel('Number of Publications')\n",
    "        \n",
    "        # Plot venues\n",
    "        sns.barplot(data=top_venues, x='Venue', y='Frequency', ax=ax2)\n",
    "        ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha='right', fontsize=14)\n",
    "        ax2.set_title(f\"Top {n_items} Venues\")\n",
    "        ax2.set_xlabel('')\n",
    "        ax2.set_ylabel('Number of Publications')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        if save:\n",
    "            output_file = os.path.join(SAVE_DIR, f\"{file_prefix}_frequency_plots.png\")\n",
    "            plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Plot saved as: {output_file}\")\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997f5517",
   "metadata": {},
   "source": [
    "# ===== DATA LOADING FUNCTIONS ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bedad57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_complete_analysis_data(date_string):\n",
    "    \"\"\"Load all saved analysis data including LDA model\"\"\"\n",
    "    print(f\"Loading complete analysis data for {date_string}...\")\n",
    "    \n",
    "    try:\n",
    "        # Load the analyzed dataframe\n",
    "        df_filename = os.path.join(SAVE_DIR, f\"semantic_scholar_{date_string}_results.csv\")\n",
    "        df_analyzed = pd.read_csv(df_filename, sep=';', encoding='utf-8')\n",
    "        \n",
    "        # Load topic names\n",
    "        topic_names_filename = os.path.join(SAVE_DIR, f\"semantic_scholar_{date_string}_topic_names.json\")\n",
    "        with open(topic_names_filename, 'r', encoding='utf-8') as f:\n",
    "            topic_names = json.load(f)\n",
    "        \n",
    "        # Load topic keywords\n",
    "        topic_keywords_filename = os.path.join(SAVE_DIR, f\"topic_keywords_{date_string}.json\")\n",
    "        with open(topic_keywords_filename, 'r', encoding='utf-8') as f:\n",
    "            topic_keywords_data = json.load(f)\n",
    "        \n",
    "        # Convert back to the expected format\n",
    "        topic_keywords = {}\n",
    "        for topic_idx, data in topic_keywords_data.items():\n",
    "            topic_keywords[int(topic_idx)] = data\n",
    "        \n",
    "        print(f\"Successfully loaded all components for {date_string}\")\n",
    "        \n",
    "        return {\n",
    "            'df_analyzed': df_analyzed,\n",
    "            'topic_names': topic_names,\n",
    "            'topic_keywords': topic_keywords\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8180d0",
   "metadata": {},
   "source": [
    "# ===== MAIN VISUALIZATION FUNCTION ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd05ad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MAIN VISUALIZATION FUNCTION =====\n",
    "\n",
    "def create_complete_visualization_suite(df_analyzed, topic_names=None, topic_keywords=None, \n",
    "                                      file_prefix=None, output_dir=None):\n",
    "    \"\"\"Create complete visualization suite for your analysis results\"\"\"\n",
    "    \n",
    "    print(\"=== CREATING COMPLETE VISUALIZATION SUITE ===\\n\")\n",
    "    \n",
    "    # 1. Topic Visualizations (if topic data available)\n",
    "    if topic_names and topic_keywords:\n",
    "        print(\"1. Topic Analysis Visualizations:\")\n",
    "        plot_topic_keywords_with_names(topic_keywords, topic_names)\n",
    "        plot_topic_importance_with_names(df_analyzed, topic_names)\n",
    "        \n",
    "        if 'year' in df_analyzed.columns:\n",
    "            plot_topic_trends_over_time(df_analyzed, year_col='year')\n",
    "    \n",
    "    # 2. Method Visualizations\n",
    "    if 'Method_Detected' in df_analyzed.columns:\n",
    "        print(\"2. Method Analysis Visualizations:\")\n",
    "        plot_method_importance(df_analyzed)\n",
    "        \n",
    "        if 'year' in df_analyzed.columns:\n",
    "            plot_method_trends_over_time(df_analyzed, year_col='year')\n",
    "            \n",
    "        # Method timeline plots\n",
    "        create_method_timeline_plots(df_analyzed, output_dir)\n",
    "    \n",
    "    # 3. Author and Venue Analysis (if file prefix provided)\n",
    "    if file_prefix:\n",
    "        print(\"3. Author and Venue Analysis:\")\n",
    "        plot_frequencies(file_prefix, n_items=15, save=True)\n",
    "    \n",
    "    print(\"\\n✓ Complete visualization suite created successfully!\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cc958f",
   "metadata": {},
   "source": [
    "# ===== DO ANALYSIS ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3129264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading complete analysis data for 2025_06_25reliability_resilience_power_systems...\n",
      "Error loading data: [Errno 2] No such file or directory: 'Saved_files_new\\\\semantic_scholar_2025_06_25reliability_resilience_power_systems_topic_names.json'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"Main function to run visualizations\"\"\"\n",
    "    \n",
    "    # Load your analysis results\n",
    "date_string = \"2025_06_25reliability_resilience_power_systems\"  # Update this\n",
    "    \n",
    "    # Option 1: Load from saved files\n",
    "analysis_data = load_complete_analysis_data(date_string)\n",
    "if analysis_data:\n",
    "    create_complete_visualization_suite(\n",
    "        df_analyzed=analysis_data['df_analyzed'],\n",
    "        topic_names=analysis_data['topic_names'],\n",
    "        topic_keywords=analysis_data['topic_keywords'],\n",
    "        file_prefix=f\"semantic_scholar_{date_string}\",\n",
    "        output_dir=os.path.join(SAVE_DIR, f\"visualizations_{date_string}\")\n",
    "    )\n",
    "    \n",
    "    # Option 2: Use existing dataframe (if already loaded)\n",
    "    # create_complete_visualization_suite(\n",
    "    #     df_analyzed=your_df_analyzed,\n",
    "    #     topic_names=your_topic_names,\n",
    "    #     topic_keywords=your_topic_keywords,\n",
    "    #     file_prefix=\"your_file_prefix\",\n",
    "    #     output_dir=\"your_output_dir\"\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e06cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "literature-search-and-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
