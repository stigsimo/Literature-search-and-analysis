{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'triu' from 'scipy.linalg' (c:\\git_repos\\Literature-search-and-analysis\\.venv\\Lib\\site-packages\\scipy\\linalg\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CoherenceModel\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpora\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcorpora\u001b[39;00m\n\u001b[0;32m     29\u001b[0m SAVE_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved_files\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\git_repos\\Literature-search-and-analysis\\.venv\\Lib\\site-packages\\gensim\\__init__.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[1;32mc:\\git_repos\\Literature-search-and-analysis\\.venv\\Lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\git_repos\\Literature-search-and-analysis\\.venv\\Lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[0;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[1;32mc:\\git_repos\\Literature-search-and-analysis\\.venv\\Lib\\site-packages\\gensim\\interfaces.py:19\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[0;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[1;32mc:\\git_repos\\Literature-search-and-analysis\\.venv\\Lib\\site-packages\\gensim\\matutils.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m entropy\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_blas_funcs, triu\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlapack\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m psi  \u001b[38;5;66;03m# gamma function utils\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'triu' from 'scipy.linalg' (c:\\git_repos\\Literature-search-and-analysis\\.venv\\Lib\\site-packages\\scipy\\linalg\\__init__.py)"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import configparser \n",
    "import tiktoken\n",
    "from difflib import SequenceMatcher\n",
    "from datetime import datetime\n",
    "import json\n",
    "import ast\n",
    "import csv\n",
    "import os\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from gensim.models import CoherenceModel\n",
    "#import gensim.corpora as corpora\n",
    "\n",
    "SAVE_DIR = \"Saved_files\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\STSI/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\STSI/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\STSI/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK data and handle stop words\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def get_custom_stop_words(search_keywords=None):\n",
    "    \n",
    "    # Get standard stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Words to keep (search keywords)\n",
    "    words_to_keep = set()\n",
    "    if search_keywords:\n",
    "        # Convert keywords to lowercase and split multi-word terms\n",
    "        for keyword in search_keywords:\n",
    "            keyword = keyword.lower()\n",
    "            # Add both the full phrase and individual words\n",
    "            words_to_keep.add(keyword)\n",
    "            for word in keyword.split():\n",
    "                words_to_keep.add(word)\n",
    "    \n",
    "    # Remove search keywords from stopwords\n",
    "    stop_words = stop_words - words_to_keep\n",
    "    \n",
    "    # Scientific paper terms to exclude (add to stopwords)\n",
    "    scientific_terms = {\n",
    "        # Citation terms\n",
    "        'et', 'al', 'ref', 'reference', 'references', 'cited', 'cite',\n",
    "        # Figure and table references\n",
    "        'fig', 'figure', 'figures', 'table', 'tables', 'chart', 'charts',\n",
    "        # Publication terms\n",
    "        'published', 'journal', 'conference', 'proceedings',\n",
    "        # Measurement units and numbers\n",
    "        'vol', 'volume', 'pp', 'page', 'pages', 'doi'\n",
    "    }\n",
    "    \n",
    "    # Add scientific terms to stopwords\n",
    "    stop_words = stop_words.union(scientific_terms)\n",
    "    \n",
    "    return stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some code for AI assistance\n",
    "# Initialize the LLM API\n",
    "def initialize_openai():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('config_LLM.txt')\n",
    "    api_key = config['LLM'].get('OPENAI_API_KEY')\n",
    "    model_type = config['LLM'].get('MODEL_TYPE')\n",
    "    return OpenAI(api_key=api_key), model_type\n",
    "\n",
    "# Credit tracker to keep track of the cost when using paid API\n",
    "class CreditTracker:\n",
    "    def __init__(self):\n",
    "        self.total_tokens = 0\n",
    "        self.total_cost = 0\n",
    "        self.cost_per_1k_tokens = 0.00015\n",
    "\n",
    "    def update(self, tokens):\n",
    "        self.total_tokens += tokens\n",
    "        self.total_cost += (tokens / 1000) * self.cost_per_1k_tokens\n",
    "\n",
    "    def get_stats(self):\n",
    "        return {\n",
    "            \"total_tokens\": self.total_tokens,\n",
    "            \"total_cost\": round(self.total_cost, 4)\n",
    "        }\n",
    "\n",
    "credit_tracker = CreditTracker()\n",
    "\n",
    "def num_tokens_from_string(string: str, model_name: str) -> int:\n",
    "    encoding = tiktoken.encoding_for_model(model_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess function\n",
    "def preprocess_text(text, search_keywords):\n",
    "    if not isinstance(text, (str, int, float)):\n",
    "        return ''\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s-]', '', text)\n",
    "    tokens = word_tokenize(text) # Split text into individual words/tokens using NLTK's tokenizer\n",
    "    stop_words = get_custom_stop_words(search_keywords)\n",
    "    tokens = [token for token in tokens if token not in stop_words] # Remove all stop words from our tokens using list comprehension\n",
    "    lemmatizer = WordNetLemmatizer() # Initialize WordNet lemmatizer to reduce words to their base/dictionary form\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens] # Convert each token to its lemma (e.g., \"systems\" → \"system\", \"running\" → \"run\")\n",
    "    return ' '.join(tokens) # Combine all processed tokens back into a single string with spaces between words\n",
    "\n",
    "\n",
    "# Function to convert string representation of list to actual list, replacing long strings with \"Unknown\"\n",
    "def string_to_list(s):\n",
    "    if isinstance(s, str):\n",
    "        # Remove brackets and split by comma\n",
    "        fields = [field.strip().strip(\"'\") for field in s.strip('[]').split(',')]\n",
    "        # Replace fields wit more than 100 characters with \"Unknown\"\n",
    "        return [\"Unknown\" if len(field) > 100 else field for field in fields]\n",
    "    return [\"Unknown\"]  # Return [\"Unknown\"] for empty or non-string entries\n",
    "\n",
    "# Clean fields of study\n",
    "def clean_fields_of_study(s):\n",
    "    valid_fields= ['Computer Science', 'Economics', 'Engineering', 'Physics', 'Mathematics', 'Medicine', 'Business', 'Environmental Science', 'Chemistry', 'Materials Science', 'Geography', 'Biology', 'Geology', 'Political Science', 'Psychology', 'Com']\n",
    "    if pd.isna(s) or s == '[]':\n",
    "        return [\"Unknown\"]\n",
    "    if isinstance(s, str):\n",
    "        fields = [field.strip().strip(\"'\\\"\") for field in s.strip('[]').split(',')]\n",
    "        cleaned_fields = []\n",
    "        for field in fields:\n",
    "            if field in valid_fields:\n",
    "                cleaned_fields.append(field)\n",
    "            else:\n",
    "                cleaned_fields.append(\"Unknown\")\n",
    "        return cleaned_fields if cleaned_fields else [\"Unknown\"]\n",
    "    return [\"Unknown\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple analysis to extract keyword and n-gram frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_frequencies(vectorizer, texts):\n",
    "    matrix = vectorizer.fit_transform(texts)\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    freqs = matrix.sum(axis=0).A1\n",
    "    return dict(sorted(zip(terms, freqs.tolist()), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "def extract_keywords_and_ngrams(df, max_features=1000):\n",
    "    # Create vectorizers\n",
    "    keyword_vectorizer = CountVectorizer(\n",
    "        max_df=0.95, \n",
    "        min_df=2,\n",
    "        #stop_words='english',# can be removed as stopwords are already removed from processed_text\n",
    "        max_features=max_features,\n",
    "        token_pattern=r'(?u)\\b[A-Za-z][A-Za-z-]+[A-Za-z]\\b'\n",
    "    )\n",
    "    \n",
    "    bigram_vectorizer = CountVectorizer(\n",
    "        ngram_range=(2,2),\n",
    "        max_df=0.95,\n",
    "        min_df=2,\n",
    "        #stop_words='english',# can be removed as stopwords are already removed from processed_text\n",
    "        max_features=max_features\n",
    "    )\n",
    "    \n",
    "    trigram_vectorizer = CountVectorizer(\n",
    "        ngram_range=(3,3),\n",
    "        max_df=0.95,\n",
    "        min_df=2,\n",
    "        max_features=max_features,\n",
    "        #stop_words='english' # can be removed as stopwords are already removed from processed_text\n",
    "    )\n",
    "    \n",
    "    # Extract frequencies\n",
    "    keyword_freq = get_term_frequencies(keyword_vectorizer, df['processed_text'])\n",
    "    bigram_freq = get_term_frequencies(bigram_vectorizer, df['processed_text'])\n",
    "    trigram_freq = get_term_frequencies(trigram_vectorizer, df['processed_text'])\n",
    "    \n",
    "    # Save results\n",
    "    current_date = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "    filename=os.path.join(SAVE_DIR,f'term_frequencies_{current_date}.json')\n",
    "    results = {\n",
    "        'keywords': keyword_freq,\n",
    "        'bigrams': bigram_freq,\n",
    "        'trigrams': trigram_freq\n",
    "    }\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Results saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_topic_keywords(lda_model, vectorizer, num_words=10):\n",
    "    \"\"\"Extract keywords and n-grams for each topic\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topic_keywords = {}\n",
    "    \n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        # Get top words\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-num_words-1:-1]]\n",
    "        \n",
    "        # Get word weights\n",
    "        word_weights = [(feature_names[i], topic[i]) \n",
    "                       for i in topic.argsort()[:-num_words-1:-1]]\n",
    "        \n",
    "        topic_keywords[topic_idx] = {\n",
    "            'top_words': top_words,\n",
    "            'word_weights': word_weights\n",
    "        }\n",
    "    \n",
    "    return topic_keywords\n",
    "\n",
    "# Model topics function\n",
    "def model_topics_by_field(df, field, num_topics=10, num_words=5):\n",
    "    df_field = df[df['fieldsOfStudy'].apply(lambda x: field in x)] # filtering so that only the documents within the field is keept.\n",
    "    print (f\"Analyzing {len(df_field)} papers\")\n",
    "    if df_field.empty:\n",
    "            print(f\"No papers found for field: {field}\")            #warning if no papers found\n",
    "            return None, None, None, None, None\n",
    "    text_data = df_field['abstract'].fillna('')                     # filtering to avoid errors due to missing fields\n",
    "    \n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english') # vectorizing with max_df = 0.95 as default (terms that appear in more than 95% of documents) and min_df =2 (terms that appear in less than 2 of the documents). Values below 1 indicates percentile, and values above 1 indicates number of docs.\n",
    "    doc_term_matrix = vectorizer.fit_transform(text_data) #creating the term matrix:  vectorizer : sklearn.feature_extraction.text.(CountVectorizer, TfIdfVectorizer).vectorizer used to convert raw documents to document-term matrix (`dtm`)\n",
    "    \n",
    "    #Using the sklearn decomposition LatentDirchletAllocation (see https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html). \n",
    "    #Fit the LDA model\n",
    "    lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42) \n",
    "    #Randomstate 42 is passed to maintain reproducibility in results (0 and 42 are commonly used values. Default \"None\" will result in using globa radnom instande (numpy.radom) and re-running may produce different results\n",
    "    \n",
    "    topic_distribution = lda_model.fit_transform(doc_term_matrix)\n",
    "    topic_keywords=extract_topic_keywords(lda_model, vectorizer)\n",
    "    \n",
    "    #feature_names = vectorizer.get_feature_names_out()\n",
    "   \n",
    "    for topic_idx, keywords in topic_keywords.items():\n",
    "        print(f\"\\nTopic {topic_idx + 1}:\")\n",
    "        # Format each word with its weight in parentheses\n",
    "        formatted_words = [f\"{word} ({weight:.2f})\" \n",
    "                         for word, weight in keywords['word_weights']]\n",
    "        print(\", \".join(formatted_words)) \n",
    "    return lda_model, vectorizer, topic_distribution, df_field, topic_keywords\n",
    "\n",
    "def tune_topic_number(df, min_topics=2, max_topics=20, step=1, use_coherence=True):\n",
    "    text_data = df['abstract'].fillna('')\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    doc_term_matrix = vectorizer.fit_transform(text_data)\n",
    "    \n",
    "    # Split data for perplexity calculation\n",
    "    train_data, test_data = train_test_split(doc_term_matrix, test_size=0.3, random_state=42)\n",
    "    \n",
    "    best_score = float('inf')\n",
    "    best_num_topics = min_topics\n",
    "    scores = []\n",
    "    \n",
    "    for num_topics in range(min_topics, max_topics + 1, step):\n",
    "        lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "        lda_model.fit(train_data)\n",
    "        \n",
    "        perplexity = lda_model.perplexity(test_data)\n",
    "        \n",
    "        if use_coherence:\n",
    "            # Prepare data for coherence calculation\n",
    "            id2word = {i: word for i, word in enumerate(vectorizer.get_feature_names_out())}\n",
    "            corpus = [id2word.doc2bow(doc.split()) for doc in text_data]\n",
    "            \n",
    "            coherence_model = CoherenceModel(model=lda_model, texts=text_data, dictionary=id2word, coherence='c_v')\n",
    "            coherence = coherence_model.get_coherence()\n",
    "            \n",
    "            # Combine perplexity and coherence\n",
    "            score = perplexity / coherence\n",
    "        else:\n",
    "            score = perplexity\n",
    "        \n",
    "        scores.append((num_topics, score))\n",
    "        \n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_num_topics = num_topics\n",
    "    \n",
    "    return best_num_topics, scores \n",
    "\n",
    "def model_topics(df, num_topics=10, num_words=5):\n",
    "    \n",
    "    print(f\"Analyzing {len(df)} papers across all fields\")\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"No papers found for analysis\")\n",
    "        return None, None, None, None, None\n",
    "        \n",
    "    text_data = df['abstract'].fillna('')\n",
    "    \n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    doc_term_matrix = vectorizer.fit_transform(text_data)\n",
    "    \n",
    "    lda_model = LatentDirichletAllocation(n_components=num_topics, learning_method='online',random_state=42)\n",
    "    topic_distribution = lda_model.fit_transform(doc_term_matrix)\n",
    "    topic_keywords = extract_topic_keywords(lda_model, vectorizer)\n",
    "    \n",
    "    for topic_idx, keywords in topic_keywords.items():\n",
    "        print(f\"\\nTopic {topic_idx + 1}:\")\n",
    "        formatted_words = [f\"{word} ({weight:.2f})\" \n",
    "                         for word, weight in keywords['word_weights']]\n",
    "        print(\", \".join(formatted_words))\n",
    "        \n",
    "    return lda_model, vectorizer, topic_distribution, df, topic_keywords\n",
    "\n",
    "\n",
    "# Generate topic name function\n",
    "def generate_topic_name(text, keywords, client, model_type, credit_tracker):\n",
    "    try:\n",
    "        tokens = num_tokens_from_string(text + \" \" + \" \".join(keywords), model_type)\n",
    "        credit_tracker.update(tokens)\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_type,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful scientific assistant that generates concise topic expressions based on academic paper titles and keywords for a collection of papers.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"\"\"Based on the following titles and keywords obtained by LDA-analysis, provide a concise single word, bigram, or trigram that best describes the main topic these abstracts have in common. The expression should be specific and descriptive and feasible for categorization.\n",
    "\n",
    "Title:\n",
    "{text}\n",
    "\n",
    "Keywords:\n",
    "{', '.join(keywords)}\n",
    "\n",
    "Concise topic expression:\"\"\"}\n",
    "            ]\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        credit_tracker.update(num_tokens_from_string(content, model_type))\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def string_similarity(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def generate_topic_name_multiple(text, keywords, client, model_type, credit_tracker, initial_iterations=3, max_iterations=10, similarity_threshold=0.7):\n",
    "    iterations = initial_iterations\n",
    "    while iterations <= max_iterations:\n",
    "        generated_names = []\n",
    "        for _ in range(iterations):\n",
    "            # Reuse the generate_topic_name function instead of duplicating code\n",
    "            name = generate_topic_name(text, keywords, client, model_type, credit_tracker)\n",
    "            if name:\n",
    "                generated_names.append(name)\n",
    "        \n",
    "        # Check for dominant topic\n",
    "        for i, name in enumerate(generated_names):\n",
    "            similar_names = [other_name for j, other_name in enumerate(generated_names) \n",
    "                             if i != j and string_similarity(name, other_name) >= similarity_threshold]\n",
    "            if len(similar_names) >= len(generated_names) // 2:\n",
    "                return name  # Return the dominant topic name\n",
    "\n",
    "        # If no dominant topic found, increase iterations\n",
    "        iterations += 2\n",
    "        print(f\"No clear common topic name found. Increasing iterations to {iterations}.\")\n",
    "\n",
    "    # If max iterations reached without finding a dominant topic, return the most common name\n",
    "    from collections import Counter\n",
    "    return Counter(generated_names).most_common(1)[0][0]\n",
    "\n",
    "\n",
    "\n",
    "def classify_papers(topic_distributions, df_field):\n",
    "    \"\"\"Classify papers based on topic distributions\"\"\"\n",
    "    paper_classifications = []\n",
    "    \n",
    "    for idx, dist in enumerate(topic_distributions): #looping throug the papers one-by-one (index by index) and topics distribution \n",
    "        # Get primary and secondary topics (only primary is used in dominance ratio)\n",
    "        top_2_topics = np.argsort(dist)[-2:][::-1] # extracting the to last [-2:] the order is reversed to highest first with the [::-1]\n",
    "        \n",
    "        # Calculate dominance ratios by :\n",
    "        primary_score = dist[top_2_topics[0]] # 1) extracting the probability of the primary topic\n",
    "        other_topics_sum = sum(dist) - primary_score # 2) calc. the sum of all the other topics (all topic-primary)\n",
    "        dominance_ratio = primary_score / (other_topics_sum + 1e-10) #3) dividing the primary score by the sum (adding very small number to avoid dividing by zero if only one topic)\n",
    "        \n",
    "        # Storing the paper classifications\n",
    "        paper_classifications.append({\n",
    "            'paper_idx': idx,                   #storing the paper index\n",
    "            'primary_topic': top_2_topics[0],   # the most probable topic\n",
    "            'secondary_topic': top_2_topics[1], # second most probable topic\n",
    "            'primary_score': primary_score,     # the probability of the most probable topic \n",
    "            'dominance_ratio': dominance_ratio  # the domincane ratio (eg. is the paper only about this, or also covering other topics?)\n",
    "        })\n",
    "    \n",
    "    return paper_classifications\n",
    "\n",
    "# Get top papers per topic per field\n",
    "\n",
    "def get_top_papers(paper_classifications, df_field, n_top=5):\n",
    "    top_papers = {}\n",
    "    author_topic_stats = {}\n",
    "    \n",
    "    # Debug print\n",
    "    #print(f\"Total papers to analyze: {len(paper_classifications)}\")\n",
    "    \n",
    "    for topic in set(p['primary_topic'] for p in paper_classifications): # creating a set of the unique primary topics in the datasett, e.g. if there are only three primary topics and all papers \"belong\" to one of them, the set will be {Primary topic 1, Primary topic 2, Primary topic 3}\n",
    "        print(f\"\\nProcessing topic {topic}\")\n",
    "        topic_papers = [p for p in paper_classifications if p['primary_topic'] == topic]  # Get all the papers for the current topic\n",
    "        print(f\"Papers for topic {topic}: {len(topic_papers)}\")\n",
    "        topic_papers.sort(key=lambda x: x['dominance_ratio'], reverse=True) # Sort by dominance ratio, reverse=True to get the highest ratios at the top.\n",
    "        top_papers[topic] = []\n",
    "        \n",
    "        # Get top n papers where n is the numebr spesified as top_n when calling the functino\n",
    "        for p in topic_papers[:n_top]: #loop through the top_n number of papers\n",
    "            paper_idx = p['paper_idx']\n",
    "            authors = df_field.iloc[paper_idx]['authors']\n",
    "            \n",
    "            # Debug print\n",
    "            #print(f\"\\nPaper index: {paper_idx}\")\n",
    "            #print(f\"Authors data type: {type(authors)}\")\n",
    "            #print(f\"Authors content: {authors}\")\n",
    "            \n",
    "            # Check if authors is string (might be stored as JSON string)\n",
    "            if isinstance(authors, str):\n",
    "                try:\n",
    "                    authors = ast.literal_eval(authors)\n",
    "                except (ValueError, SyntaxError):\n",
    "                    print(f\"Failed to parse authors string: {authors}\")\n",
    "                    authors = []\n",
    "            \n",
    "            if isinstance(authors, list):\n",
    "                author_list = []\n",
    "                for author in authors:\n",
    "                    # Debug print\n",
    "                    print(f\"Processing author: {author}\")\n",
    "                    if isinstance(author, dict):\n",
    "                        author_list.append({\n",
    "                            'name': author.get('name', 'Unknown'),\n",
    "                            'id': author.get('authorId', 'Unknown')\n",
    "                        })\n",
    "                    else:\n",
    "                        print(f\"Unexpected author format: {author}\")\n",
    "            else:\n",
    "                print(f\"Unexpected authors format: {authors}\")\n",
    "                author_list = []\n",
    "            \n",
    "            # Debug print\n",
    "            #print(f\"Processed author list: {author_list}\")\n",
    "            \n",
    "            top_papers[topic].append({\n",
    "                'paperId': df_field.iloc[paper_idx]['paperId'],\n",
    "                'title': df_field.iloc[paper_idx]['title'],\n",
    "                'abstract': df_field.iloc[paper_idx]['abstract'],\n",
    "                'authors': author_list,\n",
    "                'score': float(p['primary_score']),\n",
    "                'dominance_ratio': float(p['dominance_ratio'])\n",
    "            })\n",
    "            \n",
    "            for author in author_list:\n",
    "                author_id = author['id']\n",
    "                if author_id not in author_topic_stats:\n",
    "                    author_topic_stats[author_id] = {\n",
    "                        'name': author['name'],\n",
    "                        'topics': {},\n",
    "                        'total_papers': 0,\n",
    "                        'top_papers': 0\n",
    "                    }\n",
    "                \n",
    "                if topic not in author_topic_stats[author_id]['topics']:\n",
    "                    author_topic_stats[author_id]['topics'][topic] = {\n",
    "                        'paper_count': 0,\n",
    "                        'avg_dominance': 0,\n",
    "                        'top_papers': []\n",
    "                    }\n",
    "                \n",
    "                author_stats = author_topic_stats[author_id]['topics'][topic]\n",
    "                author_stats['paper_count'] += 1\n",
    "                author_stats['avg_dominance'] = (\n",
    "                    (author_stats['avg_dominance'] * (author_stats['paper_count'] - 1) + \n",
    "                     float(p['dominance_ratio'])) / author_stats['paper_count']\n",
    "                )\n",
    "                author_stats['top_papers'].append({\n",
    "                    'title': df_field.iloc[paper_idx]['title'],\n",
    "                    'dominance_ratio': float(p['dominance_ratio'])\n",
    "                })\n",
    "                \n",
    "                author_topic_stats[author_id]['total_papers'] += 1\n",
    "                author_topic_stats[author_id]['top_papers'] += 1\n",
    "    \n",
    "      # Convert topic numbers to regular integers for JSON serialization\n",
    "    author_topic_stats_clean = {}\n",
    "    for author_id, stats in author_topic_stats.items():\n",
    "        author_topic_stats_clean[author_id] = stats.copy()\n",
    "        author_topic_stats_clean[author_id]['topics'] = {\n",
    "            int(topic): topic_stats \n",
    "            for topic, topic_stats in stats['topics'].items()\n",
    "        }\n",
    "    \n",
    "    # Debug print with cleaned data\n",
    "    #print(f\"\\nFinal author_topic_stats: {json.dumps(author_topic_stats_clean, indent=2)}\")\n",
    "    \n",
    "    return top_papers, author_topic_stats\n",
    "\n",
    "\n",
    "def print_author_analysis(author_topic_stats, min_papers=2):\n",
    "    \"\"\"Print detailed author analysis\"\"\"\n",
    "    print(\"\\nAuthor Analysis:\")\n",
    "    for author_id, stats in author_topic_stats.items():\n",
    "        if stats['total_papers'] >= min_papers:\n",
    "            print(f\"\\nAuthor: {stats['name']}\")\n",
    "            print(f\"Total papers in top lists: {stats['total_papers']}\")\n",
    "            print(\"Topics:\")\n",
    "            for topic, topic_stats in stats['topics'].items():\n",
    "                print(f\"\\nTopic {topic}:\")\n",
    "                print(f\"  Paper count: {topic_stats['paper_count']}\")\n",
    "                print(f\"  Average dominance ratio: {topic_stats['avg_dominance']:.4f}\")\n",
    "                print(\"  Top papers:\")\n",
    "                for paper in topic_stats['top_papers']:\n",
    "                    print(f\"    - {paper['title']} (dominance: {paper['dominance_ratio']:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" needs fixing\"\"\"\n",
    "\n",
    "def analyze_specific_topic(df, topic, num_subtopics=5, n_top=5):\n",
    "    # Filter the dataframe for the specific topic\n",
    "    df_topic = df[df['Primary_Topic'] == topic].copy()\n",
    "    \n",
    "    print(f\"Analyzing topic: {topic}\")\n",
    "    print(f\"Number of papers: {len(df_topic)}\")\n",
    "    \n",
    "    if len(df_topic) < 10:  # Adjust this threshold as needed\n",
    "        print(\"Not enough papers for meaningful subtopic analysis.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Ensure index is unique\n",
    "    df_topic = df_topic.reset_index(drop=True)\n",
    "    \n",
    "    # Prepare the text data\n",
    "    text_data = df_topic['abstract'].fillna('')\n",
    "    \n",
    "    # Create document-term matrix\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    doc_term_matrix = vectorizer.fit_transform(text_data)\n",
    "    \n",
    "    # Create and fit LDA model for subtopics\n",
    "    lda_model = LatentDirichletAllocation(n_components=num_subtopics, random_state=42)\n",
    "    subtopic_distributions = lda_model.fit_transform(doc_term_matrix)\n",
    "    \n",
    "    # Get top words for each subtopic\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    for idx, subtopic in enumerate(lda_model.components_):\n",
    "        top_words = [feature_names[i] for i in subtopic.argsort()[:-10 - 1:-1]]\n",
    "        print(f\"Subtopic {idx + 1}: {', '.join(top_words)}\")\n",
    "    \n",
    "    # Get top papers for each subtopic\n",
    "    top_papers = {}\n",
    "    for subtopic in range(num_subtopics):\n",
    "        subtopic_scores = subtopic_distributions[:, subtopic]\n",
    "        other_subtopics_sum = subtopic_distributions.sum(axis=1) - subtopic_scores\n",
    "        subtopic_dominance = subtopic_scores / (other_subtopics_sum + 1e-10)\n",
    "        top_indices = np.argsort(subtopic_dominance)[-n_top:][::-1]\n",
    "        \n",
    "        top_papers[subtopic] = [\n",
    "            {\n",
    "                'title': df_topic.iloc[i]['title'],\n",
    "                'abstract': df_topic.iloc[i]['abstract'],\n",
    "                'score': float(subtopic_scores[i]),\n",
    "                'dominance_ratio': float(subtopic_dominance[i])\n",
    "            }\n",
    "            for i in top_indices\n",
    "        ]\n",
    "    # Generate subtopic names using the same method as before - Skipped until copyright breach issue is resolved\n",
    "    \"\"\"\n",
    "    subtopic_names = {}\n",
    "    for subtopic, papers in top_papers.items():\n",
    "        abstracts = \"\\n\\n\".join([paper['abstract'] for paper in papers])\n",
    "        keywords = [word for paper in papers for word in paper['abstract'].split()[:10]]\n",
    "        subtopic_name = generate_topic_name(abstracts, keywords)\n",
    "        subtopic_names[subtopic] = subtopic_name\n",
    "    \"\"\"\n",
    "    #Add subtopic scores and names to the dataframe\n",
    "    for i in range(num_subtopics):\n",
    "        df_topic[f'Subtopic_{i+1}_Score'] = subtopic_distributions[:, i]\n",
    "        df_topic['Primary_Subtopic'] = df_topic[[f'Subtopic_{i+1}_Score' for i in range(num_subtopics)]].idxmax(axis=1)\n",
    "   # df_topic['Primary_Subtopic_Name'] = df_topic['Primary_Subtopic'].map(lambda x: subtopic_names[int(x.split('_')[1]) - 1])\n",
    "    \n",
    "    # Update the main dataframe with the new subtopic information\n",
    "        df_update = df.copy()\n",
    "        df_update.loc[df_topic.index, [f'Subtopic_{i+1}_Score' for i in range(num_subtopics)]] = df_topic[[f'Subtopic_{i+1}_Score' for i in range(num_subtopics)]]\n",
    "        df_update.loc[df_topic.index, 'Primary_Subtopic'] = df_topic['Primary_Subtopic']\n",
    "  # df_update.loc[df_topic.index, 'Primary_Subtopic_Name'] = df_topic['Primary_Subtopic_Name']\n",
    "    \n",
    "    return df_update, top_papers#, subtopic_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename='semantic_scholar_2025_02_14_reliability_resilience_power_systems_results.csv' # if the search key words are changed you need to update the filename here. should be automatic...\n",
    "#filename='semantic_scholar_2025_02_27_graph_neural_network_power_system__load_flow_results.csv'\n",
    "filepath=os.path.join(SAVE_DIR,filename)\n",
    "df=pd.read_csv(filepath,sep=\";\")\n",
    "df['text'] = df['title'].fillna('') + ' ' + df['abstract'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing text data...\n",
      "Text preprocessing completed.\n"
     ]
    }
   ],
   "source": [
    "# Processing to convert title and abstract to text field.\n",
    "print(\"Preprocessing text data...\")\n",
    "search_keywords=['reliability', 'resilience', 'power system', 'capacity utilization'] # should be automitically retrieved from search script, but for the time beeing its manually input here...\n",
    "df['processed_text'] = df['text'].apply(lambda x:preprocess_text(x,search_keywords=search_keywords))\n",
    "print(\"Text preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preparing for analysis (need to fill the empty fieldsOfStudy with ''):\n",
    "valid_fields= ['Computer Science', 'Economics', 'Engineering', 'Physics', 'Mathematics', 'Medicine', 'Business', 'Environmental Science', 'Chemistry', 'Materials Science', 'Geography', 'Biology', 'Geology', 'Political Science', 'Psychology', 'Com']\n",
    "df['fieldsOfStudy'] = df['fieldsOfStudy'].apply(clean_fields_of_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to Saved_files\\term_frequencies_2025_03_03.json\n"
     ]
    }
   ],
   "source": [
    "# simple analysis before topic modelling\n",
    "# Usage\n",
    "extract_keywords_and_ngrams(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new better version of the analyzis:\n",
    "\"\"\"\n",
    "def analyze_papers_with_topic_names(df, fields_to_analyze,n_papers=5, output_suffix=\"_analyzed_results\"):\n",
    "    # Initialize OpenAI client\n",
    "    client, model_type = initialize_openai()\n",
    "    credit_tracker = CreditTracker()\n",
    "    \n",
    "    # Create a copy of the original dataframe to store all results\n",
    "    df_analyzed = df.copy()\n",
    "    all_author_stats = {}\n",
    "    all_topic_names = {}\n",
    "    \n",
    "    for field in fields_to_analyze:\n",
    "        print(f\"\\nAnalyzing field: {field}\")\n",
    "        \n",
    "        # Step 1: Model topics\n",
    "        lda_model, vectorizer, topic_distributions, df_field, topic_keywords = model_topics(\n",
    "            df, field, num_topics=10)\n",
    "        \n",
    "        if lda_model is None:\n",
    "            continue\n",
    "            \n",
    "        # Step 2: Classify papers and add to main dataframe\n",
    "        paper_classifications = classify_papers(topic_distributions, df_field)\n",
    "        \n",
    "        # Add classifications to the analyzed dataframe\n",
    "        for p in paper_classifications:\n",
    "            idx = df_field.index[p['paper_idx']]\n",
    "            df_analyzed.loc[idx, f'{field}_Primary_Topic'] = p['primary_topic']\n",
    "            df_analyzed.loc[idx, f'{field}_Secondary_Topic'] = p['secondary_topic']\n",
    "            df_analyzed.loc[idx, f'{field}_Primary_Score'] = p['primary_score']\n",
    "            df_analyzed.loc[idx, f'{field}_Dominance_Ratio'] = p['dominance_ratio']\n",
    "        \n",
    "        # Step 3: Get top papers and author stats\n",
    "        top_papers, author_stats = get_top_papers(paper_classifications, df_field, n_top=n_papers)\n",
    "        \n",
    "        # Step 4: Generate topic names\n",
    "        field_topic_names = {}\n",
    "        for topic_idx, papers in top_papers.items():\n",
    "            # Combine abstracts and extract keywords\n",
    "            input_text = \"\\n\\n\".join([str(paper.get('title', '')) for paper in papers])\n",
    "            keywords = topic_keywords[topic_idx]['top_words']\n",
    "            \n",
    "            # Generate topic name\n",
    "            topic_name = generate_topic_name_multiple(input_text, keywords, client, model_type, credit_tracker)\n",
    "            \n",
    "            if topic_name:\n",
    "                field_topic_names[topic_idx] = topic_name\n",
    "                # Add topic name to dataframe\n",
    "                topic_column = f'{field}_Topic_{topic_idx}_Name'\n",
    "                df_analyzed[topic_column] = topic_name\n",
    "                \n",
    "                # Update primary topic name for papers with this primary topic\n",
    "                primary_topic_mask = df_analyzed[f'{field}_Primary_Topic'] == topic_idx\n",
    "                df_analyzed.loc[primary_topic_mask, f'{field}_Primary_Topic_Name'] = topic_name\n",
    "            \n",
    "            # Print topic name and top papers\n",
    "            print(f\"\\nTopic {topic_idx + 1}: {topic_name if topic_name else 'Unnamed'}\")\n",
    "            print(f\"Keywords: {', '.join(topic_keywords[topic_idx]['top_words'])}\")\n",
    "            for paper in papers:\n",
    "                print(f\"- {paper['title']} (dominance: {paper['dominance_ratio']:.4f})\")\n",
    "        \n",
    "        # Store results\n",
    "        all_author_stats[field] = author_stats\n",
    "        all_topic_names[field] = field_topic_names\n",
    "        \n",
    "        # Add topic keywords to dataframe metadata\n",
    "        df_analyzed.attrs[f'{field}_topic_keywords'] = topic_keywords\n",
    "        df_analyzed.attrs[f'{field}_topic_names'] = field_topic_names\n",
    "    \n",
    "    # Save results\n",
    "    current_date = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "    output_filename = os.path.join(SAVE_DIR,f\"semantic_scholar_{current_date}{output_suffix}.csv\")\n",
    "    \n",
    "    # Save main results\n",
    "    df_analyzed.to_csv(output_filename, sep=';', encoding='utf-8', \n",
    "                      quoting=csv.QUOTE_NONNUMERIC, escapechar='\\\\')\n",
    "    \n",
    "    # Save author statistics with topic frequencies\n",
    "    author_filename = os.path.join(SAVE_DIR,f\"semantic_scholar_{current_date}_author_analysis.csv\")\n",
    "    save_author_analysis(all_author_stats, author_filename)\n",
    "    \n",
    "    # Save topic names\n",
    "    converted_topic_names = {}\n",
    "    for field, topics in all_topic_names.items():\n",
    "        converted_topic_names[field] = {\n",
    "        int(topic_idx): name \n",
    "        for topic_idx, name in topics.items()\n",
    "        }\n",
    "\n",
    "    topic_names_filename = os.path.join(SAVE_DIR,f\"semantic_scholar_{current_date}_topic_names.json\")\n",
    "    with open(topic_names_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_topic_names, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Print credit usage\n",
    "    print(\"\\nAPI Usage Statistics:\")\n",
    "    stats = credit_tracker.get_stats()\n",
    "    print(f\"Total tokens: {stats['total_tokens']}\")\n",
    "    print(f\"Estimated cost: ${stats['total_cost']}\")\n",
    "    \n",
    "    return df_analyzed, all_author_stats, all_topic_names\n",
    "\"\"\"\n",
    "def analyze_papers_with_topic_names(df, fields_to_analyze=None, n_papers=5, output_suffix=\"_analyzed_results\"):\n",
    "    \"\"\"\n",
    "    Analyze papers with topic naming\n",
    "    \n",
    "    Parameters:\n",
    "    df - DataFrame containing papers\n",
    "    fields_to_analyze - List of fields to filter papers by (if None, use all papers)\n",
    "    n_papers - Number of top papers to extract per topic\n",
    "    output_suffix - Suffix for output files\n",
    "    \"\"\"\n",
    "    # Initialize OpenAI client\n",
    "    client, model_type = initialize_openai()\n",
    "    credit_tracker = CreditTracker()\n",
    "    \n",
    "    # Create a copy of the original dataframe to store all results\n",
    "    df_analyzed = df.copy()\n",
    "    all_author_stats = {}\n",
    "    all_topic_names = {}\n",
    "    \n",
    "    # Filter dataset by fields if specified\n",
    "    if fields_to_analyze:\n",
    "        df_filtered = df[df['fieldsOfStudy'].apply(lambda x: any(field in x for field in fields_to_analyze))]\n",
    "        print(f\"Filtered to {len(df_filtered)} papers from fields: {', '.join(fields_to_analyze)}\")\n",
    "    else:\n",
    "        df_filtered = df\n",
    "        print(f\"Using all {len(df_filtered)} papers for analysis\")\n",
    "    \n",
    "    if df_filtered.empty:\n",
    "        print(\"No papers found after filtering\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Step 1: Model topics on the filtered dataset\n",
    "    lda_model, vectorizer, topic_distributions, df_field, topic_keywords = model_topics(\n",
    "        df_filtered)\n",
    "    \n",
    "    if lda_model is None:\n",
    "        return None, None, None\n",
    "        \n",
    "    # Step 2: Classify papers and add to main dataframe\n",
    "    paper_classifications = classify_papers(topic_distributions, df_field)\n",
    "    \n",
    "    # Step 3: Generate topic names first, so we can use them in column names\n",
    "    top_papers, author_stats = get_top_papers(paper_classifications, df_field, n_top=n_papers)\n",
    "    \n",
    "    # Generate topic names\n",
    "    topic_names = {}\n",
    "    for topic_idx, papers in top_papers.items():\n",
    "        # Combine titles and extract keywords\n",
    "        input_text = \"\\n\\n\".join([str(paper.get('title', '')) for paper in papers])\n",
    "        keywords = topic_keywords[topic_idx]['top_words']\n",
    "        \n",
    "        # Generate topic name\n",
    "        topic_name = generate_topic_name_multiple(input_text, keywords, client, model_type, credit_tracker)\n",
    "        \n",
    "        if topic_name:\n",
    "            topic_names[topic_idx] = topic_name\n",
    "            \n",
    "        # Print topic name and top papers\n",
    "        print(f\"\\nTopic {topic_idx + 1}: {topic_name if topic_name else 'Unnamed'}\")\n",
    "        print(f\"Keywords: {', '.join(topic_keywords[topic_idx]['top_words'])}\")\n",
    "        for paper in papers:\n",
    "            print(f\"- {paper['title']} (dominance: {paper['dominance_ratio']:.4f})\")\n",
    "    \n",
    "    # Step 4: Add classifications to the analyzed dataframe using topic names\n",
    "    for p in paper_classifications:\n",
    "        idx = df_field.index[p['paper_idx']]\n",
    "        primary_topic_idx = p['primary_topic']\n",
    "        secondary_topic_idx = p['secondary_topic']\n",
    "        \n",
    "        # Store topic indices for reference\n",
    "        df_analyzed.loc[idx, 'Primary_Topic_Index'] = primary_topic_idx\n",
    "        df_analyzed.loc[idx, 'Secondary_Topic_Index'] = secondary_topic_idx\n",
    "        \n",
    "        # Store topic names as primary columns\n",
    "        primary_name = topic_names.get(primary_topic_idx, f\"Topic_{primary_topic_idx}\")\n",
    "        secondary_name = topic_names.get(secondary_topic_idx, f\"Topic_{secondary_topic_idx}\")\n",
    "        \n",
    "        df_analyzed.loc[idx, 'Primary_Topic'] = primary_name\n",
    "        df_analyzed.loc[idx, 'Secondary_Topic'] = secondary_name\n",
    "        df_analyzed.loc[idx, 'Primary_Score'] = p['primary_score']\n",
    "        df_analyzed.loc[idx, 'Dominance_Ratio'] = p['dominance_ratio']\n",
    "    \n",
    "    # Store results\n",
    "    all_author_stats['all'] = author_stats\n",
    "    all_topic_names['all'] = topic_names\n",
    "    \n",
    "    # Add topic keywords to dataframe metadata\n",
    "    df_analyzed.attrs['topic_keywords'] = topic_keywords\n",
    "    df_analyzed.attrs['topic_names'] = topic_names\n",
    "    \n",
    "    # Save results\n",
    "    current_date = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "    output_filename = os.path.join(SAVE_DIR, f\"semantic_scholar_{current_date}{output_suffix}.csv\")\n",
    "    \n",
    "    # Save main results\n",
    "    df_analyzed.to_csv(output_filename, sep=';', encoding='utf-8', \n",
    "                      quoting=csv.QUOTE_NONNUMERIC, escapechar='\\\\')\n",
    "    \n",
    "    # Save author statistics with topic frequencies\n",
    "    author_filename = os.path.join(SAVE_DIR, f\"semantic_scholar_{current_date}_author_analysis.csv\")\n",
    "    save_author_analysis(all_author_stats, author_filename)\n",
    "    \n",
    "    # Save topic names\n",
    "    converted_topic_names = {}\n",
    "    for field, topics in all_topic_names.items():\n",
    "        converted_topic_names[field] = {\n",
    "            int(topic_idx): name \n",
    "            for topic_idx, name in topics.items()\n",
    "        }\n",
    "\n",
    "    topic_names_filename = os.path.join(SAVE_DIR, f\"semantic_scholar_{current_date}_topic_names.json\")\n",
    "    with open(topic_names_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(converted_topic_names, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Print credit usage\n",
    "    print(\"\\nAPI Usage Statistics:\")\n",
    "    stats = credit_tracker.get_stats()\n",
    "    print(f\"Total tokens: {stats['total_tokens']}\")\n",
    "    print(f\"Estimated cost: ${stats['total_cost']}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def save_author_analysis(all_author_stats, filename):\n",
    "    \"\"\"Save detailed author analysis including topic frequencies\"\"\"\n",
    "    author_rows = []\n",
    "    \n",
    "    for field, author_stats in all_author_stats.items():\n",
    "        for author_id, stats in author_stats.items():\n",
    "            for topic, topic_stats in stats['topics'].items():\n",
    "                author_rows.append({\n",
    "                    'Field': field,\n",
    "                    'Author_ID': author_id,\n",
    "                    'Author_Name': stats['name'],\n",
    "                    'Topic': topic,\n",
    "                    'Paper_Count': topic_stats['paper_count'],\n",
    "                    'Avg_Dominance': topic_stats['avg_dominance'],\n",
    "                    'Total_Papers': stats['total_papers']\n",
    "                })\n",
    "    \n",
    "    author_df = pd.DataFrame(author_rows)\n",
    "    author_df.to_csv(filename, sep=';', encoding='utf-8', \n",
    "                    quoting=csv.QUOTE_NONNUMERIC, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered to 16939 papers from fields: Computer Science, Economics, Engineering, Physics, Mathematics\n",
      "Analyzing 16939 papers across all fields\n",
      "\n",
      "Topic 1:\n",
      "power (10967.79), control (6555.78), voltage (5748.47), current (3943.91), dc (3439.33), high (2983.18), circuit (2154.45), converter (2009.30), ac (1568.50), connected (1521.70)\n",
      "\n",
      "Topic 2:\n",
      "power (15487.74), energy (7692.56), wind (7497.67), grid (6307.49), generation (5350.14), reliability (4860.13), load (4763.48), capacity (4607.03), distribution (3828.50), pv (3350.94)\n",
      "\n",
      "Topic 3:\n",
      "energy (7590.37), power (3853.70), heat (2037.56), solar (1992.66), plant (1795.41), water (1772.26), gas (1723.82), fuel (1618.81), electricity (1497.94), capacity (1283.07)\n",
      "\n",
      "Topic 4:\n",
      "sensor (1615.53), data (1068.12), iot (971.63), high (955.61), space (926.55), low (842.02), satellite (734.77), sensors (670.73), technology (654.87), sensing (582.79)\n",
      "\n",
      "Topic 5:\n",
      "design (2140.88), high (1068.70), test (941.08), engine (727.44), performance (699.67), testing (597.79), mechanical (562.98), pressure (547.90), speed (529.21), 10 (501.17)\n",
      "\n",
      "Topic 6:\n",
      "systems (2654.20), research (1924.47), data (1778.40), analysis (1496.34), development (1442.50), resilience (1390.73), new (1335.79), reliability (1326.95), information (1132.22), study (1118.34)\n",
      "\n",
      "Topic 7:\n",
      "communication (3525.62), channel (3111.63), wireless (3043.50), capacity (2751.44), transmission (2124.05), performance (2015.86), multiple (1911.04), interference (1799.96), scheme (1738.96), rate (1673.64)\n",
      "\n",
      "Topic 8:\n",
      "power (7626.66), reliability (5425.14), method (4407.61), battery (3784.84), based (3771.93), capacity (3462.48), model (3414.19), proposed (3294.35), results (3151.20), time (3122.93)\n",
      "\n",
      "Topic 9:\n",
      "network (7104.30), power (6969.69), systems (5860.41), based (4638.96), energy (4486.06), data (3997.88), performance (3637.62), reliability (3294.24), time (3284.26), control (3156.17)\n",
      "\n",
      "Topic 10:\n",
      "memory (1659.35), cloud (1399.43), la (625.82), server (588.48), data (501.06), storage (499.11), en (469.94), cache (376.69), yang (357.23), dram (356.90)\n",
      "\n",
      "Processing topic 0\n",
      "Papers for topic 0: 1994\n",
      "Processing author: {'authorId': '100768753', 'name': '朱良合'}\n",
      "Processing author: {'authorId': '101909795', 'name': '栾会'}\n",
      "Processing author: {'authorId': '52340846', 'name': '盛超'}\n",
      "Processing author: {'authorId': '52430229', 'name': '陈晓科'}\n",
      "Processing author: {'authorId': '65841492', 'name': '毛承雄'}\n",
      "Processing author: {'authorId': '100530836', 'name': '翁洪杰'}\n",
      "Processing author: {'authorId': '101520226', 'name': '张俊峰'}\n",
      "Processing author: {'authorId': '65941798', 'name': '陈锐'}\n",
      "Processing author: {'authorId': '73206968', 'name': '袁其平'}\n",
      "Processing author: {'authorId': '72053442', 'name': '李称鑫'}\n",
      "Processing author: {'authorId': '72605903', 'name': '唐朝阳'}\n",
      "Processing author: {'authorId': '119833674', 'name': '熊振兴'}\n",
      "Processing author: {'authorId': '69933096', 'name': '王振民'}\n",
      "Processing author: {'authorId': '150451883', 'name': '陈意庭'}\n",
      "Processing author: {'authorId': '69442004', 'name': '黄石生'}\n",
      "Processing author: {'authorId': '73110359', 'name': '蔡成林'}\n",
      "Processing author: {'authorId': '2060111055', 'name': '刘钊'}\n",
      "Processing author: {'authorId': '73110359', 'name': '蔡成林'}\n",
      "Processing author: {'authorId': '66231026', 'name': '孙孝峰'}\n",
      "Processing author: {'authorId': '2097676692', 'name': '周悦'}\n",
      "Processing author: {'authorId': '2074968619', 'name': '李昕'}\n",
      "Processing author: {'authorId': '103221875', 'name': 'Kyung-Ha Jee'}\n",
      "Processing author: {'authorId': '1450867025', 'name': 'キュン−ハ ジー、'}\n",
      "Processing author: {'authorId': '152945267', 'name': 'T. Kohara'}\n",
      "Processing author: {'authorId': '50278515', 'name': 'T. Noguchi'}\n",
      "Processing author: {'authorId': '47159477', 'name': 'S. Kondo'}\n",
      "Processing author: {'authorId': '72321257', 'name': '徐志望'}\n",
      "Processing author: {'authorId': '71794859', 'name': '陈亦文'}\n",
      "Processing author: {'authorId': '72752313', 'name': '陈道炼'}\n",
      "\n",
      "Processing topic 1\n",
      "Papers for topic 1: 3078\n",
      "Processing author: {'authorId': '3052892', 'name': 'Kyeonghee Cho'}\n",
      "Processing author: {'authorId': '3176862', 'name': 'Jeongje Park'}\n",
      "Processing author: {'authorId': '12340013', 'name': 'T. Oh'}\n",
      "Processing author: {'authorId': '1399924314', 'name': 'Fengming Zhang'}\n",
      "Processing author: {'authorId': '1941804', 'name': 'Jaeseok Choi'}\n",
      "Processing author: {'authorId': '2111526733', 'name': 'Hongseok Choi'}\n",
      "Processing author: {'authorId': '98759303', 'name': 'A. Sendegeya'}\n",
      "Processing author: {'authorId': '145875247', 'name': 'M. Amelin'}\n",
      "Processing author: {'authorId': '145493036', 'name': 'L. Soder'}\n",
      "Processing author: {'authorId': '92109848', 'name': 'E. Lugujjo'}\n",
      "Processing author: {'authorId': '122007469', 'name': 'I. P. Da Silva'}\n",
      "Processing author: {'authorId': '119675269', 'name': 'J.S. Choi'}\n",
      "Processing author: {'authorId': '2331502', 'name': 'T. Tran'}\n",
      "Processing author: {'authorId': '2642338', 'name': 'S. Moon'}\n",
      "Processing author: {'authorId': '48266224', 'name': 'D. Park'}\n",
      "Processing author: {'authorId': '153529850', 'name': 'J. Yoon'}\n",
      "Processing author: {'authorId': '2560520', 'name': 'R. Billinton'}\n",
      "Processing author: {'authorId': '3052892', 'name': 'Kyeonghee Cho'}\n",
      "Processing author: {'authorId': '3176862', 'name': 'Jeongje Park'}\n",
      "Processing author: {'authorId': '1941804', 'name': 'Jaeseok Choi'}\n",
      "Processing author: {'authorId': '1448282211', 'name': 'A. Traca-de-Almeida'}\n",
      "Processing author: {'authorId': '152177797', 'name': 'A. Martins'}\n",
      "Processing author: {'authorId': '51383987', 'name': 'H. Jesus'}\n",
      "Processing author: {'authorId': '152896481', 'name': 'J. Climaco'}\n",
      "Processing author: {'authorId': '152702586', 'name': 'P. Wang'}\n",
      "Processing author: {'authorId': '2560520', 'name': 'R. Billinton'}\n",
      "Processing author: {'authorId': '2560520', 'name': 'R. Billinton'}\n",
      "Processing author: {'authorId': '153760760', 'name': 'L. Gan'}\n",
      "Processing author: {'authorId': '3052892', 'name': 'Kyeonghee Cho'}\n",
      "Processing author: {'authorId': '3176862', 'name': 'Jeongje Park'}\n",
      "Processing author: {'authorId': '12340013', 'name': 'T. Oh'}\n",
      "Processing author: {'authorId': '1941804', 'name': 'Jaeseok Choi'}\n",
      "Processing author: {'authorId': '1381132824', 'name': 'A. El-Keib'}\n",
      "Processing author: {'authorId': '1705137', 'name': 'M. Shahidehpour'}\n",
      "Processing author: {'authorId': '49926214', 'name': 'M. Bashir'}\n",
      "Processing author: {'authorId': '2483320', 'name': 'J. Sadeh'}\n",
      "Processing author: {'authorId': '40038646', 'name': 'K. A. Agyeman'}\n",
      "Processing author: {'authorId': '2436486', 'name': 'Sekyung Han'}\n",
      "\n",
      "Processing topic 2\n",
      "Papers for topic 2: 1065\n",
      "Processing author: {'authorId': '90871416', 'name': 'J. Bianchini'}\n",
      "Processing author: {'authorId': '95087102', 'name': 'R. Rogali'}\n",
      "Processing author: {'authorId': '92365106', 'name': 'J. Wysocki'}\n",
      "Processing author: {'authorId': '2069789064', 'name': 'W. Bradley'}\n",
      "Processing author: {'authorId': '9092405', 'name': 'S. Garimella'}\n",
      "Processing author: {'authorId': '2092793865', 'name': 'V. Garimella'}\n",
      "Processing author: {'authorId': '95087102', 'name': 'R. Rogali'}\n",
      "Processing author: {'authorId': '92365106', 'name': 'J. Wysocki'}\n",
      "Processing author: {'authorId': '94890320', 'name': 'S. Kursman'}\n",
      "Processing author: {'authorId': '91928697', 'name': 'I. Shnaid'}\n",
      "Processing author: {'authorId': '40166430', 'name': 'D. Weiner'}\n",
      "Processing author: {'authorId': '92189164', 'name': 'S. Brokman'}\n",
      "Processing author: {'authorId': '31793505', 'name': 'G. Heath'}\n",
      "Processing author: {'authorId': '11324940', 'name': 'J. J. Burkhardt'}\n",
      "Processing author: {'authorId': '31832842', 'name': 'C. Turchi'}\n",
      "Processing author: {'authorId': '11324940', 'name': 'J. J. Burkhardt'}\n",
      "Processing author: {'authorId': '31793505', 'name': 'G. Heath'}\n",
      "Processing author: {'authorId': '31832842', 'name': 'C. Turchi'}\n",
      "Processing author: {'authorId': '69358335', 'name': 'Y. Tsujikawa'}\n",
      "Processing author: {'authorId': '40306590', 'name': 'L. Harris'}\n",
      "Processing author: {'authorId': '94664486', 'name': 'D. T. Gallaspy'}\n",
      "Processing author: {'authorId': '46539946', 'name': 'T. Johnson'}\n",
      "Processing author: {'authorId': '76923769', 'name': 'R. Sears'}\n",
      "Processing author: {'authorId': '144652049', 'name': 'A. Subiantoro'}\n",
      "\n",
      "Processing topic 3\n",
      "Papers for topic 3: 322\n",
      "Processing author: {'authorId': '2024200', 'name': 'R. Kee'}\n",
      "Processing author: {'authorId': '4957541', 'name': 'R. Corell'}\n",
      "Processing author: {'authorId': '70018296', 'name': 'D. Kolba'}\n",
      "Processing author: {'authorId': '97813256', 'name': 'David Mvelroy'}\n",
      "Processing author: {'authorId': '39764566', 'name': 'W. Cummings'}\n",
      "Processing author: {'authorId': '2346668', 'name': 'J. Caulfield'}\n",
      "Processing author: {'authorId': '3436437', 'name': 'J. Curzan'}\n",
      "Processing author: {'authorId': '2286336221', 'name': 'Herb Sims'}\n",
      "Processing author: {'authorId': '2286344716', 'name': 'Sims. Herb'}\n",
      "Processing author: {'authorId': '9117297', 'name': 'Kosta Varnavas'}\n",
      "Processing author: {'authorId': '2286345031', 'name': 'Eric Eberly'}\n",
      "Processing author: {'authorId': '144729744', 'name': 'M. V. Ortiz'}\n",
      "Processing author: {'authorId': '152607231', 'name': 'Tony D. Coleman'}\n",
      "Processing author: {'authorId': '95436882', 'name': 'R. Pon'}\n",
      "Processing author: {'authorId': '6827531', 'name': 'W. Morey'}\n",
      "Processing author: {'authorId': '71971317', 'name': 'G. Ball'}\n",
      "Processing author: {'authorId': '16595273', 'name': 'E. Stuhlinger'}\n",
      "Processing author: {'authorId': '46615280', 'name': 'Mark Benton'}\n",
      "Processing author: {'authorId': '144190250', 'name': 'A. Gasser'}\n",
      "Processing author: {'authorId': '145908939', 'name': 'E. Kreutz'}\n",
      "Processing author: {'authorId': '152288279', 'name': 'K. Wissenbach'}\n",
      "\n",
      "Processing topic 4\n",
      "Papers for topic 4: 588\n",
      "Processing author: {'authorId': '96073309', 'name': 'Marlin P. Neese'}\n",
      "Processing author: {'authorId': '96063894', 'name': 'James S. Cauble'}\n",
      "Processing author: {'authorId': '96031522', 'name': 'E. Evans'}\n",
      "Processing author: {'authorId': '96755602', 'name': 'R. T. Hines'}\n",
      "Processing author: {'authorId': '92232010', 'name': 'A. J. Couvillion'}\n",
      "Processing author: {'authorId': '35909582', 'name': 'J. D. Smith'}\n",
      "Processing author: {'authorId': '94880761', 'name': 'R. Kuberek'}\n",
      "Processing author: {'authorId': '91378233', 'name': 'D. Noran'}\n",
      "Processing author: {'authorId': '98527888', 'name': 'R. H. Tomren'}\n",
      "Processing author: {'authorId': '97057668', 'name': 'L. Burroughs'}\n",
      "Processing author: {'authorId': '48031088', 'name': 'H. Noguchi'}\n",
      "Processing author: {'authorId': '153840106', 'name': 'Y. Fujita'}\n",
      "Processing author: {'authorId': '72714024', 'name': 'H. Sakai'}\n",
      "Processing author: {'authorId': '73197295', 'name': 'H. Kanesaka'}\n",
      "Processing author: {'authorId': '69901435', 'name': 'H. Bloch'}\n",
      "Processing author: {'authorId': '48453366', 'name': 'C. Wagner'}\n",
      "Processing author: {'authorId': '90845570', 'name': 'W. R. Tausig'}\n",
      "Processing author: {'authorId': '96676508', 'name': 'R. Brackett'}\n",
      "Processing author: {'authorId': '144029286', 'name': 'Hao Zheng'}\n",
      "Processing author: {'authorId': '3357494', 'name': 'Xiangrong Shen'}\n",
      "\n",
      "Processing topic 5\n",
      "Papers for topic 5: 1255\n",
      "Processing author: {'authorId': '1403221687', 'name': \"J. O'Hara\"}\n",
      "Processing author: {'authorId': '34533034', 'name': 'B. Ratner'}\n",
      "Processing author: {'authorId': '37430900', 'name': 'P. Cohen'}\n",
      "Processing author: {'authorId': '30629085', 'name': 'B. Barman'}\n",
      "Processing author: {'authorId': '103755753', 'name': 'K. Mam'}\n",
      "Processing author: {'authorId': '9607504', 'name': 'J. Nagoli'}\n",
      "Processing author: {'authorId': '4195108', 'name': 'E. Allison'}\n",
      "Processing author: {'authorId': '50483997', 'name': 'M. Reiner'}\n",
      "Processing author: {'authorId': '100505487', 'name': 'Lisa McElvaney'}\n",
      "Processing author: {'authorId': '2118494338', 'name': 'W. J. Zhang'}\n",
      "Processing author: {'authorId': '1420153809', 'name': 'A. F. Al-assaf'}\n",
      "Processing author: {'authorId': '12865777', 'name': 'J. Schmele'}\n",
      "Processing author: {'authorId': '1410787229', 'name': 'S. O’Hara'}\n",
      "Processing author: {'authorId': '83932063', 'name': 'J. Ohara'}\n",
      "Processing author: {'authorId': '38278133', 'name': 'J. Wachtel'}\n",
      "Processing author: {'authorId': '6387000', 'name': 'Michael H. Dworkin'}\n",
      "Processing author: {'authorId': '119445164', 'name': 'Rachel Aslin Goldwasser'}\n",
      "Processing author: {'authorId': '48453518', 'name': 'P. Olsson'}\n",
      "Processing author: {'authorId': '49081667', 'name': 'C. Folke'}\n",
      "Processing author: {'authorId': '49209688', 'name': 'T. Hahn'}\n",
      "Processing author: {'authorId': '2111127592', 'name': 'Bokyeong Park'}\n",
      "Processing author: {'authorId': '1741084580', 'name': 'Deok Ryong Yoon'}\n",
      "Processing author: {'authorId': '66228358', 'name': 'Dong‐Eun Rhee'}\n",
      "\n",
      "Processing topic 6\n",
      "Papers for topic 6: 1780\n",
      "Processing author: {'authorId': '50790806', 'name': 'J. Ji'}\n",
      "Processing author: {'authorId': '6403634', 'name': 'Wen Chen'}\n",
      "Processing author: {'authorId': '2235701', 'name': 'Haibin Wan'}\n",
      "Processing author: {'authorId': '2189281', 'name': 'Yong Liu'}\n",
      "Processing author: {'authorId': '7181925', 'name': 'Jin-Kyu Han'}\n",
      "Processing author: {'authorId': '153586578', 'name': 'Dong-Ku Kim'}\n",
      "Processing author: {'authorId': '2607664', 'name': 'Han-kyu Park'}\n",
      "Processing author: {'authorId': '7181925', 'name': 'Jin-Kyu Han'}\n",
      "Processing author: {'authorId': '2607664', 'name': 'Han-kyu Park'}\n",
      "Processing author: {'authorId': '2096797231', 'name': 'Namrata Maharaja'}\n",
      "Processing author: {'authorId': '37389970', 'name': 'R. Bansode'}\n",
      "Processing author: {'authorId': '1398439410', 'name': 'S. A. El-atty'}\n",
      "Processing author: {'authorId': '145707553', 'name': 'Xinyue Guo'}\n",
      "Processing author: {'authorId': '73954285', 'name': 'Keer Zhang'}\n",
      "Processing author: {'authorId': '143834717', 'name': 'Xufa Huang'}\n",
      "Processing author: {'authorId': '34776556', 'name': 'Ligang Ren'}\n",
      "Processing author: {'authorId': '2110155696', 'name': 'Zhijie Yan'}\n",
      "Processing author: {'authorId': '145126238', 'name': 'Mei Song'}\n",
      "Processing author: {'authorId': '1804427', 'name': 'Junde Song'}\n",
      "Processing author: {'authorId': '143705043', 'name': 'Cong Shen'}\n",
      "Processing author: {'authorId': '1744576', 'name': 'L. Dai'}\n",
      "Processing author: {'authorId': '2288351063', 'name': 'Shidong Zhou'}\n",
      "Processing author: {'authorId': '145056595', 'name': 'Yan Yao'}\n",
      "Processing author: {'authorId': '7494161', 'name': 'B. Ng'}\n",
      "Processing author: {'authorId': '144614107', 'name': 'E. Sousa'}\n",
      "Processing author: {'authorId': '2147806', 'name': 'Ya-Han Pan'}\n",
      "Processing author: {'authorId': None, 'name': 'K.B. Leraief'}\n",
      "Processing author: {'authorId': '48572239', 'name': 'Z. Cao'}\n",
      "\n",
      "Processing topic 7\n",
      "Papers for topic 7: 2209\n",
      "Processing author: {'authorId': '2146071484', 'name': 'Lei Chen'}\n",
      "Processing author: {'authorId': '32851018', 'name': 'X. Yi'}\n",
      "Processing author: {'authorId': '2069300645', 'name': 'P. Lu'}\n",
      "Processing author: {'authorId': '2114188352', 'name': 'Tao Ma'}\n",
      "Processing author: {'authorId': '152661064', 'name': 'P. Hou'}\n",
      "Processing author: {'authorId': '2107666338', 'name': 'T. Wang'}\n",
      "Processing author: {'authorId': '2117847279', 'name': 'Q. G. Wang'}\n",
      "Processing author: {'authorId': '2155302980', 'name': 'Peng Wang'}\n",
      "Processing author: {'authorId': '2152563906', 'name': 'Y. X. Zhang'}\n",
      "Processing author: {'authorId': '4874663', 'name': 'Y. Koizumi'}\n",
      "Processing author: {'authorId': '50669044', 'name': 'K. Yotsumoto'}\n",
      "Processing author: {'authorId': '31980868', 'name': 'H. Hayama'}\n",
      "Processing author: {'authorId': '34942462', 'name': 'S. Furubo'}\n",
      "Processing author: {'authorId': '49993085', 'name': 'Yuchen Song'}\n",
      "Processing author: {'authorId': '7416020', 'name': 'Datong Liu'}\n",
      "Processing author: {'authorId': '145730941', 'name': 'Yu Peng'}\n",
      "Processing author: {'authorId': '2706251', 'name': 'X. Ye'}\n",
      "Processing author: {'authorId': '118033164', 'name': 'Qisen Sun'}\n",
      "Processing author: {'authorId': '47113338', 'name': 'Wenwen Li'}\n",
      "Processing author: {'authorId': '2144119534', 'name': 'G. Zhai'}\n",
      "Processing author: {'authorId': '49447421', 'name': 'Bingxiang Sun'}\n",
      "Processing author: {'authorId': '51056373', 'name': 'Pengbo Ren'}\n",
      "Processing author: {'authorId': '90051827', 'name': 'Minming Gong'}\n",
      "Processing author: {'authorId': '102900925', 'name': 'Xingzhen Zhou'}\n",
      "Processing author: {'authorId': '113710594', 'name': 'Jingji Bian'}\n",
      "Processing author: {'authorId': '31934220', 'name': 'Bhuvnesh Rathore'}\n",
      "Processing author: {'authorId': '2110418897', 'name': 'Megha Singh'}\n",
      "Processing author: {'authorId': '152760820', 'name': 'F. H. Gandoman'}\n",
      "Processing author: {'authorId': '22397935', 'name': 'Y. Firouz'}\n",
      "Processing author: {'authorId': '123221202', 'name': 'M. Hosen'}\n",
      "Processing author: {'authorId': '15476855', 'name': 'T. Kalogiannis'}\n",
      "Processing author: {'authorId': '3418155', 'name': 'J. Jaguemont'}\n",
      "Processing author: {'authorId': '2462348', 'name': 'M. Berecibar'}\n",
      "Processing author: {'authorId': '117724783', 'name': 'J. Van Mierlo'}\n",
      "Processing author: {'authorId': '30653688', 'name': 'A. Patton'}\n",
      "Processing author: {'authorId': '2260222', 'name': 'M. K. Rahmat'}\n",
      "Processing author: {'authorId': '1799181', 'name': 'S. Jovanovic'}\n",
      "\n",
      "Processing topic 8\n",
      "Papers for topic 8: 3724\n",
      "Processing author: {'authorId': '20579450', 'name': 'D. Hall'}\n",
      "Processing author: {'authorId': '2983714', 'name': 'S. Ciraci'}\n",
      "Processing author: {'authorId': '144926880', 'name': 'Jian Yin'}\n",
      "Processing author: {'authorId': '3195412', 'name': 'L. Svobodova'}\n",
      "Processing author: {'authorId': '1922577', 'name': 'Hao Che'}\n",
      "Processing author: {'authorId': '103030158', 'name': 'San-qi Li'}\n",
      "Processing author: {'authorId': '2677603', 'name': 'A. Lin'}\n",
      "Processing author: {'authorId': '2117121312', 'name': 'Jie Zhu'}\n",
      "Processing author: {'authorId': '2116821399', 'name': 'Juanjuan Li'}\n",
      "Processing author: {'authorId': '1865824', 'name': 'Erikson Hardesty'}\n",
      "Processing author: {'authorId': '145496305', 'name': 'Hai Jiang'}\n",
      "Processing author: {'authorId': '69486668', 'name': 'Kuan-Ching Li'}\n",
      "Processing author: {'authorId': '145674819', 'name': 'K. Shuaib'}\n",
      "Processing author: {'authorId': '1783739', 'name': 'Issa M. Khalil'}\n",
      "Processing author: {'authorId': '145033713', 'name': 'Mohammed Abdelhafez'}\n",
      "Processing author: {'authorId': '29635537', 'name': 'Gang-Yu Xiong'}\n",
      "Processing author: {'authorId': '2427557', 'name': 'T. Nyberg'}\n",
      "Processing author: {'authorId': '50389672', 'name': 'P. Hamalainen'}\n",
      "Processing author: {'authorId': '39083111', 'name': 'Xisong Dong'}\n",
      "Processing author: {'authorId': '2143860798', 'name': 'Yuan Liu'}\n",
      "Processing author: {'authorId': '51305184', 'name': 'Jiacheng Hou'}\n",
      "Processing author: {'authorId': '145356314', 'name': 'Harold Lim'}\n",
      "Processing author: {'authorId': '144753525', 'name': 'A. Kansal'}\n",
      "Processing author: {'authorId': '2146651037', 'name': 'Jie Liu'}\n",
      "Processing author: {'authorId': '2117121312', 'name': 'Jie Zhu'}\n",
      "Processing author: {'authorId': '145496305', 'name': 'Hai Jiang'}\n",
      "Processing author: {'authorId': '2116821399', 'name': 'Juanjuan Li'}\n",
      "Processing author: {'authorId': '1865824', 'name': 'Erikson Hardesty'}\n",
      "Processing author: {'authorId': '69486668', 'name': 'Kuan-Ching Li'}\n",
      "Processing author: {'authorId': '48458819', 'name': 'Zhongwen Li'}\n",
      "Processing author: {'authorId': '8879996', 'name': 'Felipe Rocha da Rosa'}\n",
      "Processing author: {'authorId': '143766367', 'name': 'R. Reis'}\n",
      "Processing author: {'authorId': '2623594', 'name': 'Luciano Ost'}\n",
      "\n",
      "Processing topic 9\n",
      "Papers for topic 9: 924\n",
      "Processing author: {'authorId': '101319241', 'name': 'A. Hamdi'}\n",
      "Processing author: {'authorId': '2728369', 'name': 'P. Pouyan'}\n",
      "Processing author: {'authorId': '145856590', 'name': 'Machmud Effendy'}\n",
      "Processing author: {'authorId': '97657652', 'name': 'Nandi Wardhana'}\n",
      "Processing author: {'authorId': '2101628986', 'name': 'Salvatore Spadaro'}\n",
      "Processing author: {'authorId': '103088196', 'name': 'Cahyana'}\n",
      "Processing author: {'authorId': '38633931', 'name': 'M. Etinski'}\n",
      "Processing author: {'authorId': '31502490', 'name': 'Kholed Langsari'}\n",
      "Processing author: {'authorId': '71884443', 'name': 'Achmad Imam Agung'}\n",
      "Processing author: {'authorId': '90263542', 'name': 'Hufiadi Hufiadi'}\n",
      "Processing author: {'authorId': '72199515', 'name': 'Sugeng Hari Wisudo'}\n",
      "\n",
      "Topic 1: Power Control Systems\n",
      "Keywords: power, control, voltage, current, dc, high, circuit, converter, ac, connected\n",
      "- An AC excitation system based on full-controlled member (dominance: 19.4725)\n",
      "- SPWM (Sinusoidal Pulse Width Modulation) control method of inverter based on STC (Sensitivity Time Control) single chip microcomputer (dominance: 13.5666)\n",
      "- Power converting control apparatus for fuel cell (dominance: 9.1482)\n",
      "- Controller device of intelligent residual current circuit breaker (dominance: 8.3063)\n",
      "- High-current switch power supply and high-current switch power supply system (dominance: 7.9253)\n",
      "- Intelligent controller device of residual current breaker (dominance: 7.4503)\n",
      "- Non-isolated three-port serial-parallel integrated converter (dominance: 7.3580)\n",
      "- Automatic voltage control circuit of reactive power control system (dominance: 7.2613)\n",
      "- Carrier phase error detection method and synchronization control of parallel‐connected PWM inverters without signal line (dominance: 7.2057)\n",
      "- A double-isolation boosting multi-input direct current convertor (dominance: 6.7597)\n",
      "\n",
      "Topic 2: Renewable Energy Reliability\n",
      "Keywords: power, energy, wind, grid, generation, reliability, load, capacity, distribution, pv\n",
      "- A Study on Grid Expansion Planning of Power System Including Wind Turbine Generator (dominance: 14.6481)\n",
      "- Altruistic versus Profit Maximising System Operators of Rural Power Systems (dominance: 12.0762)\n",
      "- Tie line equivalent constrained assisting generator model (TEAG) considering forced outage rates of transmission systems-II (dominance: 9.8565)\n",
      "- Probabilistic Reliability Based Grid Expansion Planning of Power System Including Wind Turbine Generators (dominance: 9.1355)\n",
      "- Source Reliability in a Combined Wind-Solar-Hydro System (dominance: 9.0885)\n",
      "- Rleliability Benefit Analysis of Adding WTG in a Distribution System (dominance: 7.6292)\n",
      "- Wind power modeling and application in generating adequacy assessment (dominance: 7.5181)\n",
      "- Probabilistic reliability criterion for expansion planning of grids including wind turbine generators (dominance: 7.3181)\n",
      "- Optimal sizing of hybrid wind/photovoltaic/battery considering the uncertainty of wind and photovoltaic power using Monte Carlo (dominance: 7.1134)\n",
      "- Evaluation of reliability index and worth analysis for on day-ahead unit commitment using LLP (dominance: 6.9396)\n",
      "\n",
      "Topic 3: Energy Generation Systems\n",
      "Keywords: energy, power, heat, solar, plant, water, gas, fuel, electricity, capacity\n",
      "- Preliminary assessment of alternative atmospheric fluidized-bed-combustion power-plant systems. Final report (dominance: 14.9134)\n",
      "- Commercial Boiler Waste-Heat Utilization for Air Conditioning in Developing Countries (dominance: 8.8852)\n",
      "- Assessment of atmospheric fluidized-bed combustion recycle systems. Final report (dominance: 8.5918)\n",
      "- Novel Compressed Air Energy Storage (CAES) Systems Applying Air Expanders (dominance: 7.6840)\n",
      "- Life Cycle Assessment of a Parabolic Trough Concentrating Solar Power Plant and Impacts of Key Design Alternatives: Preprint (dominance: 6.8142)\n",
      "- Life cycle assessment of a parabolic trough concentrating solar power plant and the impacts of key design alternatives. (dominance: 6.6623)\n",
      "- A Highly Efficient Cogeneration System Using APT Coupled With Biomass Gasification (dominance: 6.5425)\n",
      "- Design and calculated performance and cost of the ECAS Phase II open cycle MHD power generation system (dominance: 6.2723)\n",
      "- Southern Company Services' study of a Kellogg Rust Westinghouse (KRW)-based gasification-combined-cycle (GCC) power plant (dominance: 6.0539)\n",
      "- Improving Energy Efficiency of a Refrigeration System with a Rankine Cycle and an Expander (dominance: 5.2166)\n",
      "\n",
      "Topic 4: Space-based Sensor Technology\n",
      "Keywords: sensor, data, iot, high, space, low, satellite, sensors, technology, sensing\n",
      "- The Feasibility of Interrogating and Locating Ocean Platforms with Orbiting Satellites (dominance: 1.9211)\n",
      "- Lightweight EHF satellites for augmentation of anti-jam communications (dominance: 1.7318)\n",
      "- Small pixel infrared sensor technology (dominance: 1.6325)\n",
      "- Programmable Ultra Lightweight System Adaptable Radio (PULSAR) Low Cost Telemetry - Access from Space Advanced Technologies or Down the Middle (dominance: 1.6194)\n",
      "- Programmable Ultra Lightweight System Adaptable Radio (PULSAR) Low Cost Telemetry - Access from Space Advanced Technologies or Down the Middle (dominance: 1.6194)\n",
      "- Long-term performance of a high average-power tunable laser system for photodynamic therapy (dominance: 1.5704)\n",
      "- Fiber Bragg grating technology (dominance: 1.5301)\n",
      "- Electronics in Planning Space Flights (dominance: 1.4606)\n",
      "- A Conceptual Mars Exploration Vehicle Architecture with Chemical Propulsion, Near-Term Technology, and High Modularity to Enable Near-Term Human Missions to Mars (dominance: 1.2950)\n",
      "- Beam guiding and shaping for surface processing with laser radiation (dominance: 1.2562)\n",
      "\n",
      "Topic 5: Mechanical Systems Engineering\n",
      "Keywords: design, high, test, engine, performance, testing, mechanical, pressure, speed, 10\n",
      "- INTEGRATED ENGINEERING AND SERVICE TESTS OF SYSTEM, VESSEL MOORING, MULTI-LEG. (dominance: 19.3895)\n",
      "- HIGH RESPONSE, LOW LEVEL PULSE ENGINE THRUST STAND SYSTEM. (dominance: 6.6004)\n",
      "- Innovative workover/drilling rigs to utilize hydraulics (dominance: 4.3862)\n",
      "- Cycloidal Cam Transmission. Revision (dominance: 4.0969)\n",
      "- POWER TRANSMISSION STUDIES FOR SHAFTDRIVEN HEAVY-LIFT HELICOPTERS (dominance: 3.6547)\n",
      "- A NEW TYPE OF MILLER CYCLE GASOLINE ENGINE (2ND REPORT--PERFORMANCE OF SUPERCHARGED ENGINE) (dominance: 2.6494)\n",
      "- A practical guide to compressor technology (dominance: 2.1009)\n",
      "- Baseline Gas Turbine Development Program fifth quarterly progress report (dominance: 2.0712)\n",
      "- Anacapa Island split pipe inspection of June 1977 and April 1978. Technical note, Sep 74-Jun 78 (dominance: 2.0411)\n",
      "- Sleeve muscle actuator and its application in transtibial prostheses (dominance: 1.9649)\n",
      "\n",
      "Topic 6: Human-System Governance\n",
      "Keywords: systems, research, data, analysis, development, resilience, new, reliability, information, study\n",
      "- Evaluation of Complex Human-Machine Systems Using HFE Guidelines (dominance: 12.6523)\n",
      "- Governance of aquatic agricultural systems: analyzing representation, power, and accountability (dominance: 11.8411)\n",
      "- Foundational infrastructure framework for city resilience (dominance: 6.8393)\n",
      "- Guest Editor's foreword (dominance: 6.7641)\n",
      "- The textbook of total quality in healthcare (dominance: 6.0856)\n",
      "- Sustainability: Social and Ecological Dimensions (dominance: 5.8238)\n",
      "- The development and evaluation of guidelines for the review of advanced human-system interfaces (dominance: 5.3631)\n",
      "- Ensuring Consideration of the Public Interest in the Governance and Accountability of Regional Transmission Organizations (dominance: 4.4432)\n",
      "- Social-Ecological Transformation for Ecosystem Management: the Development of Adaptive Co-management of a Wetland Landscape in Southern Sweden (dominance: 4.4338)\n",
      "- 글로벌 금융위기 이후 국제경제환경의 변화와 한국의 대외경제정책 방향 (제2권) (Changes in International Economic Order after the Global Financial Crisis and Korea's International Economic Policy-2) (dominance: 4.3748)\n",
      "\n",
      "Topic 7: Wireless Communication Capacity\n",
      "Keywords: communication, channel, wireless, capacity, transmission, performance, multiple, interference, scheme, rate\n",
      "- Capacity Analysis of Multicast Network in Spectrum Sharing Systems (dominance: 7.3985)\n",
      "- SVD pre/post-rake with adaptive trellis coded modulation (dominance: 6.1929)\n",
      "- SVD pre/post-RAKE with adaptive trellis-coded modulation for TDD DSSS applications (dominance: 5.1786)\n",
      "- Performance Evaluation of Spatial Multiplexing MIMO-OFDM System using MMSE Detection under Frequency Selective Rayleigh Channel (dominance: 4.9208)\n",
      "- Efficient Packet Scheduling with Pre-defined QoS using Cross-Layer Technique in Wireless Networks (dominance: 4.8809)\n",
      "- Switching MIMO System with Adaptive OFDM Modulation for Indoor Visible Light Communication (dominance: 4.8483)\n",
      "- An improved water-filling algorithm for mobile MIMO communication systems over time-varying fading channels (dominance: 4.6607)\n",
      "- A novel spectral efficient transmit precoder scheme based on channel feedback (dominance: 4.6371)\n",
      "- Spread space-spectrum multiple access (dominance: 4.5445)\n",
      "- Space-time coded adaptive transmit antenna arrays for OFDM wireless systems utilizing channel side information (dominance: 4.5374)\n",
      "\n",
      "Topic 8: Power System Reliability\n",
      "Keywords: power, reliability, method, battery, based, capacity, model, proposed, results, time\n",
      "- Quantitative reliability analysis method for power systems with multi-level standby structure based on GO method (dominance: 6.9641)\n",
      "- Thermal monitoring and reliability analysis system for underground substation (dominance: 6.3763)\n",
      "- Design considerations of power systems for the air conditioner used in telecommunication networks (dominance: 6.2139)\n",
      "- Series-connected lithium-ion battery pack health modeling with cell inconsistency evaluation (dominance: 6.0571)\n",
      "- Life prediction of lithium thionyl chloride batteries based on the pulse load test and accelerated degradation test (dominance: 5.9061)\n",
      "- SOH Estimation for Li-ion Batteries Based on Features of IC Curves and Multi-output Gaussian Process Regression Method (dominance: 5.8137)\n",
      "- Stockwell Transform based Decision Tree for Transmission Line Fault Diagnosis (dominance: 5.4987)\n",
      "- Reliability Assessment of NMC Li-Ion Battery for Electric Vehicles Application (dominance: 5.3831)\n",
      "- Duty cycle effects on generating unit availability (dominance: 5.3063)\n",
      "- Power systems reliability estimation method (dominance: 5.2932)\n",
      "\n",
      "Topic 9: Distributed Systems Management\n",
      "Keywords: network, power, systems, based, energy, data, performance, reliability, time, control\n",
      "- Computer system isolates faults (dominance: 62.3194)\n",
      "- Towards a scalable and reliable real time in-network data analysis infrastructure (dominance: 51.2103)\n",
      "- Performance Problems In Distributed Systems (dominance: 29.9007)\n",
      "- Adaptive resource management for flow-based IP/ATM hybrid switching systems (dominance: 20.5661)\n",
      "- GPU-in-Hadoop: Enabling MapReduce across distributed heterogeneous platforms (dominance: 19.4271)\n",
      "- Communications in Smart Grid: A Review with Performance, Reliability and Security Consideration (dominance: 18.0583)\n",
      "- To enhance power distribution network management of local power service enterprise by using cloud platform (dominance: 15.1029)\n",
      "- Power Budgeting for Virtualized Data Centers (dominance: 14.7412)\n",
      "- Embedding GPU Computations in Hadoop (dominance: 14.6524)\n",
      "- Early evaluation of multicore systems soft error reliability using virtual platforms (dominance: 13.0068)\n",
      "\n",
      "Topic 10: Energy Generation Systems\n",
      "Keywords: memory, cloud, la, server, data, storage, en, cache, yang, dram\n",
      "- SISTEM PEMBUATAN LAPORAN OPERASIONAL PENDUKUNG PENAGIHAN DI PEMBANGKIT LISTRIK TENAGA GAS DAN UAP (PLTGU) SENGKANG (dominance: 9.5332)\n",
      "- Reliability-aware memory design using advanced reconfiguration mechanisms (dominance: 2.2734)\n",
      "- PENGGUNAAN TEKNOLOGI MPPT (MAXIMUM POWER POINT TRACKER) PADA SISTEM PEMBANGKIT LISTRIK TENAGA ANGIN (PLTB) (dominance: 2.0875)\n",
      "- ANALISIS KEANDALAN KOMPOSIT PEMBANGKIT DAN TRANSMISI (KONTINGENSI N-2) SISTEM TENAGA LISTRIK (dominance: 1.9020)\n",
      "- Enginyeria de tràfic en xarxes de transport òptiques per a entorns d'àrea metropolitana (RPR) i de gran abast (ASON) (dominance: 1.8439)\n",
      "- PEMBUATAN PEMBANGKIT LISTRIK TENAGA SURYA UNTUK PEMBANGKIT LISTRIK TENAGA HIBRID BERBASIS TENAGA AIR DAN SURYA (dominance: 1.7975)\n",
      "- DVFS power management in HPC systems (dominance: 1.7798)\n",
      "- The Impact Of Design Patterns In Refactoring Technique To Measure Performance Efficiency (dominance: 1.7756)\n",
      "- POTENSI SUMBER ENERGI ALTERNATIF DALAM MENDUKUNG KELISTRIKAN NASIONAL (dominance: 1.7731)\n",
      "- EFISIENSI TEKNIS PERIKANAN PUKAT CINCIN DI PEKALONGAN (dominance: 1.7465)\n",
      "\n",
      "API Usage Statistics:\n",
      "Total tokens: 5395\n",
      "Estimated cost: $0.0008\n"
     ]
    }
   ],
   "source": [
    "# Analyze papers with topic naming\n",
    "fields_to_analyze = ['Computer Science', 'Engineering', 'Physics', 'Mathematics', 'Business', 'Environmental Science']\n",
    "\n",
    "df_analyzed = analyze_papers_with_topic_names(df,fields_to_analyze=fields_to_analyze, n_papers=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define valid fields and clean fields of study\n",
    "valid_fields = ['Computer Science', 'Economics', 'Engineering', 'Physics', 'Mathematics', 'Medicine', 'Business', 'Environmental Science', 'Chemistry', 'Materials Science', 'Geography', 'Biology', 'Geology', 'Political Science', 'Psychology', 'Com']\n",
    "df['fieldsOfStudy'] = df['fieldsOfStudy'].apply(clean_fields_of_study)\n",
    "# Filter out papers with excluded fields\n",
    "exclude_fields = ['Chemistry', 'Materials Science', 'Geography', 'Biology', 'Geology', 'Medicine', 'Political Science', 'Psychology', 'Com']\n",
    "df_filtered = df[df['fieldsOfStudy'].apply(lambda x: not set(x).issubset(set(exclude_fields)))]\n",
    "# Get unique fields of study\n",
    "unique_fields = set([field for fields in df_filtered['fieldsOfStudy'] for field in fields if field not in exclude_fields])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
