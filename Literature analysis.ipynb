{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import configparser \n",
    "import tiktoken\n",
    "from difflib import SequenceMatcher\n",
    "from datetime import datetime\n",
    "import json\n",
    "import ast\n",
    "import csv\n",
    "import os\n",
    "\n",
    "SAVE_DIR = \"Saved_files\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\STSI/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\STSI/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\STSI/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK data and handle stop words\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def get_custom_stop_words(search_keywords=None):\n",
    "    \n",
    "    # Get standard stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Words to keep (search keywords)\n",
    "    words_to_keep = set()\n",
    "    if search_keywords:\n",
    "        # Convert keywords to lowercase and split multi-word terms\n",
    "        for keyword in search_keywords:\n",
    "            keyword = keyword.lower()\n",
    "            # Add both the full phrase and individual words\n",
    "            words_to_keep.add(keyword)\n",
    "            for word in keyword.split():\n",
    "                words_to_keep.add(word)\n",
    "    \n",
    "    # Remove search keywords from stopwords\n",
    "    stop_words = stop_words - words_to_keep\n",
    "    \n",
    "    # Scientific paper terms to exclude (add to stopwords)\n",
    "    scientific_terms = {\n",
    "        # Citation terms\n",
    "        'et', 'al', 'ref', 'reference', 'references', 'cited', 'cite',\n",
    "        # Figure and table references\n",
    "        'fig', 'figure', 'figures', 'table', 'tables', 'chart', 'charts',\n",
    "        # Publication terms\n",
    "        'published', 'journal', 'conference', 'proceedings',\n",
    "        # Measurement units and numbers\n",
    "        'vol', 'volume', 'pp', 'page', 'pages', 'doi'\n",
    "    }\n",
    "    \n",
    "    # Add scientific terms to stopwords\n",
    "    stop_words = stop_words.union(scientific_terms)\n",
    "    \n",
    "    return stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some code for AI assistance\n",
    "# Initialize the LLM API\n",
    "def initialize_openai():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('config_LLM.txt')\n",
    "    api_key = config['LLM'].get('OPENAI_API_KEY')\n",
    "    model_type = config['LLM'].get('MODEL_TYPE')\n",
    "    return OpenAI(api_key=api_key), model_type\n",
    "\n",
    "# Credit tracker to keep track of the cost when using paid API\n",
    "class CreditTracker:\n",
    "    def __init__(self):\n",
    "        self.total_tokens = 0\n",
    "        self.total_cost = 0\n",
    "        self.cost_per_1k_tokens = 0.00015\n",
    "\n",
    "    def update(self, tokens):\n",
    "        self.total_tokens += tokens\n",
    "        self.total_cost += (tokens / 1000) * self.cost_per_1k_tokens\n",
    "\n",
    "    def get_stats(self):\n",
    "        return {\n",
    "            \"total_tokens\": self.total_tokens,\n",
    "            \"total_cost\": round(self.total_cost, 4)\n",
    "        }\n",
    "\n",
    "credit_tracker = CreditTracker()\n",
    "\n",
    "def num_tokens_from_string(string: str, model_name: str) -> int:\n",
    "    encoding = tiktoken.encoding_for_model(model_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess function\n",
    "def preprocess_text(text, search_keywords):\n",
    "    if not isinstance(text, (str, int, float)):\n",
    "        return ''\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s-]', '', text)\n",
    "    tokens = word_tokenize(text) # Split text into individual words/tokens using NLTK's tokenizer\n",
    "    stop_words = get_custom_stop_words(search_keywords)\n",
    "    tokens = [token for token in tokens if token not in stop_words] # Remove all stop words from our tokens using list comprehension\n",
    "    lemmatizer = WordNetLemmatizer() # Initialize WordNet lemmatizer to reduce words to their base/dictionary form\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens] # Convert each token to its lemma (e.g., \"systems\" → \"system\", \"running\" → \"run\")\n",
    "    return ' '.join(tokens) # Combine all processed tokens back into a single string with spaces between words\n",
    "\n",
    "\n",
    "# Function to convert string representation of list to actual list, replacing long strings with \"Unknown\"\n",
    "def string_to_list(s):\n",
    "    if isinstance(s, str):\n",
    "        # Remove brackets and split by comma\n",
    "        fields = [field.strip().strip(\"'\") for field in s.strip('[]').split(',')]\n",
    "        # Replace fields wit more than 100 characters with \"Unknown\"\n",
    "        return [\"Unknown\" if len(field) > 100 else field for field in fields]\n",
    "    return [\"Unknown\"]  # Return [\"Unknown\"] for empty or non-string entries\n",
    "\n",
    "# Clean fields of study\n",
    "def clean_fields_of_study(s):\n",
    "    valid_fields= ['Computer Science', 'Economics', 'Engineering', 'Physics', 'Mathematics', 'Medicine', 'Business', 'Environmental Science', 'Chemistry', 'Materials Science', 'Geography', 'Biology', 'Geology', 'Political Science', 'Psychology', 'Com']\n",
    "    if pd.isna(s) or s == '[]':\n",
    "        return [\"Unknown\"]\n",
    "    if isinstance(s, str):\n",
    "        fields = [field.strip().strip(\"'\\\"\") for field in s.strip('[]').split(',')]\n",
    "        cleaned_fields = []\n",
    "        for field in fields:\n",
    "            if field in valid_fields:\n",
    "                cleaned_fields.append(field)\n",
    "            else:\n",
    "                cleaned_fields.append(\"Unknown\")\n",
    "        return cleaned_fields if cleaned_fields else [\"Unknown\"]\n",
    "    return [\"Unknown\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple analysis to extract keyword and n-gram frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_frequencies(vectorizer, texts):\n",
    "    matrix = vectorizer.fit_transform(texts)\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    freqs = matrix.sum(axis=0).A1\n",
    "    return dict(sorted(zip(terms, freqs.tolist()), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "def extract_keywords_and_ngrams(df, max_features=1000):\n",
    "    # Create vectorizers\n",
    "    keyword_vectorizer = CountVectorizer(\n",
    "        max_df=0.95, \n",
    "        min_df=2,\n",
    "        #stop_words='english',# can be removed as stopwords are already removed from processed_text\n",
    "        max_features=max_features,\n",
    "        token_pattern=r'(?u)\\b[A-Za-z][A-Za-z-]+[A-Za-z]\\b'\n",
    "    )\n",
    "    \n",
    "    bigram_vectorizer = CountVectorizer(\n",
    "        ngram_range=(2,2),\n",
    "        max_df=0.95,\n",
    "        min_df=2,\n",
    "        #stop_words='english',# can be removed as stopwords are already removed from processed_text\n",
    "        max_features=max_features\n",
    "    )\n",
    "    \n",
    "    trigram_vectorizer = CountVectorizer(\n",
    "        ngram_range=(3,3),\n",
    "        max_df=0.95,\n",
    "        min_df=2,\n",
    "        max_features=max_features,\n",
    "        #stop_words='english' # can be removed as stopwords are already removed from processed_text\n",
    "    )\n",
    "    \n",
    "    # Extract frequencies\n",
    "    keyword_freq = get_term_frequencies(keyword_vectorizer, df['processed_text'])\n",
    "    bigram_freq = get_term_frequencies(bigram_vectorizer, df['processed_text'])\n",
    "    trigram_freq = get_term_frequencies(trigram_vectorizer, df['processed_text'])\n",
    "    \n",
    "    # Save results\n",
    "    current_date = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "    filename=os.path.join(SAVE_DIR,f'term_frequencies_{current_date}.json')\n",
    "    results = {\n",
    "        'keywords': keyword_freq,\n",
    "        'bigrams': bigram_freq,\n",
    "        'trigrams': trigram_freq\n",
    "    }\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Results saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_topic_keywords(lda_model, vectorizer, num_words=10):\n",
    "    \"\"\"Extract keywords and n-grams for each topic\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topic_keywords = {}\n",
    "    \n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        # Get top words\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-num_words-1:-1]]\n",
    "        \n",
    "        # Get word weights\n",
    "        word_weights = [(feature_names[i], topic[i]) \n",
    "                       for i in topic.argsort()[:-num_words-1:-1]]\n",
    "        \n",
    "        topic_keywords[topic_idx] = {\n",
    "            'top_words': top_words,\n",
    "            'word_weights': word_weights\n",
    "        }\n",
    "    \n",
    "    return topic_keywords\n",
    "\n",
    "# Model topics function\n",
    "def model_topics_by_field(df, field, num_topics=10, num_words=5):\n",
    "    df_field = df[df['fieldsOfStudy'].apply(lambda x: field in x)] # filtering so that only the documents within the field is keept.\n",
    "    print (f\"Analyzing {len(df_field)} papers\")\n",
    "    if df_field.empty:\n",
    "            print(f\"No papers found for field: {field}\")            #warning if no papers found\n",
    "            return None, None, None, None, None\n",
    "    text_data = df_field['abstract'].fillna('')                     # filtering to avoid errors due to missing fields\n",
    "    \n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english') # vectorizing with max_df = 0.95 as default (terms that appear in more than 95% of documents) and min_df =2 (terms that appear in less than 2 of the documents). Values below 1 indicates percentile, and values above 1 indicates number of docs.\n",
    "    doc_term_matrix = vectorizer.fit_transform(text_data) #creating the term matrix:  vectorizer : sklearn.feature_extraction.text.(CountVectorizer, TfIdfVectorizer).vectorizer used to convert raw documents to document-term matrix (`dtm`)\n",
    "    \n",
    "    #Using the sklearn decomposition LatentDirchletAllocation (see https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html). \n",
    "    #Fit the LDA model\n",
    "    lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42) \n",
    "    #Randomstate 42 is passed to maintain reproducibility in results (0 and 42 are commonly used values. Default \"None\" will result in using globa radnom instande (numpy.radom) and re-running may produce different results\n",
    "    \n",
    "    topic_distribution = lda_model.fit_transform(doc_term_matrix)\n",
    "    topic_keywords=extract_topic_keywords(lda_model, vectorizer)\n",
    "    \n",
    "    #feature_names = vectorizer.get_feature_names_out()\n",
    "   \n",
    "    for topic_idx, keywords in topic_keywords.items():\n",
    "        print(f\"\\nTopic {topic_idx + 1}:\")\n",
    "        # Format each word with its weight in parentheses\n",
    "        formatted_words = [f\"{word} ({weight:.2f})\" \n",
    "                         for word, weight in keywords['word_weights']]\n",
    "        print(\", \".join(formatted_words)) \n",
    "    return lda_model, vectorizer, topic_distribution, df_field, topic_keywords\n",
    "\n",
    "def model_topics(df, num_topics=10, num_words=5):\n",
    "    \n",
    "    print(f\"Analyzing {len(df)} papers across all fields\")\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"No papers found for analysis\")\n",
    "        return None, None, None, None, None\n",
    "        \n",
    "    text_data = df['abstract'].fillna('')\n",
    "    \n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    doc_term_matrix = vectorizer.fit_transform(text_data)\n",
    "    \n",
    "    lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "    topic_distribution = lda_model.fit_transform(doc_term_matrix)\n",
    "    topic_keywords = extract_topic_keywords(lda_model, vectorizer)\n",
    "    \n",
    "    for topic_idx, keywords in topic_keywords.items():\n",
    "        print(f\"\\nTopic {topic_idx + 1}:\")\n",
    "        formatted_words = [f\"{word} ({weight:.2f})\" \n",
    "                         for word, weight in keywords['word_weights']]\n",
    "        print(\", \".join(formatted_words))\n",
    "        \n",
    "    return lda_model, vectorizer, topic_distribution, df, topic_keywords\n",
    "\n",
    "\n",
    "# Generate topic name function\n",
    "def generate_topic_name(text, keywords, client, model_type, credit_tracker):\n",
    "    try:\n",
    "        tokens = num_tokens_from_string(text + \" \" + \" \".join(keywords), model_type)\n",
    "        credit_tracker.update(tokens)\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_type,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful scientific assistant that generates concise topic expressions based on academic paper titles and keywords for a collection of papers.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"\"\"Based on the following titles and keywords obtained by LDA-analysis, provide a concise single word, bigram, or trigram that best describes the main topic these abstracts have in common. The expression should be specific and descriptive and feasible for categorization.\n",
    "\n",
    "Title:\n",
    "{text}\n",
    "\n",
    "Keywords:\n",
    "{', '.join(keywords)}\n",
    "\n",
    "Concise topic expression:\"\"\"}\n",
    "            ]\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        credit_tracker.update(num_tokens_from_string(content, model_type))\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def string_similarity(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def generate_topic_name_multiple(text, keywords, client, model_type, credit_tracker, initial_iterations=3, max_iterations=10, similarity_threshold=0.7):\n",
    "    iterations = initial_iterations\n",
    "    while iterations <= max_iterations:\n",
    "        generated_names = []\n",
    "        for _ in range(iterations):\n",
    "            # Reuse the generate_topic_name function instead of duplicating code\n",
    "            name = generate_topic_name(text, keywords, client, model_type, credit_tracker)\n",
    "            if name:\n",
    "                generated_names.append(name)\n",
    "        \n",
    "        # Check for dominant topic\n",
    "        for i, name in enumerate(generated_names):\n",
    "            similar_names = [other_name for j, other_name in enumerate(generated_names) \n",
    "                             if i != j and string_similarity(name, other_name) >= similarity_threshold]\n",
    "            if len(similar_names) >= len(generated_names) // 2:\n",
    "                return name  # Return the dominant topic name\n",
    "\n",
    "        # If no dominant topic found, increase iterations\n",
    "        iterations += 2\n",
    "        print(f\"No clear common topic name found. Increasing iterations to {iterations}.\")\n",
    "\n",
    "    # If max iterations reached without finding a dominant topic, return the most common name\n",
    "    from collections import Counter\n",
    "    return Counter(generated_names).most_common(1)[0][0]\n",
    "\n",
    "\n",
    "\n",
    "def classify_papers(topic_distributions, df_field):\n",
    "    \"\"\"Classify papers based on topic distributions\"\"\"\n",
    "    paper_classifications = []\n",
    "    \n",
    "    for idx, dist in enumerate(topic_distributions): #looping throug the papers one-by-one (index by index) and topics distribution \n",
    "        # Get primary and secondary topics (only primary is used in dominance ratio)\n",
    "        top_2_topics = np.argsort(dist)[-2:][::-1] # extracting the to last [-2:] the order is reversed to highest first with the [::-1]\n",
    "        \n",
    "        # Calculate dominance ratios by :\n",
    "        primary_score = dist[top_2_topics[0]] # 1) extracting the probability of the primary topic\n",
    "        other_topics_sum = sum(dist) - primary_score # 2) calc. the sum of all the other topics (all topic-primary)\n",
    "        dominance_ratio = primary_score / (other_topics_sum + 1e-10) #3) dividing the primary score by the sum (adding very small number to avoid dividing by zero if only one topic)\n",
    "        \n",
    "        # Storing the paper classifications\n",
    "        paper_classifications.append({\n",
    "            'paper_idx': idx,                   #storing the paper index\n",
    "            'primary_topic': top_2_topics[0],   # the most probable topic\n",
    "            'secondary_topic': top_2_topics[1], # second most probable topic\n",
    "            'primary_score': primary_score,     # the probability of the most probable topic \n",
    "            'dominance_ratio': dominance_ratio  # the domincane ratio (eg. is the paper only about this, or also covering other topics?)\n",
    "        })\n",
    "    \n",
    "    return paper_classifications\n",
    "\n",
    "# Get top papers per topic per field\n",
    "\n",
    "def get_top_papers(paper_classifications, df_field, n_top=5):\n",
    "    top_papers = {}\n",
    "    author_topic_stats = {}\n",
    "    \n",
    "    # Debug print\n",
    "    #print(f\"Total papers to analyze: {len(paper_classifications)}\")\n",
    "    \n",
    "    for topic in set(p['primary_topic'] for p in paper_classifications): # creating a set of the unique primary topics in the datasett, e.g. if there are only three primary topics and all papers \"belong\" to one of them, the set will be {Primary topic 1, Primary topic 2, Primary topic 3}\n",
    "        print(f\"\\nProcessing topic {topic}\")\n",
    "        topic_papers = [p for p in paper_classifications if p['primary_topic'] == topic]  # Get all the papers for the current topic\n",
    "        print(f\"Papers for topic {topic}: {len(topic_papers)}\")\n",
    "        topic_papers.sort(key=lambda x: x['dominance_ratio'], reverse=True) # Sort by dominance ratio, reverse=True to get the highest ratios at the top.\n",
    "        top_papers[topic] = []\n",
    "        \n",
    "        # Get top n papers where n is the numebr spesified as top_n when calling the functino\n",
    "        for p in topic_papers[:n_top]: #loop through the top_n number of papers\n",
    "            paper_idx = p['paper_idx']\n",
    "            authors = df_field.iloc[paper_idx]['authors']\n",
    "            \n",
    "            # Debug print\n",
    "            #print(f\"\\nPaper index: {paper_idx}\")\n",
    "            #print(f\"Authors data type: {type(authors)}\")\n",
    "            #print(f\"Authors content: {authors}\")\n",
    "            \n",
    "            # Check if authors is string (might be stored as JSON string)\n",
    "            if isinstance(authors, str):\n",
    "                try:\n",
    "                    authors = ast.literal_eval(authors)\n",
    "                except (ValueError, SyntaxError):\n",
    "                    print(f\"Failed to parse authors string: {authors}\")\n",
    "                    authors = []\n",
    "            \n",
    "            if isinstance(authors, list):\n",
    "                author_list = []\n",
    "                for author in authors:\n",
    "                    # Debug print\n",
    "                    print(f\"Processing author: {author}\")\n",
    "                    if isinstance(author, dict):\n",
    "                        author_list.append({\n",
    "                            'name': author.get('name', 'Unknown'),\n",
    "                            'id': author.get('authorId', 'Unknown')\n",
    "                        })\n",
    "                    else:\n",
    "                        print(f\"Unexpected author format: {author}\")\n",
    "            else:\n",
    "                print(f\"Unexpected authors format: {authors}\")\n",
    "                author_list = []\n",
    "            \n",
    "            # Debug print\n",
    "            #print(f\"Processed author list: {author_list}\")\n",
    "            \n",
    "            top_papers[topic].append({\n",
    "                'paperId': df_field.iloc[paper_idx]['paperId'],\n",
    "                'title': df_field.iloc[paper_idx]['title'],\n",
    "                'abstract': df_field.iloc[paper_idx]['abstract'],\n",
    "                'authors': author_list,\n",
    "                'score': float(p['primary_score']),\n",
    "                'dominance_ratio': float(p['dominance_ratio'])\n",
    "            })\n",
    "            \n",
    "            for author in author_list:\n",
    "                author_id = author['id']\n",
    "                if author_id not in author_topic_stats:\n",
    "                    author_topic_stats[author_id] = {\n",
    "                        'name': author['name'],\n",
    "                        'topics': {},\n",
    "                        'total_papers': 0,\n",
    "                        'top_papers': 0\n",
    "                    }\n",
    "                \n",
    "                if topic not in author_topic_stats[author_id]['topics']:\n",
    "                    author_topic_stats[author_id]['topics'][topic] = {\n",
    "                        'paper_count': 0,\n",
    "                        'avg_dominance': 0,\n",
    "                        'top_papers': []\n",
    "                    }\n",
    "                \n",
    "                author_stats = author_topic_stats[author_id]['topics'][topic]\n",
    "                author_stats['paper_count'] += 1\n",
    "                author_stats['avg_dominance'] = (\n",
    "                    (author_stats['avg_dominance'] * (author_stats['paper_count'] - 1) + \n",
    "                     float(p['dominance_ratio'])) / author_stats['paper_count']\n",
    "                )\n",
    "                author_stats['top_papers'].append({\n",
    "                    'title': df_field.iloc[paper_idx]['title'],\n",
    "                    'dominance_ratio': float(p['dominance_ratio'])\n",
    "                })\n",
    "                \n",
    "                author_topic_stats[author_id]['total_papers'] += 1\n",
    "                author_topic_stats[author_id]['top_papers'] += 1\n",
    "    \n",
    "      # Convert topic numbers to regular integers for JSON serialization\n",
    "    author_topic_stats_clean = {}\n",
    "    for author_id, stats in author_topic_stats.items():\n",
    "        author_topic_stats_clean[author_id] = stats.copy()\n",
    "        author_topic_stats_clean[author_id]['topics'] = {\n",
    "            int(topic): topic_stats \n",
    "            for topic, topic_stats in stats['topics'].items()\n",
    "        }\n",
    "    \n",
    "    # Debug print with cleaned data\n",
    "    #print(f\"\\nFinal author_topic_stats: {json.dumps(author_topic_stats_clean, indent=2)}\")\n",
    "    \n",
    "    return top_papers, author_topic_stats\n",
    "\n",
    "\n",
    "def print_author_analysis(author_topic_stats, min_papers=2):\n",
    "    \"\"\"Print detailed author analysis\"\"\"\n",
    "    print(\"\\nAuthor Analysis:\")\n",
    "    for author_id, stats in author_topic_stats.items():\n",
    "        if stats['total_papers'] >= min_papers:\n",
    "            print(f\"\\nAuthor: {stats['name']}\")\n",
    "            print(f\"Total papers in top lists: {stats['total_papers']}\")\n",
    "            print(\"Topics:\")\n",
    "            for topic, topic_stats in stats['topics'].items():\n",
    "                print(f\"\\nTopic {topic}:\")\n",
    "                print(f\"  Paper count: {topic_stats['paper_count']}\")\n",
    "                print(f\"  Average dominance ratio: {topic_stats['avg_dominance']:.4f}\")\n",
    "                print(\"  Top papers:\")\n",
    "                for paper in topic_stats['top_papers']:\n",
    "                    print(f\"    - {paper['title']} (dominance: {paper['dominance_ratio']:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" needs fixing\"\"\"\n",
    "\n",
    "def analyze_specific_topic(df, topic, num_subtopics=5, n_top=5):\n",
    "    # Filter the dataframe for the specific topic\n",
    "    df_topic = df[df['Primary_Topic'] == topic].copy()\n",
    "    \n",
    "    print(f\"Analyzing topic: {topic}\")\n",
    "    print(f\"Number of papers: {len(df_topic)}\")\n",
    "    \n",
    "    if len(df_topic) < 10:  # Adjust this threshold as needed\n",
    "        print(\"Not enough papers for meaningful subtopic analysis.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Ensure index is unique\n",
    "    df_topic = df_topic.reset_index(drop=True)\n",
    "    \n",
    "    # Prepare the text data\n",
    "    text_data = df_topic['abstract'].fillna('')\n",
    "    \n",
    "    # Create document-term matrix\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    doc_term_matrix = vectorizer.fit_transform(text_data)\n",
    "    \n",
    "    # Create and fit LDA model for subtopics\n",
    "    lda_model = LatentDirichletAllocation(n_components=num_subtopics, random_state=42)\n",
    "    subtopic_distributions = lda_model.fit_transform(doc_term_matrix)\n",
    "    \n",
    "    # Get top words for each subtopic\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    for idx, subtopic in enumerate(lda_model.components_):\n",
    "        top_words = [feature_names[i] for i in subtopic.argsort()[:-10 - 1:-1]]\n",
    "        print(f\"Subtopic {idx + 1}: {', '.join(top_words)}\")\n",
    "    \n",
    "    # Get top papers for each subtopic\n",
    "    top_papers = {}\n",
    "    for subtopic in range(num_subtopics):\n",
    "        subtopic_scores = subtopic_distributions[:, subtopic]\n",
    "        other_subtopics_sum = subtopic_distributions.sum(axis=1) - subtopic_scores\n",
    "        subtopic_dominance = subtopic_scores / (other_subtopics_sum + 1e-10)\n",
    "        top_indices = np.argsort(subtopic_dominance)[-n_top:][::-1]\n",
    "        \n",
    "        top_papers[subtopic] = [\n",
    "            {\n",
    "                'title': df_topic.iloc[i]['title'],\n",
    "                'abstract': df_topic.iloc[i]['abstract'],\n",
    "                'score': float(subtopic_scores[i]),\n",
    "                'dominance_ratio': float(subtopic_dominance[i])\n",
    "            }\n",
    "            for i in top_indices\n",
    "        ]\n",
    "    # Generate subtopic names using the same method as before - Skipped until copyright breach issue is resolved\n",
    "    \"\"\"\n",
    "    subtopic_names = {}\n",
    "    for subtopic, papers in top_papers.items():\n",
    "        abstracts = \"\\n\\n\".join([paper['abstract'] for paper in papers])\n",
    "        keywords = [word for paper in papers for word in paper['abstract'].split()[:10]]\n",
    "        subtopic_name = generate_topic_name(abstracts, keywords)\n",
    "        subtopic_names[subtopic] = subtopic_name\n",
    "    \"\"\"\n",
    "    #Add subtopic scores and names to the dataframe\n",
    "    for i in range(num_subtopics):\n",
    "        df_topic[f'Subtopic_{i+1}_Score'] = subtopic_distributions[:, i]\n",
    "        df_topic['Primary_Subtopic'] = df_topic[[f'Subtopic_{i+1}_Score' for i in range(num_subtopics)]].idxmax(axis=1)\n",
    "   # df_topic['Primary_Subtopic_Name'] = df_topic['Primary_Subtopic'].map(lambda x: subtopic_names[int(x.split('_')[1]) - 1])\n",
    "    \n",
    "    # Update the main dataframe with the new subtopic information\n",
    "        df_update = df.copy()\n",
    "        df_update.loc[df_topic.index, [f'Subtopic_{i+1}_Score' for i in range(num_subtopics)]] = df_topic[[f'Subtopic_{i+1}_Score' for i in range(num_subtopics)]]\n",
    "        df_update.loc[df_topic.index, 'Primary_Subtopic'] = df_topic['Primary_Subtopic']\n",
    "  # df_update.loc[df_topic.index, 'Primary_Subtopic_Name'] = df_topic['Primary_Subtopic_Name']\n",
    "    \n",
    "    return df_update, top_papers#, subtopic_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename='semantic_scholar_2025_02_14_reliability_resilience_power_systems_results.csv' # if the search key words are changed you need to update the filename here. should be automatic...\n",
    "filepath=os.path.join(SAVE_DIR,filename)\n",
    "df=pd.read_csv(filepath,sep=\";\")\n",
    "df['text'] = df['title'].fillna('') + ' ' + df['abstract'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing text data...\n",
      "Text preprocessing completed.\n"
     ]
    }
   ],
   "source": [
    "# Processing to convert title and abstract to text field.\n",
    "print(\"Preprocessing text data...\")\n",
    "search_keywords=['reliability', 'resilience', 'power system', 'capacity utilization'] # should be automitically retrieved from search script, but for the time beeing its manually input here...\n",
    "df['processed_text'] = df['text'].apply(lambda x:preprocess_text(x,search_keywords=search_keywords))\n",
    "print(\"Text preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preparing for analysis (need to fill the empty fieldsOfStudy with ''):\n",
    "valid_fields= ['Computer Science', 'Economics', 'Engineering', 'Physics', 'Mathematics', 'Medicine', 'Business', 'Environmental Science', 'Chemistry', 'Materials Science', 'Geography', 'Biology', 'Geology', 'Political Science', 'Psychology', 'Com']\n",
    "df['fieldsOfStudy'] = df['fieldsOfStudy'].apply(clean_fields_of_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to Saved_files\\term_frequencies_2025_02_28.json\n"
     ]
    }
   ],
   "source": [
    "# simple analysis before topic modelling\n",
    "# Usage\n",
    "extract_keywords_and_ngrams(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new better version of the analyzis:\n",
    "\"\"\"\n",
    "def analyze_papers_with_topic_names(df, fields_to_analyze,n_papers=5, output_suffix=\"_analyzed_results\"):\n",
    "    # Initialize OpenAI client\n",
    "    client, model_type = initialize_openai()\n",
    "    credit_tracker = CreditTracker()\n",
    "    \n",
    "    # Create a copy of the original dataframe to store all results\n",
    "    df_analyzed = df.copy()\n",
    "    all_author_stats = {}\n",
    "    all_topic_names = {}\n",
    "    \n",
    "    for field in fields_to_analyze:\n",
    "        print(f\"\\nAnalyzing field: {field}\")\n",
    "        \n",
    "        # Step 1: Model topics\n",
    "        lda_model, vectorizer, topic_distributions, df_field, topic_keywords = model_topics(\n",
    "            df, field, num_topics=10)\n",
    "        \n",
    "        if lda_model is None:\n",
    "            continue\n",
    "            \n",
    "        # Step 2: Classify papers and add to main dataframe\n",
    "        paper_classifications = classify_papers(topic_distributions, df_field)\n",
    "        \n",
    "        # Add classifications to the analyzed dataframe\n",
    "        for p in paper_classifications:\n",
    "            idx = df_field.index[p['paper_idx']]\n",
    "            df_analyzed.loc[idx, f'{field}_Primary_Topic'] = p['primary_topic']\n",
    "            df_analyzed.loc[idx, f'{field}_Secondary_Topic'] = p['secondary_topic']\n",
    "            df_analyzed.loc[idx, f'{field}_Primary_Score'] = p['primary_score']\n",
    "            df_analyzed.loc[idx, f'{field}_Dominance_Ratio'] = p['dominance_ratio']\n",
    "        \n",
    "        # Step 3: Get top papers and author stats\n",
    "        top_papers, author_stats = get_top_papers(paper_classifications, df_field, n_top=n_papers)\n",
    "        \n",
    "        # Step 4: Generate topic names\n",
    "        field_topic_names = {}\n",
    "        for topic_idx, papers in top_papers.items():\n",
    "            # Combine abstracts and extract keywords\n",
    "            input_text = \"\\n\\n\".join([str(paper.get('title', '')) for paper in papers])\n",
    "            keywords = topic_keywords[topic_idx]['top_words']\n",
    "            \n",
    "            # Generate topic name\n",
    "            topic_name = generate_topic_name_multiple(input_text, keywords, client, model_type, credit_tracker)\n",
    "            \n",
    "            if topic_name:\n",
    "                field_topic_names[topic_idx] = topic_name\n",
    "                # Add topic name to dataframe\n",
    "                topic_column = f'{field}_Topic_{topic_idx}_Name'\n",
    "                df_analyzed[topic_column] = topic_name\n",
    "                \n",
    "                # Update primary topic name for papers with this primary topic\n",
    "                primary_topic_mask = df_analyzed[f'{field}_Primary_Topic'] == topic_idx\n",
    "                df_analyzed.loc[primary_topic_mask, f'{field}_Primary_Topic_Name'] = topic_name\n",
    "            \n",
    "            # Print topic name and top papers\n",
    "            print(f\"\\nTopic {topic_idx + 1}: {topic_name if topic_name else 'Unnamed'}\")\n",
    "            print(f\"Keywords: {', '.join(topic_keywords[topic_idx]['top_words'])}\")\n",
    "            for paper in papers:\n",
    "                print(f\"- {paper['title']} (dominance: {paper['dominance_ratio']:.4f})\")\n",
    "        \n",
    "        # Store results\n",
    "        all_author_stats[field] = author_stats\n",
    "        all_topic_names[field] = field_topic_names\n",
    "        \n",
    "        # Add topic keywords to dataframe metadata\n",
    "        df_analyzed.attrs[f'{field}_topic_keywords'] = topic_keywords\n",
    "        df_analyzed.attrs[f'{field}_topic_names'] = field_topic_names\n",
    "    \n",
    "    # Save results\n",
    "    current_date = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "    output_filename = os.path.join(SAVE_DIR,f\"semantic_scholar_{current_date}{output_suffix}.csv\")\n",
    "    \n",
    "    # Save main results\n",
    "    df_analyzed.to_csv(output_filename, sep=';', encoding='utf-8', \n",
    "                      quoting=csv.QUOTE_NONNUMERIC, escapechar='\\\\')\n",
    "    \n",
    "    # Save author statistics with topic frequencies\n",
    "    author_filename = os.path.join(SAVE_DIR,f\"semantic_scholar_{current_date}_author_analysis.csv\")\n",
    "    save_author_analysis(all_author_stats, author_filename)\n",
    "    \n",
    "    # Save topic names\n",
    "    converted_topic_names = {}\n",
    "    for field, topics in all_topic_names.items():\n",
    "        converted_topic_names[field] = {\n",
    "        int(topic_idx): name \n",
    "        for topic_idx, name in topics.items()\n",
    "        }\n",
    "\n",
    "    topic_names_filename = os.path.join(SAVE_DIR,f\"semantic_scholar_{current_date}_topic_names.json\")\n",
    "    with open(topic_names_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_topic_names, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Print credit usage\n",
    "    print(\"\\nAPI Usage Statistics:\")\n",
    "    stats = credit_tracker.get_stats()\n",
    "    print(f\"Total tokens: {stats['total_tokens']}\")\n",
    "    print(f\"Estimated cost: ${stats['total_cost']}\")\n",
    "    \n",
    "    return df_analyzed, all_author_stats, all_topic_names\n",
    "\"\"\"\n",
    "def analyze_papers_with_topic_names(df, fields_to_analyze=None, n_papers=5, output_suffix=\"_analyzed_results\"):\n",
    "    \"\"\"\n",
    "    Analyze papers with topic naming\n",
    "    \n",
    "    Parameters:\n",
    "    df - DataFrame containing papers\n",
    "    fields_to_analyze - List of fields to filter papers by (if None, use all papers)\n",
    "    n_papers - Number of top papers to extract per topic\n",
    "    output_suffix - Suffix for output files\n",
    "    \"\"\"\n",
    "    # Initialize OpenAI client\n",
    "    client, model_type = initialize_openai()\n",
    "    credit_tracker = CreditTracker()\n",
    "    \n",
    "    # Create a copy of the original dataframe to store all results\n",
    "    df_analyzed = df.copy()\n",
    "    all_author_stats = {}\n",
    "    all_topic_names = {}\n",
    "    \n",
    "    # Filter dataset by fields if specified\n",
    "    if fields_to_analyze:\n",
    "        df_filtered = df[df['fieldsOfStudy'].apply(lambda x: any(field in x for field in fields_to_analyze))]\n",
    "        print(f\"Filtered to {len(df_filtered)} papers from fields: {', '.join(fields_to_analyze)}\")\n",
    "    else:\n",
    "        df_filtered = df\n",
    "        print(f\"Using all {len(df_filtered)} papers for analysis\")\n",
    "    \n",
    "    if df_filtered.empty:\n",
    "        print(\"No papers found after filtering\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Step 1: Model topics on the filtered dataset\n",
    "    lda_model, vectorizer, topic_distributions, df_field, topic_keywords = model_topics(\n",
    "        df_filtered, num_topics=10)\n",
    "    \n",
    "    if lda_model is None:\n",
    "        return None, None, None\n",
    "        \n",
    "    # Step 2: Classify papers and add to main dataframe\n",
    "    paper_classifications = classify_papers(topic_distributions, df_field)\n",
    "    \n",
    "    # Step 3: Generate topic names first, so we can use them in column names\n",
    "    top_papers, author_stats = get_top_papers(paper_classifications, df_field, n_top=n_papers)\n",
    "    \n",
    "    # Generate topic names\n",
    "    topic_names = {}\n",
    "    for topic_idx, papers in top_papers.items():\n",
    "        # Combine titles and extract keywords\n",
    "        input_text = \"\\n\\n\".join([str(paper.get('title', '')) for paper in papers])\n",
    "        keywords = topic_keywords[topic_idx]['top_words']\n",
    "        \n",
    "        # Generate topic name\n",
    "        topic_name = generate_topic_name_multiple(input_text, keywords, client, model_type, credit_tracker)\n",
    "        \n",
    "        if topic_name:\n",
    "            topic_names[topic_idx] = topic_name\n",
    "            \n",
    "        # Print topic name and top papers\n",
    "        print(f\"\\nTopic {topic_idx + 1}: {topic_name if topic_name else 'Unnamed'}\")\n",
    "        print(f\"Keywords: {', '.join(topic_keywords[topic_idx]['top_words'])}\")\n",
    "        for paper in papers:\n",
    "            print(f\"- {paper['title']} (dominance: {paper['dominance_ratio']:.4f})\")\n",
    "    \n",
    "    # Step 4: Add classifications to the analyzed dataframe using topic names\n",
    "    for p in paper_classifications:\n",
    "        idx = df_field.index[p['paper_idx']]\n",
    "        primary_topic_idx = p['primary_topic']\n",
    "        secondary_topic_idx = p['secondary_topic']\n",
    "        \n",
    "        # Store topic indices for reference\n",
    "        df_analyzed.loc[idx, 'Primary_Topic_Index'] = primary_topic_idx\n",
    "        df_analyzed.loc[idx, 'Secondary_Topic_Index'] = secondary_topic_idx\n",
    "        \n",
    "        # Store topic names as primary columns\n",
    "        primary_name = topic_names.get(primary_topic_idx, f\"Topic_{primary_topic_idx}\")\n",
    "        secondary_name = topic_names.get(secondary_topic_idx, f\"Topic_{secondary_topic_idx}\")\n",
    "        \n",
    "        df_analyzed.loc[idx, 'Primary_Topic'] = primary_name\n",
    "        df_analyzed.loc[idx, 'Secondary_Topic'] = secondary_name\n",
    "        df_analyzed.loc[idx, 'Primary_Score'] = p['primary_score']\n",
    "        df_analyzed.loc[idx, 'Dominance_Ratio'] = p['dominance_ratio']\n",
    "    \n",
    "    # Store results\n",
    "    all_author_stats['all'] = author_stats\n",
    "    all_topic_names['all'] = topic_names\n",
    "    \n",
    "    # Add topic keywords to dataframe metadata\n",
    "    df_analyzed.attrs['topic_keywords'] = topic_keywords\n",
    "    df_analyzed.attrs['topic_names'] = topic_names\n",
    "    \n",
    "    # Save results\n",
    "    current_date = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "    output_filename = os.path.join(SAVE_DIR, f\"semantic_scholar_{current_date}{output_suffix}.csv\")\n",
    "    \n",
    "    # Save main results\n",
    "    df_analyzed.to_csv(output_filename, sep=';', encoding='utf-8', \n",
    "                      quoting=csv.QUOTE_NONNUMERIC, escapechar='\\\\')\n",
    "    \n",
    "    # Save author statistics with topic frequencies\n",
    "    author_filename = os.path.join(SAVE_DIR, f\"semantic_scholar_{current_date}_author_analysis.csv\")\n",
    "    save_author_analysis(all_author_stats, author_filename)\n",
    "    \n",
    "    # Save topic names\n",
    "    converted_topic_names = {}\n",
    "    for field, topics in all_topic_names.items():\n",
    "        converted_topic_names[field] = {\n",
    "            int(topic_idx): name \n",
    "            for topic_idx, name in topics.items()\n",
    "        }\n",
    "\n",
    "    topic_names_filename = os.path.join(SAVE_DIR, f\"semantic_scholar_{current_date}_topic_names.json\")\n",
    "    with open(topic_names_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(converted_topic_names, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Print credit usage\n",
    "    print(\"\\nAPI Usage Statistics:\")\n",
    "    stats = credit_tracker.get_stats()\n",
    "    print(f\"Total tokens: {stats['total_tokens']}\")\n",
    "    print(f\"Estimated cost: ${stats['total_cost']}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def save_author_analysis(all_author_stats, filename):\n",
    "    \"\"\"Save detailed author analysis including topic frequencies\"\"\"\n",
    "    author_rows = []\n",
    "    \n",
    "    for field, author_stats in all_author_stats.items():\n",
    "        for author_id, stats in author_stats.items():\n",
    "            for topic, topic_stats in stats['topics'].items():\n",
    "                author_rows.append({\n",
    "                    'Field': field,\n",
    "                    'Author_ID': author_id,\n",
    "                    'Author_Name': stats['name'],\n",
    "                    'Topic': topic,\n",
    "                    'Paper_Count': topic_stats['paper_count'],\n",
    "                    'Avg_Dominance': topic_stats['avg_dominance'],\n",
    "                    'Total_Papers': stats['total_papers']\n",
    "                })\n",
    "    \n",
    "    author_df = pd.DataFrame(author_rows)\n",
    "    author_df.to_csv(filename, sep=';', encoding='utf-8', \n",
    "                    quoting=csv.QUOTE_NONNUMERIC, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered to 16939 papers from fields: Computer Science, Economics, Engineering, Physics, Mathematics\n",
      "Analyzing 16939 papers across all fields\n",
      "\n",
      "Topic 1:\n",
      "power (8399.84), high (3621.12), voltage (3567.04), control (3530.43), current (3223.15), dc (2746.68), circuit (1964.44), converter (1857.09), reliability (1823.41), phase (1489.17)\n",
      "\n",
      "Topic 2:\n",
      "data (5036.40), systems (3021.98), performance (2468.24), based (2304.72), power (1967.72), reliability (1920.28), computing (1891.35), time (1872.24), memory (1780.22), applications (1666.71)\n",
      "\n",
      "Topic 3:\n",
      "energy (2607.99), systems (2480.62), power (2312.30), gas (1783.11), water (1762.72), heat (1735.19), plant (1578.83), development (1502.95), design (1453.81), technology (1452.37)\n",
      "\n",
      "Topic 4:\n",
      "power (7846.12), network (3354.04), distribution (3292.57), reliability (2673.37), transmission (2565.92), grid (2326.40), fault (2175.20), systems (1710.60), line (1541.14), paper (1528.06)\n",
      "\n",
      "Topic 5:\n",
      "battery (2894.33), charging (1602.86), vehicle (1474.09), electric (1318.45), vehicles (1284.69), batteries (959.36), energy (924.55), power (749.52), capacity (628.97), ev (572.10)\n",
      "\n",
      "Topic 6:\n",
      "power (4016.57), communication (3466.83), capacity (3388.91), channel (3099.05), wireless (2959.49), network (2740.55), performance (2593.55), proposed (2276.66), transmission (2151.20), systems (2113.99)\n",
      "\n",
      "Topic 7:\n",
      "temperature (862.06), design (714.41), la (654.10), thermal (511.54), cooling (498.74), en (477.38), performance (437.82), engine (417.73), test (411.13), high (405.37)\n",
      "\n",
      "Topic 8:\n",
      "power (6400.03), reliability (6200.21), model (3991.03), proposed (3360.08), capacity (3224.95), method (3088.42), load (2690.95), paper (2602.75), based (2452.64), optimization (2099.28)\n",
      "\n",
      "Topic 9:\n",
      "power (15170.49), energy (13158.55), wind (7729.95), grid (5000.35), storage (4343.71), generation (3948.32), control (3689.22), renewable (3559.58), pv (3409.17), capacity (3333.29)\n",
      "\n",
      "Topic 10:\n",
      "control (506.25), used (355.07), yang (350.92), power (337.78), speed (323.96), dan (323.10), wave (314.57), signal (302.10), using (283.27), data (265.13)\n",
      "\n",
      "Processing topic 0\n",
      "Papers for topic 0: 1689\n",
      "Processing author: {'authorId': '73206968', 'name': '袁其平'}\n",
      "Processing author: {'authorId': '72053442', 'name': '李称鑫'}\n",
      "Processing author: {'authorId': '2284267', 'name': 'D. Divan'}\n",
      "Processing author: {'authorId': '2060111055', 'name': '刘钊'}\n",
      "Processing author: {'authorId': '101590820', 'name': '刘从臻'}\n",
      "Processing author: {'authorId': '70452096', 'name': '张学义'}\n",
      "Processing author: {'authorId': '102077853', 'name': '徐进彬'}\n",
      "Processing author: {'authorId': '101120825', 'name': '简国荣'}\n",
      "Processing author: {'authorId': '2056731411', 'name': '陈旭'}\n",
      "Processing author: {'authorId': '101588330', 'name': '寿卫林'}\n",
      "Processing author: {'authorId': '101497862', 'name': '戴建卫'}\n",
      "Processing author: {'authorId': '71761654', 'name': '吴安'}\n",
      "Processing author: {'authorId': '81861726', 'name': '丁金龙'}\n",
      "Processing author: {'authorId': '2330719', 'name': 'Qing Hua'}\n",
      "Processing author: {'authorId': '9248029', 'name': 'Zehong Li'}\n",
      "Processing author: {'authorId': '91170927', 'name': 'Bo Zhang'}\n",
      "Processing author: {'authorId': '1994437985', 'name': 'Weizhong Chen'}\n",
      "Processing author: {'authorId': '2117000090', 'name': 'Xiangjun Huang'}\n",
      "Processing author: {'authorId': '2053697203', 'name': 'Dekai Cheng'}\n",
      "Processing author: {'authorId': '114209238', 'name': '赵井福'}\n",
      "Processing author: {'authorId': '9354330', 'name': 'Gao Hanying'}\n",
      "Processing author: {'authorId': '31171005', 'name': 'Zhang Momo'}\n",
      "Processing author: {'authorId': '144824305', 'name': 'Chen Kai'}\n",
      "Processing author: {'authorId': '9290521', 'name': 'Z. Tianyu'}\n",
      "\n",
      "Processing topic 1\n",
      "Papers for topic 1: 2259\n",
      "Processing author: {'authorId': '2267947312', 'name': 'Sheharyar Khan'}\n",
      "Processing author: {'authorId': '2237499037', 'name': 'Jiangbin Zheng'}\n",
      "Processing author: {'authorId': '2293974833', 'name': 'Farhan Ullah'}\n",
      "Processing author: {'authorId': '69936718', 'name': 'Muhammad Pervez Akhter'}\n",
      "Processing author: {'authorId': '2163370113', 'name': 'Sohrab Khan'}\n",
      "Processing author: {'authorId': '2238690322', 'name': 'Fuad A. Awwad'}\n",
      "Processing author: {'authorId': '2209142337', 'name': 'Emad A. A. Ismail'}\n",
      "Processing author: {'authorId': '2357662', 'name': 'Saurabh Hukerikar'}\n",
      "Processing author: {'authorId': '1686571', 'name': 'C. Engelmann'}\n",
      "Processing author: {'authorId': '3028254', 'name': 'Vahid Lari'}\n",
      "Processing author: {'authorId': '144570963', 'name': 'Alexandru Tanase'}\n",
      "Processing author: {'authorId': '1697402', 'name': 'Frank Hannig'}\n",
      "Processing author: {'authorId': '144365535', 'name': 'J. Teich'}\n",
      "Processing author: {'authorId': '145315390', 'name': 'M. Fallah'}\n",
      "Processing author: {'authorId': '3091989', 'name': 'Mostafa Ghobaei Arani'}\n",
      "Processing author: {'authorId': '1617598635', 'name': 'Yehonatan Fridman'}\n",
      "Processing author: {'authorId': '2125722635', 'name': 'Yaniv Snir'}\n",
      "Processing author: {'authorId': '14354063', 'name': 'Harel Levin'}\n",
      "Processing author: {'authorId': '1718098', 'name': 'Danny Hendler'}\n",
      "Processing author: {'authorId': '144142235', 'name': 'H. Attiya'}\n",
      "Processing author: {'authorId': '151504666', 'name': 'Gal Oren'}\n",
      "Processing author: {'authorId': '50811700', 'name': 'Anant V. Nori'}\n",
      "Processing author: {'authorId': '2067157527', 'name': 'Rahul Bera'}\n",
      "Processing author: {'authorId': '1703505', 'name': 'S. Balachandran'}\n",
      "Processing author: {'authorId': '39213970', 'name': 'Joydeep Rakshit'}\n",
      "Processing author: {'authorId': '123918823', 'name': 'O. J. Omer'}\n",
      "Processing author: {'authorId': '2028358086', 'name': 'Avishaii Abuhatzera'}\n",
      "Processing author: {'authorId': '119771774', 'name': 'B. Kuttanna'}\n",
      "Processing author: {'authorId': '1706165', 'name': 'S. Subramoney'}\n",
      "Processing author: {'authorId': '50811700', 'name': 'Anant V. Nori'}\n",
      "Processing author: {'authorId': '2067157527', 'name': 'Rahul Bera'}\n",
      "Processing author: {'authorId': '1703505', 'name': 'S. Balachandran'}\n",
      "Processing author: {'authorId': '39213970', 'name': 'Joydeep Rakshit'}\n",
      "Processing author: {'authorId': '123918823', 'name': 'O. J. Omer'}\n",
      "Processing author: {'authorId': '2028358086', 'name': 'Avishaii Abuhatzera'}\n",
      "Processing author: {'authorId': '119771774', 'name': 'B. Kuttanna'}\n",
      "Processing author: {'authorId': '1706165', 'name': 'S. Subramoney'}\n",
      "Processing author: {'authorId': '1778215', 'name': 'Swagath Venkataramani'}\n",
      "Processing author: {'authorId': '145941003', 'name': 'Vijayalakshmi Srinivasan'}\n",
      "Processing author: {'authorId': '2506452', 'name': 'Jungwook Choi'}\n",
      "Processing author: {'authorId': '1750961', 'name': 'P. Heidelberger'}\n",
      "Processing author: {'authorId': '39613278', 'name': 'Leland Chang'}\n",
      "Processing author: {'authorId': '33678523', 'name': 'K. Gopalakrishnan'}\n",
      "Processing author: {'authorId': '35170994', 'name': 'S. Sood'}\n",
      "Processing author: {'authorId': '48951726', 'name': 'Bingwei Chen'}\n",
      "Processing author: {'authorId': '2028215214', 'name': 'Jianquan Li'}\n",
      "Processing author: {'authorId': '2812506', 'name': 'Bozhong Liu'}\n",
      "Processing author: {'authorId': '145202330', 'name': 'Zhibin Yu'}\n",
      "\n",
      "Processing topic 2\n",
      "Papers for topic 2: 1793\n",
      "Processing author: {'authorId': '95384270', 'name': 'B. Lonia'}\n",
      "Processing author: {'authorId': '38596084', 'name': 'N. Nayar'}\n",
      "Processing author: {'authorId': '1948654', 'name': 'S. Singh'}\n",
      "Processing author: {'authorId': '2071074384', 'name': 'P. Bali'}\n",
      "Processing author: {'authorId': '47676258', 'name': 'M. Huberman'}\n",
      "Processing author: {'authorId': '46838102', 'name': 'Pallav Purohit'}\n",
      "Processing author: {'authorId': '97784887', 'name': 'Ghezel-Ayagh Hossein'}\n",
      "Processing author: {'authorId': '92507425', 'name': 'D. Bonilla'}\n",
      "Processing author: {'authorId': '49692668', 'name': 'C. Whittaker'}\n",
      "Processing author: {'authorId': '100516317', 'name': 'E. Wikramanayake'}\n",
      "Processing author: {'authorId': '2495867', 'name': 'V. Bahadur'}\n",
      "Processing author: {'authorId': '150916825', 'name': 'A. Petrov'}\n",
      "Processing author: {'authorId': '92525076', 'name': 'A. V. Shutikov'}\n",
      "Processing author: {'authorId': '2113292817', 'name': 'N. N. Ponomarev-Stepnoy'}\n",
      "Processing author: {'authorId': '151243280', 'name': 'V. S. Bezzubtsev'}\n",
      "Processing author: {'authorId': '81700185', 'name': 'M. V. Bakanov'}\n",
      "Processing author: {'authorId': '2113169738', 'name': 'V. Troyanov'}\n",
      "Processing author: {'authorId': '69577987', 'name': 'Syamsul Ma’arif'}\n",
      "Processing author: {'authorId': '101224386', 'name': 'Rena Juwita Sari'}\n",
      "Processing author: {'authorId': '152915119', 'name': 'Moch. Syamsiro'}\n",
      "Processing author: {'authorId': '101681674', 'name': 'A. D. Pranadi'}\n",
      "Processing author: {'authorId': '119181963', 'name': 'Sihana'}\n",
      "Processing author: {'authorId': '101576555', 'name': 'Kutut Suryopratomo'}\n",
      "Processing author: {'authorId': '100658332', 'name': 'F. R. Salis'}\n",
      "Processing author: {'authorId': '81510888', 'name': 'J. Agnew'}\n",
      "Processing author: {'authorId': '153673131', 'name': 'P. Lung'}\n",
      "Processing author: {'authorId': '2080347368', 'name': 'B. Lung'}\n",
      "\n",
      "Processing topic 3\n",
      "Papers for topic 3: 1645\n",
      "Processing author: {'authorId': '2392589', 'name': 'Qian Suxiang'}\n",
      "Processing author: {'authorId': '3032018', 'name': 'Hu Hongsheng'}\n",
      "Processing author: {'authorId': '2059357859', 'name': 'Cao Jian'}\n",
      "Processing author: {'authorId': '9295542', 'name': 'Yan Gong-biao'}\n",
      "Processing author: {'authorId': '10774411', 'name': 'R. Klyuev'}\n",
      "Processing author: {'authorId': '10750159', 'name': 'I. Bosikov'}\n",
      "Processing author: {'authorId': '80008008', 'name': 'O. Gavrina'}\n",
      "Processing author: {'authorId': '31073972', 'name': 'A. Karpatchev'}\n",
      "Processing author: {'authorId': '152786902', 'name': 'G. Andersson'}\n",
      "Processing author: {'authorId': '9343232', 'name': 'H. Glavitsch'}\n",
      "Processing author: {'authorId': '7238155', 'name': 'Yong-sun Cho'}\n",
      "Processing author: {'authorId': '31400225', 'name': 'Hyosang Choi'}\n",
      "Processing author: {'authorId': '2083077', 'name': 'B. Jung'}\n",
      "Processing author: {'authorId': '82697467', 'name': '王韶'}\n",
      "Processing author: {'authorId': '117448740', 'name': '董光德'}\n",
      "Processing author: {'authorId': '113201292', 'name': '刘沛铮'}\n",
      "Processing author: {'authorId': '115696896', 'name': '张煜成'}\n",
      "Processing author: {'authorId': '118238859', 'name': '朱姜峰'}\n",
      "Processing author: {'authorId': '113820468', 'name': '江卓翰'}\n",
      "Processing author: {'authorId': '2053497642', 'name': '王洋'}\n",
      "Processing author: {'authorId': '101939765', 'name': '恭秀芬'}\n",
      "Processing author: {'authorId': '2144343154', 'name': 'Hong Li'}\n",
      "Processing author: {'authorId': '2158505972', 'name': 'Wei Wang'}\n",
      "Processing author: {'authorId': '2143717062', 'name': 'Jin Xing Wang'}\n",
      "Processing author: {'authorId': '98828216', 'name': 'Shu Wang'}\n",
      "Processing author: {'authorId': '2150203586', 'name': 'Li Hong'}\n",
      "Processing author: {'authorId': '19294788', 'name': 'P. P. Mawle'}\n",
      "Processing author: {'authorId': '9155837', 'name': 'G. Dhomane'}\n",
      "Processing author: {'authorId': '2325569804', 'name': 'P. G. Burade'}\n",
      "Processing author: {'authorId': '66769771', 'name': 'O. Ostapchuk'}\n",
      "Processing author: {'authorId': '148349485', 'name': 'Włodzimierz Kruczek'}\n",
      "Processing author: {'authorId': '49990158', 'name': 'V. Kuznetsov'}\n",
      "Processing author: {'authorId': '153551954', 'name': 'Vitaliy Kuznetsov'}\n",
      "Processing author: {'authorId': '97001203', 'name': 'D. Tsyplenkov'}\n",
      "Processing author: {'authorId': '2108001398', 'name': 'Xueqin Zhang'}\n",
      "Processing author: {'authorId': '2137380031', 'name': 'Xudong Wang'}\n",
      "Processing author: {'authorId': '2147249284', 'name': 'J. Duan'}\n",
      "Processing author: {'authorId': '2144831119', 'name': 'Xiaojun Sun'}\n",
      "Processing author: {'authorId': '2133693973', 'name': 'Guohua Zhang'}\n",
      "Processing author: {'authorId': None, 'name': 'Bo Zhang'}\n",
      "Processing author: {'authorId': '144332269', 'name': 'K. R. Jones'}\n",
      "\n",
      "Processing topic 4\n",
      "Papers for topic 4: 423\n",
      "Processing author: {'authorId': '52026236', 'name': '차지훈'}\n",
      "Processing author: {'authorId': '66326857', 'name': '박용성'}\n",
      "Processing author: {'authorId': '82138694', 'name': '배중호'}\n",
      "Processing author: {'authorId': '71062644', 'name': '이광범'}\n",
      "Processing author: {'authorId': '66386274', 'name': '정혁'}\n",
      "Processing author: {'authorId': '3461144', 'name': 'Fabio Antonio V. Pinto'}\n",
      "Processing author: {'authorId': '1766913', 'name': 'L. Costa'}\n",
      "Processing author: {'authorId': '2132320', 'name': 'M. Amorim'}\n",
      "Processing author: {'authorId': '145836227', 'name': 'Liang He'}\n",
      "Processing author: {'authorId': '3254296', 'name': 'L. Kong'}\n",
      "Processing author: {'authorId': '3117803', 'name': 'Siyu Lin'}\n",
      "Processing author: {'authorId': '1939801', 'name': 'Shaodong Ying'}\n",
      "Processing author: {'authorId': '2112677670', 'name': 'Y. Gu'}\n",
      "Processing author: {'authorId': '1679043', 'name': 'T. He'}\n",
      "Processing author: {'authorId': '21270680', 'name': 'Cong Liu'}\n",
      "Processing author: {'authorId': '2311646161', 'name': 'Ruijun Zhang'}\n",
      "Processing author: {'authorId': '2125225978', 'name': 'Kun Xia'}\n",
      "Processing author: {'authorId': '2111611580', 'name': 'Jianhong Yin'}\n",
      "Processing author: {'authorId': '2118321064', 'name': 'Yiming Zhu'}\n",
      "Processing author: {'authorId': '49993085', 'name': 'Yuchen Song'}\n",
      "Processing author: {'authorId': '7416020', 'name': 'Datong Liu'}\n",
      "Processing author: {'authorId': '145730941', 'name': 'Yu Peng'}\n",
      "Processing author: {'authorId': '153087606', 'name': 'W. Miller'}\n",
      "Processing author: {'authorId': '118076554', 'name': 'T. Kaun'}\n",
      "Processing author: {'authorId': '2115812423', 'name': 'Chi Zhang'}\n",
      "Processing author: {'authorId': '2150928700', 'name': 'Fuwu Yan'}\n",
      "Processing author: {'authorId': '49614986', 'name': 'C. Du'}\n",
      "Processing author: {'authorId': '9319352', 'name': 'Jianqiang Kang'}\n",
      "Processing author: {'authorId': '9314246', 'name': 'Richard Turkson'}\n",
      "Processing author: {'authorId': '2065346865', 'name': 'He Qiang'}\n",
      "Processing author: {'authorId': '2074441997', 'name': 'Feng Na'}\n",
      "Processing author: {'authorId': '2041275315', 'name': 'Dong Qingwei'}\n",
      "Processing author: {'authorId': '2220430697', 'name': 'Dai Duo'}\n",
      "Processing author: {'authorId': '2220430778', 'name': 'Yuan JianYin'}\n",
      "Processing author: {'authorId': '2109270755', 'name': 'Wei-wei Liu'}\n",
      "Processing author: {'authorId': '2117856850', 'name': 'Honglei Li'}\n",
      "Processing author: {'authorId': '49873290', 'name': 'Yelin Deng'}\n",
      "Processing author: {'authorId': '2152891397', 'name': 'Fan Yang'}\n",
      "Processing author: {'authorId': '46566487', 'name': 'Liang Cong'}\n",
      "Processing author: {'authorId': '144373402', 'name': 'M. M. Hoque'}\n",
      "Processing author: {'authorId': '144395103', 'name': 'M. Hannan'}\n",
      "Processing author: {'authorId': '144199975', 'name': 'A. Mohamed'}\n",
      "\n",
      "Processing topic 5\n",
      "Papers for topic 5: 2455\n",
      "Processing author: {'authorId': '1938538', 'name': 'Ali Jemmali'}\n",
      "Processing author: {'authorId': '145495386', 'name': 'J. Conan'}\n",
      "Processing author: {'authorId': '144251547', 'name': 'M. Torabi'}\n",
      "Processing author: {'authorId': '2115410388', 'name': 'Hua Sun'}\n",
      "Processing author: {'authorId': '2622510', 'name': 'A. Aljohani'}\n",
      "Processing author: {'authorId': '143957560', 'name': 'Kun Fang'}\n",
      "Processing author: {'authorId': '2152479471', 'name': 'Fei Xu'}\n",
      "Processing author: {'authorId': '2840748', 'name': 'X. Rui'}\n",
      "Processing author: {'authorId': '2108124297', 'name': 'Yunhan Zhang'}\n",
      "Processing author: {'authorId': '8876251', 'name': 'S. Arzykulov'}\n",
      "Processing author: {'authorId': '145671922', 'name': 'Liang Li'}\n",
      "Processing author: {'authorId': '114849135', 'name': 'Ehsan Haj Mirza Alian Aminabadi'}\n",
      "Processing author: {'authorId': '1857336', 'name': 'Fahimeh Jasbi'}\n",
      "\n",
      "Processing topic 6\n",
      "Papers for topic 6: 367\n",
      "Processing author: {'authorId': '104713904', 'name': 'David Silva Rodríguez'}\n",
      "Processing author: {'authorId': '96929755', 'name': 'Galione Klot'}\n",
      "Processing author: {'authorId': '2074139876', 'name': 'Pedro Andrés'}\n",
      "Processing author: {'authorId': '66420166', 'name': 'Lluís Jofre Cruanyes'}\n",
      "Processing author: {'authorId': '46615280', 'name': 'Mark Benton'}\n",
      "Processing author: {'authorId': '145163631', 'name': 'R. Raju'}\n",
      "Processing author: {'authorId': '40453067', 'name': 'P. Prasad'}\n",
      "Processing author: {'authorId': '145436390', 'name': 'V. Rao'}\n",
      "Processing author: {'authorId': '94956449', 'name': 'A. Ugural'}\n",
      "Processing author: {'authorId': '2790751', 'name': 'P. Love'}\n",
      "Processing author: {'authorId': '102970568', 'name': 'A. Hoffman'}\n",
      "Processing author: {'authorId': '51145041', 'name': 'N. Lum'}\n",
      "Processing author: {'authorId': '134238678', 'name': 'K. J. Ando'}\n",
      "Processing author: {'authorId': '92328505', 'name': 'J. Rosbeck'}\n",
      "Processing author: {'authorId': '102998331', 'name': 'W. Ritchie'}\n",
      "Processing author: {'authorId': '101622845', 'name': 'N. Therrien'}\n",
      "Processing author: {'authorId': '97887979', 'name': 'Roger S. Holcombe'}\n",
      "Processing author: {'authorId': '103130669', 'name': 'E. Corrales'}\n",
      "Processing author: {'authorId': '2790751', 'name': 'P. Love'}\n",
      "Processing author: {'authorId': '102970568', 'name': 'A. Hoffman'}\n",
      "Processing author: {'authorId': '51145041', 'name': 'N. Lum'}\n",
      "Processing author: {'authorId': '134238678', 'name': 'K. J. Ando'}\n",
      "Processing author: {'authorId': '102998331', 'name': 'W. Ritchie'}\n",
      "Processing author: {'authorId': '101622845', 'name': 'N. Therrien'}\n",
      "Processing author: {'authorId': '98718661', 'name': 'Andrew G. Toth'}\n",
      "Processing author: {'authorId': '97887979', 'name': 'Roger S. Holcombe'}\n",
      "Processing author: {'authorId': '2203334', 'name': 'J. Querol'}\n",
      "Processing author: {'authorId': '91417050', 'name': 'V. Mei'}\n",
      "Processing author: {'authorId': '2107179039', 'name': 'F. Chen'}\n",
      "Processing author: {'authorId': '88392248', 'name': 'R. A. Sullivan'}\n",
      "\n",
      "Processing topic 7\n",
      "Papers for topic 7: 2465\n",
      "Processing author: {'authorId': '31044894', 'name': 'Kang Chong-qing'}\n",
      "Processing author: {'authorId': '2113480002', 'name': 'Guo Lin'}\n",
      "Processing author: {'authorId': '2080334793', 'name': 'Bai Lichao'}\n",
      "Processing author: {'authorId': '2074441124', 'name': 'Xu Ruilin'}\n",
      "Processing author: {'authorId': '30728607', 'name': 'He Jianjun'}\n",
      "Processing author: {'authorId': '31159408', 'name': 'Xu Kunyao'}\n",
      "Processing author: {'authorId': '49053398', 'name': 'E. Muneender'}\n",
      "Processing author: {'authorId': '2107296015', 'name': 'D. Kumar'}\n",
      "Processing author: {'authorId': '2299453378', 'name': 'B. M. Alshammari'}\n",
      "Processing author: {'authorId': '1409053287', 'name': 'M. El-Kady'}\n",
      "Processing author: {'authorId': '40272209', 'name': 'A. Chowdhury'}\n",
      "Processing author: {'authorId': '32556024', 'name': 'Rahmat Azami'}\n",
      "Processing author: {'authorId': '32787002', 'name': 'A. H. Abbasi'}\n",
      "Processing author: {'authorId': '46676741', 'name': 'J. Shakeri'}\n",
      "Processing author: {'authorId': '30846899', 'name': 'Amir Faraji Fard'}\n",
      "Processing author: {'authorId': '3228072', 'name': 'R. Aazami'}\n",
      "Processing author: {'authorId': '70241580', 'name': 'Nosratallah Mohammadbeigi'}\n",
      "Processing author: {'authorId': '15017829', 'name': 'Hadis Mirzaei'}\n",
      "Processing author: {'authorId': '2663482', 'name': 'A. Mansouri'}\n",
      "Processing author: {'authorId': '70136786', 'name': 'Ehsan Mohamadian'}\n",
      "Processing author: {'authorId': '2423133', 'name': 'Dange Huang'}\n",
      "Processing author: {'authorId': '2285852285', 'name': 'Kaliyamoorthy Suresh'}\n",
      "Processing author: {'authorId': '2387093', 'name': 'N. Kumarappan'}\n",
      "Processing author: {'authorId': '50233124', 'name': 'W. Haider'}\n",
      "Processing author: {'authorId': '2239541170', 'name': 'Jarjees Ul Hassan'}\n",
      "Processing author: {'authorId': '2090674300', 'name': 'Arif Mehdi'}\n",
      "Processing author: {'authorId': '80325025', 'name': 'A. Hussain'}\n",
      "Processing author: {'authorId': '2090248615', 'name': 'Gerardo Ondo Micha Adjayeng'}\n",
      "Processing author: {'authorId': '2423870', 'name': 'Chul-Hwan Kim'}\n",
      "Processing author: {'authorId': '2805077', 'name': 'S. Elsaiah'}\n",
      "Processing author: {'authorId': '2732268', 'name': 'M. Benidris'}\n",
      "Processing author: {'authorId': '49902464', 'name': 'Yuting Tian'}\n",
      "Processing author: {'authorId': '2137057', 'name': 'J. Mitra'}\n",
      "\n",
      "Processing topic 8\n",
      "Papers for topic 8: 2768\n",
      "Processing author: {'authorId': '2126670728', 'name': 'Bilal Naji Alhasnawi'}\n",
      "Processing author: {'authorId': '46238341', 'name': 'Basil H. Jasim'}\n",
      "Processing author: {'authorId': '3337613', 'name': 'M. D. Esteban'}\n",
      "Processing author: {'authorId': '2192863176', 'name': 'Guangze Meng'}\n",
      "Processing author: {'authorId': '120738696', 'name': 'Yuanyuan Sun'}\n",
      "Processing author: {'authorId': '2117902694', 'name': 'Tao Yu'}\n",
      "Processing author: {'authorId': '2135747899', 'name': 'Chaofan Wang'}\n",
      "Processing author: {'authorId': '46848355', 'name': 'Kaiqi Sun'}\n",
      "Processing author: {'authorId': '2065972116', 'name': 'Peng An'}\n",
      "Processing author: {'authorId': '49444614', 'name': 'C. Guo'}\n",
      "Processing author: {'authorId': '49697620', 'name': 'Yuwei Sun'}\n",
      "Processing author: {'authorId': '8302338', 'name': 'C. Yuan'}\n",
      "Processing author: {'authorId': '46580989', 'name': 'Xin-ping Yan'}\n",
      "Processing author: {'authorId': '2153675759', 'name': 'Yang Wang'}\n",
      "Processing author: {'authorId': '74180580', 'name': 'Qizhen Jiang'}\n",
      "Processing author: {'authorId': '90610080', 'name': 'A. Demeo'}\n",
      "Processing author: {'authorId': '2057774567', 'name': 'Michael L. Peterson'}\n",
      "Processing author: {'authorId': '2275993313', 'name': 'Zhijian Ding'}\n",
      "Processing author: {'authorId': '2275764057', 'name': 'Junjie Lin'}\n",
      "Processing author: {'authorId': '2276154474', 'name': 'Tianxu Wang'}\n",
      "Processing author: {'authorId': '2275625132', 'name': 'Xia Yi'}\n",
      "Processing author: {'authorId': '2275743028', 'name': 'Yitong Wang'}\n",
      "Processing author: {'authorId': '35946065', 'name': 'S. Agrawal'}\n",
      "Processing author: {'authorId': '2225939', 'name': 'K. Porate'}\n",
      "Processing author: {'authorId': '32296679', 'name': 'A. Nag'}\n",
      "Processing author: {'authorId': '32489281', 'name': 'Rami J. Haddad'}\n",
      "Processing author: {'authorId': '2146083940', 'name': 'Chia-Hung Lin'}\n",
      "Processing author: {'authorId': '120108979', 'name': 'W. Hsieh'}\n",
      "Processing author: {'authorId': '102574499', 'name': 'Chao-Shun Chen'}\n",
      "Processing author: {'authorId': '46652684', 'name': 'Cheng-Ting Hsu'}\n",
      "Processing author: {'authorId': '144581126', 'name': 'T. Ku'}\n",
      "Processing author: {'authorId': '102574499', 'name': 'Chao-Shun Chen'}\n",
      "Processing author: {'authorId': '2146083940', 'name': 'Chia-Hung Lin'}\n",
      "Processing author: {'authorId': '120108979', 'name': 'W. Hsieh'}\n",
      "Processing author: {'authorId': '46652684', 'name': 'Cheng-Ting Hsu'}\n",
      "Processing author: {'authorId': '144581126', 'name': 'T. Ku'}\n",
      "Processing author: {'authorId': '2107541971', 'name': 'C. H. Lin'}\n",
      "Processing author: {'authorId': '46730015', 'name': 'C. S. Chen'}\n",
      "Processing author: {'authorId': '120108979', 'name': 'W. Hsieh'}\n",
      "Processing author: {'authorId': '2107794410', 'name': 'C. Hsu'}\n",
      "Processing author: {'authorId': '145797294', 'name': 'H. Chuang'}\n",
      "Processing author: {'authorId': '2148846454', 'name': 'C. Ho'}\n",
      "\n",
      "Processing topic 9\n",
      "Papers for topic 9: 1075\n",
      "Processing author: {'authorId': '101319241', 'name': 'A. Hamdi'}\n",
      "Processing author: {'authorId': '102765027', 'name': 'Aip Abdul Latip'}\n",
      "Processing author: {'authorId': '145856590', 'name': 'Machmud Effendy'}\n",
      "Processing author: {'authorId': '2075576146', 'name': 'Yuliana Fistriyani Hayon'}\n",
      "Processing author: {'authorId': '101985297', 'name': 'R. S. Wicaksono'}\n",
      "Processing author: {'authorId': '71884443', 'name': 'Achmad Imam Agung'}\n",
      "Processing author: {'authorId': None, 'name': 'K radiratu'}\n",
      "Processing author: {'authorId': '41016875', 'name': 'Belly Yan Dewantara'}\n",
      "Processing author: {'authorId': '93734647', 'name': 'Rudi Rustandi'}\n",
      "Processing author: {'authorId': '2056050937', 'name': 'M. Hara'}\n",
      "Processing author: {'authorId': '2005917056', 'name': 'Nur Fauziyah'}\n",
      "Processing author: {'authorId': '150997640', 'name': 'Fikri P. Djamaludin'}\n",
      "Processing author: {'authorId': '66831843', 'name': 'Vecky C. Poekoel'}\n",
      "Processing author: {'authorId': '49023019', 'name': 'Meita Rumbayan'}\n",
      "\n",
      "Topic 1: Power Electronics and Control Systems\n",
      "Keywords: power, high, voltage, control, current, dc, circuit, converter, reliability, phase\n",
      "- SPWM (Sinusoidal Pulse Width Modulation) control method of inverter based on STC (Sensitivity Time Control) single chip microcomputer (dominance: 155.6471)\n",
      "- High power density dc/dc converter: Selection of converter topology (dominance: 154.5175)\n",
      "- High-current switch power supply and high-current switch power supply system (dominance: 151.2035)\n",
      "- Salient electromagnetic pole and permanent-magnet invisible magnetic pole mixed excitation power generation device with vacuum pump (dominance: 142.3111)\n",
      "- Variable frequency power-saving control system of circulating pump (dominance: 142.3014)\n",
      "- Energy-saving power supply system (dominance: 138.9672)\n",
      "- Switched reluctance motor (dominance: 136.7564)\n",
      "- Full-Integrated power module for motor drive applications (dominance: 135.6397)\n",
      "- Door opening apparatus for induction type entrance guard system (dominance: 134.5376)\n",
      "- Study on three-phase four-leg PMSM control system (dominance: 131.1926)\n",
      "\n",
      "Topic 2: Resource Efficiency in Computing\n",
      "Keywords: data, systems, performance, based, power, reliability, computing, time, memory, applications\n",
      "- Hybrid computing framework security in dynamic offloading for IoT-enabled smart home system (dominance: 304.4814)\n",
      "- Resilience Design Patterns: A Structured Approach to Resilience at Extreme Scale (dominance: 292.2709)\n",
      "- Massively Parallel Processor Architectures for Resource-aware Computing (dominance: 243.3791)\n",
      "- ASTAW: Auto-Scaling Threshold-based Approach for Web Application in Cloud Computing Environment (dominance: 238.9464)\n",
      "- Recovery of Distributed Iterative Solvers for Linear Systems Using Non-Volatile RAM (dominance: 237.8239)\n",
      "- REDUCT: Keep it Close, Keep it Cool! : Efficient Scaling of DNN Inference on Multi-core CPUs with Near-Cache Compute (dominance: 233.3972)\n",
      "- Proximu: Efficiently Scaling DNN Inference in Multi-core CPUs through Near-Cache Compute (dominance: 201.1500)\n",
      "- Memory and Interconnect Optimizations for Peta-Scale Deep Learning Systems (dominance: 181.1851)\n",
      "- A Value Based Dynamic Resource Provisioning Model in Cloud (dominance: 181.1800)\n",
      "- An automatic QoS-aware resource partitioning framework for cloud environment (dominance: 178.9602)\n",
      "\n",
      "Topic 3: Energy Systems Development\n",
      "Keywords: energy, systems, power, gas, water, heat, plant, development, design, technology\n",
      "- Techno Economic Aspects of Power Generation From Agriwaste in India (dominance: 437.7986)\n",
      "- Warshipbuilding on the Clyde: Naval Orders and the Prosperity of the Clyde Shipbuilding Industry, 1889-1939. By Hugh B. Peebles. Edinburgh: John Donald Publishers, 1987. Pp. 205. $45.00 (dominance: 271.1582)\n",
      "- Biomass gasifier based power projects under clean development mechanism in India: A preliminary assessment (dominance: 261.1646)\n",
      "- Fuel Cell/Turbine Ultra High Efficiency Power System (dominance: 241.1614)\n",
      "- The Simple Logistics of Biofuels, Sustainability, and Indirect Energy Needs (dominance: 220.0675)\n",
      "- Flared natural gas-based onsite atmospheric water harvesting (AWH) for oilfield operations (dominance: 207.8458)\n",
      "- Prospects of creation of the two-component nuclear energy system (dominance: 200.0583)\n",
      "- Studi Kelayakan Ekonomi Pembangunan PLTD Sistem Dual Fuel dengan Gasifikasi Sekam Padi Kapasitas 50 kVA (dominance: 175.6058)\n",
      "- Economic and Thermodynamic Analysis for Preliminary Design of Dry Steam Geothermal Power Plant (GPP) with Multifarious Gas Removal System (GRS) in Kamojang, West Java, Indonesia (dominance: 170.0711)\n",
      "- Design and commissioning of a pilot-scale solid state anaerobic digester for the Canadian Prairies (dominance: 156.7356)\n",
      "\n",
      "Topic 4: Power System Reliability\n",
      "Keywords: power, network, distribution, reliability, transmission, grid, fault, systems, line, paper\n",
      "- Research on the characteristic gas acquisition and data processing system for power transformer (dominance: 160.0502)\n",
      "- Improving the efficiency of relay protection at a mining and processing plant (dominance: 155.6281)\n",
      "- Increased transmission capacity by forced symmetrization (dominance: 140.0780)\n",
      "- Current Limiting and Recovering Characteristics of Three-Phase Transformer-Type SFCL With Neutral Lines According to Reclosing Procedure (dominance: 127.8708)\n",
      "- Electrical power system key node identification method based on active power load flow betweenness (dominance: 123.4195)\n",
      "- Research and Analysis of Reliability of Power Grid SVG Master Circuit in Rural Areas (dominance: 118.9765)\n",
      "- EHVAC transmission lines maintenance techniques in Indian perspective - A review (dominance: 116.7578)\n",
      "- Analysis of the neutral grounding modes influence on the reliability characteristics of local systems with renewable energy sources (dominance: 116.7460)\n",
      "- Power marketing and distribution network remote visual control system based on big data architecture (dominance: 113.4150)\n",
      "- Power swing relaying in the Texas Panhandle - an out-of-step Odyssey (dominance: 112.3068)\n",
      "\n",
      "Topic 5: Lithium-Ion Battery Optimization\n",
      "Keywords: battery, charging, vehicle, electric, vehicles, batteries, energy, power, capacity, ev\n",
      "- 전기자동차용 납산, 리튬이온 구동축전지팩 성능특성 비교 (dominance: 100.0828)\n",
      "- Modeling spare capacity reuse in EV charging stations based on the Li-ion battery profile (dominance: 22.5033)\n",
      "- RAC: Reconfiguration-Assisted Charging in Large-Scale Lithium-Ion Battery Systems (dominance: 22.1524)\n",
      "- Research on Group Vertical Value Chain Optimization and Information Integration Tactics: Guarantee for Group Financial Strategy Execution Power - Case Study from CEC (dominance: 21.5654)\n",
      "- Series-connected lithium-ion battery pack health modeling with cell inconsistency evaluation (dominance: 15.8809)\n",
      "- Electric vehicles: a strategy for reduced utilization of critical fuels in transportation (dominance: 12.2197)\n",
      "- Evaluating the Degradation Mechanism and State of Health of LiFePO4 Lithium-Ion Batteries in Real-World Plug-in Hybrid Electric Vehicles Application for Different Ageing Paths (dominance: 9.5270)\n",
      "- Equalization technology of special vehicle power battery pack (dominance: 8.3887)\n",
      "- Fast screening of capacity and internal resistance for cascade utilization of the retired power lithium-ion batteries (dominance: 8.0446)\n",
      "- Voltage equalization for series connected lithium-ion battery cells (dominance: 6.9396)\n",
      "\n",
      "Topic 6: Wireless Communication Performance\n",
      "Keywords: power, communication, capacity, channel, wireless, network, performance, proposed, transmission, systems\n",
      "- Bit Error Rate Analysis of MIMO Schemes in LTE Systems (dominance: 478.8758)\n",
      "- Layered turbo trellis-coded modulation for cooperative communications (dominance: 406.7038)\n",
      "- Distributed joint source-channel coding and modulation for wireless communications (dominance: 394.4513)\n",
      "- Wireless Communication over Dispersive Channels (dominance: 391.1489)\n",
      "- Performance Analysis of Cooperative Communication in the Presence of Co-channel Interference (dominance: 365.5896)\n",
      "- Performance analysis of relay-aided wireless communication systems (dominance: 350.0400)\n",
      "- Enhanced cognitive radio with energy harvesting and non-ortohogonal multiple access (dominance: 333.3702)\n",
      "- Transmit and Multiuser Diversity Techniques in Wireless Communications (dominance: 293.3936)\n",
      "- Physical Layer Techniques for OFDM-Based Cognitive Radios (dominance: 271.1719)\n",
      "- HYBRID OVERLAY/UNDERLAY COGNITIVE RADIO NETWORKS WITH MC-CDMA (dominance: 267.8156)\n",
      "No clear common topic name found. Increasing iterations to 5.\n",
      "No clear common topic name found. Increasing iterations to 7.\n",
      "No clear common topic name found. Increasing iterations to 9.\n",
      "No clear common topic name found. Increasing iterations to 11.\n",
      "\n",
      "Topic 7: Engineering Applications\n",
      "Keywords: temperature, design, la, thermal, cooling, en, performance, engine, test, high\n",
      "- Concentrador fotovoltaico para iluminación por fibra óptica de células multiunión (dominance: 28.8602)\n",
      "- Numerical simulations of thermal storage systems : emphasis on latent energy storage using phase change materials (PCM) (dominance: 17.6130)\n",
      "- Numerical simulation of multiphase immiscible flow on unstructured meshes (dominance: 15.5425)\n",
      "- A Conceptual Mars Exploration Vehicle Architecture with Chemical Propulsion, Near-Term Technology, and High Modularity to Enable Near-Term Human Missions to Mars (dominance: 15.4896)\n",
      "- Design, Development and Performance Evaluation of the Manually Controlled Rotary Type Paddy Cutter with the Knapsack Sprayer Power Utilized (dominance: 13.0891)\n",
      "- MECHANICAL DESIGN of Machine Components (dominance: 11.6545)\n",
      "- 1024 × 1024 Si:As IBC detector arrays for JWST MIRI (dominance: 11.1353)\n",
      "- 1Kx1K Si:As IBC detector arrays for JWST MIRI and other applications (dominance: 9.5917)\n",
      "- Radio frequency interference detection and mitigation techniques for navigation and Earth observation (dominance: 9.1988)\n",
      "- Experimental study of a liquid overfeeding mobile air-conditioning system (dominance: 8.9928)\n",
      "\n",
      "Topic 8: Power System Reliability\n",
      "Keywords: power, reliability, model, proposed, capacity, method, load, paper, based, optimization\n",
      "- Joint analysis of power system reliability and market price considering the uncertainties of load forecasts (dominance: 194.5165)\n",
      "- Optimal rescheduling of real and reactive powers of generators for zonal congestion management based on FDR PSO (dominance: 173.4074)\n",
      "- Probabilistic Assessment of Power System Performance Quality (dominance: 171.1727)\n",
      "- Methodology for planning tie lines between interconnected power systems (dominance: 170.0707)\n",
      "- Impact of EDRP on composite reliability of restructured power systems (dominance: 164.5149)\n",
      "- Power System Reliability Analysis with Emergency Demand Response Program (dominance: 164.5149)\n",
      "- Basic considerations in electrical generating capacity adequacy evaluation (dominance: 161.1868)\n",
      "- Coordination mechanism of maintenance scheduling using modified PSO in a restructured power market (dominance: 161.1816)\n",
      "- Voltage Profile Enhancement and Loss Minimization Using Optimal Placement and Sizing of Distributed Generation in Reconfigured Network (dominance: 158.9667)\n",
      "- A Comprehensive Analysis of Reliability-oriented Optimal Distribution System Reconfiguration (dominance: 157.8562)\n",
      "\n",
      "Topic 9: Renewable Energy Management\n",
      "Keywords: power, energy, wind, grid, storage, generation, control, renewable, pv, capacity\n",
      "- A New Robust Energy Management and Control Strategy for a Hybrid Microgrid System Based on Green Energy (dominance: 234.5026)\n",
      "- A Multi-objective Control Strategy of the Energy Storage System in Distribution Networks with a High Proportion of Renewable Energy (dominance: 198.9568)\n",
      "- Research on power load flow calculation for photovoltaic-ship power system based on PSAT (dominance: 196.7260)\n",
      "- Community Smart Grid Utilizing Dynamic Demand Response and Tidal Power for Grid Stabilization (dominance: 182.2800)\n",
      "- New energy consumption optimization algorithm and model analysisbased on demand-side response (dominance: 177.8471)\n",
      "- Economic Dispatch of Thermal Units with the Impact of Wind Power Plant (dominance: 176.7435)\n",
      "- A novel centralized storage model for distributed photovoltaic generation systems (dominance: 174.5139)\n",
      "- Optimization of Photovoltaic Penetration in Distribution Systems Considering Annual Duration Curve of Solar Irradiation (dominance: 172.2967)\n",
      "- Enhancement of PV Penetration With DSTATCOM in Taipower Distribution System (dominance: 164.5216)\n",
      "- Optimization of photovoltaic penetration with DSTATCOM in distribution systems (dominance: 163.4109)\n",
      "\n",
      "Topic 10: Renewable Energy Systems\n",
      "Keywords: control, used, yang, power, speed, dan, wave, signal, using, data\n",
      "- SISTEM PEMBUATAN LAPORAN OPERASIONAL PENDUKUNG PENAGIHAN DI PEMBANGKIT LISTRIK TENAGA GAS DAN UAP (PLTGU) SENGKANG (dominance: 80.1105)\n",
      "- Studi Eksperimen Pengaruh Variasi Sudut Penempatan Plat Datar Penghalang di Depan Returning Blade Terhadap Performa Turbin Angin Savonius (Studi Kasus untuk \"Studi Kasus untuk.... (dominance: 27.2642)\n",
      "- PENGGUNAAN TEKNOLOGI MPPT (MAXIMUM POWER POINT TRACKER) PADA SISTEM PEMBANGKIT LISTRIK TENAGA ANGIN (PLTB) (dominance: 19.3922)\n",
      "- APLIKASI PERANCANGAN GENERATOR SINKRON MAGNETPERMANEN PADA PEMBANGKIT LISTRIK TENAGA BAYUMENGGUNAKAN MATLAB SIMULINK (dominance: 13.9657)\n",
      "- ANALISIS GANGGUAN GENERATOR DI PT. PJB UNIT PEMBANGKITAN BRANTASPLTA MENDALAN MALANG (dominance: 10.1719)\n",
      "- Perhitungan Kebutuhan Daya Listrik untuk Penggerak Perahu Nelayan Bertenaga Surya (dominance: 8.4725)\n",
      "- Pemanfaatan Air Kondensat Untuk Meningkatkan Unjuk Kerja Dan Efisiensi AC Split (dominance: 8.4666)\n",
      "- Effects of Magnetic and Gravitational Torques on Spinning Satellite Attitude (dominance: 7.7657)\n",
      "- Perancangan Pembangkit Listrik Tenaga Mikrohidro Dengan Turbin Cross Flow Menggunakan Generator DC Magnet Permanen (dominance: 6.4196)\n",
      "- Audit Energi Gedung Rektorat Universitas Sam Ratulangi Manado (dominance: 6.3444)\n",
      "\n",
      "API Usage Statistics:\n",
      "Total tokens: 9844\n",
      "Estimated cost: $0.0015\n"
     ]
    }
   ],
   "source": [
    "# Analyze papers with topic naming\n",
    "fields_to_analyze = ['Computer Science', 'Economics', 'Engineering', 'Physics', 'Mathematics']\n",
    "df_analyzed = analyze_papers_with_topic_names(df,fields_to_analyze=fields_to_analyze, n_papers=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define valid fields and clean fields of study\n",
    "valid_fields = ['Computer Science', 'Economics', 'Engineering', 'Physics', 'Mathematics', 'Medicine', 'Business', 'Environmental Science', 'Chemistry', 'Materials Science', 'Geography', 'Biology', 'Geology', 'Political Science', 'Psychology', 'Com']\n",
    "df['fieldsOfStudy'] = df['fieldsOfStudy'].apply(clean_fields_of_study)\n",
    "# Filter out papers with excluded fields\n",
    "exclude_fields = ['Chemistry', 'Materials Science', 'Geography', 'Biology', 'Geology', 'Medicine', 'Political Science', 'Psychology', 'Com']\n",
    "df_filtered = df[df['fieldsOfStudy'].apply(lambda x: not set(x).issubset(set(exclude_fields)))]\n",
    "# Get unique fields of study\n",
    "unique_fields = set([field for fields in df_filtered['fieldsOfStudy'] for field in fields if field not in exclude_fields])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
