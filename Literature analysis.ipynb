{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import configparser\n",
    "import tiktoken\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess function\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, (str, int, float)):\n",
    "        return ''\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(tokens)\n",
    "# Function to convert string representation of list to actual list, replacing long strings with \"Unknown\"\n",
    "def string_to_list(s):\n",
    "    if isinstance(s, str):\n",
    "        # Remove brackets and split by comma\n",
    "        fields = [field.strip().strip(\"'\") for field in s.strip('[]').split(',')]\n",
    "        # Replace fields wit more than 100 characters with \"Unknown\"\n",
    "        return [\"Unknown\" if len(field) > 100 else field for field in fields]\n",
    "    return [\"Unknown\"]  # Return [\"Unknown\"] for empty or non-string entries\n",
    "\n",
    "# Clean fields of study\n",
    "def clean_fields_of_study(s):\n",
    "    if pd.isna(s) or s == '[]':\n",
    "        return [\"Unknown\"]\n",
    "    if isinstance(s, str):\n",
    "        fields = [field.strip().strip(\"'\\\"\") for field in s.strip('[]').split(',')]\n",
    "        cleaned_fields = []\n",
    "        for field in fields:\n",
    "            if field in valid_fields:\n",
    "                cleaned_fields.append(field)\n",
    "            else:\n",
    "                cleaned_fields.append(\"Unknown\")\n",
    "        return cleaned_fields if cleaned_fields else [\"Unknown\"]\n",
    "    return [\"Unknown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model topics function\n",
    "def model_topics(df, field, num_topics=10, num_words=5):\n",
    "    df_field = df[df['fieldsOfStudy'].apply(lambda x: field in x)]\n",
    "    text_data = df_field['abstract'].fillna('')\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    doc_term_matrix = vectorizer.fit_transform(text_data)\n",
    "    lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "    lda_output = lda_model.fit_transform(doc_term_matrix)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    topic_names = []\n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-num_words - 1:-1]]\n",
    "        topic_name = f\"Topic_{field}_{topic_idx+1}\"\n",
    "        print(f\"{topic_name}: {', '.join(top_words)}\")\n",
    "        topic_names.append(topic_name)\n",
    "    \n",
    "    return lda_output, df_field.index, topic_names\n",
    "\n",
    "\n",
    "# Get top papers per topic per field\n",
    "def get_top_papers_per_topic_per_field(df, text_column, field_column, fields_to_analyze, num_topics=5, n_top=5):\n",
    "    all_top_papers = {}\n",
    "    for field in fields_to_analyze:\n",
    "        print(f\"\\nProcessing field: {field}\")\n",
    "        df_field = df[df[field_column].apply(lambda x: field in x)]\n",
    "        if df_field.empty:\n",
    "            print(f\"No papers found for field: {field}\")\n",
    "            continue\n",
    "        text_data = df_field[text_column].fillna('')\n",
    "        vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "        doc_term_matrix = vectorizer.fit_transform(text_data)\n",
    "        lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "        topic_distributions = lda_model.fit_transform(doc_term_matrix)\n",
    "        top_papers = {}\n",
    "        for topic in range(num_topics):\n",
    "            topic_scores = topic_distributions[:, topic]\n",
    "            other_topics_sum = topic_distributions.sum(axis=1) - topic_scores\n",
    "            topic_dominance = topic_scores / (other_topics_sum + 1e-10)  # Avoid division by zero\n",
    "            top_indices = np.argsort(topic_dominance)[-n_top:][::-1]\n",
    "            top_papers[topic] = [\n",
    "                {\n",
    "                    'paperId': df_field.iloc[i]['paperId'],  # Include paperId\n",
    "                    'title': df_field.iloc[i]['title'],\n",
    "                    'document': df_field.iloc[i][text_column],\n",
    "                    'abstract': df_field.iloc[i]['abstract'],\n",
    "                    'score': float(topic_scores[i]),\n",
    "                    'dominance_ratio': float(topic_dominance[i])\n",
    "                }\n",
    "                for i in top_indices\n",
    "            ]\n",
    "        all_top_papers[field] = top_papers\n",
    "        \n",
    "        # Print top words for each topic\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        for topic_idx, topic in enumerate(lda_model.components_):\n",
    "            top_words = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]\n",
    "            print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")\n",
    "    \n",
    "    return all_top_papers\n",
    "\n",
    "def classify_paper(row):\n",
    "    # Get all topic columns\n",
    "    topic_columns = [col for col in row.index if col.startswith(\"Topic_\")]\n",
    "    \n",
    "    if not topic_columns:\n",
    "        return 'N/A', 'N/A'\n",
    "    \n",
    "    topic_scores = row[topic_columns]\n",
    "    \n",
    "    # Convert topic scores to numeric, replacing non-numeric values with NaN\n",
    "    topic_scores = pd.to_numeric(topic_scores, errors='coerce')\n",
    "    \n",
    "    # Drop any NaN values\n",
    "    topic_scores = topic_scores.dropna()\n",
    "    \n",
    "    if topic_scores.empty:\n",
    "        return 'N/A', 'N/A'\n",
    "    \n",
    "    # Get top 2 topics\n",
    "    top_2_topics = topic_scores.nlargest(2)\n",
    "    \n",
    "    # Extract topic names without the \"Topic_\" prefix\n",
    "    primary_topic = top_2_topics.index[0].split('_', 1)[1] if len(top_2_topics) > 0 else 'N/A'\n",
    "    secondary_topic = top_2_topics.index[1].split('_', 1)[1] if len(top_2_topics) > 1 else 'N/A'\n",
    "    \n",
    "    return primary_topic, secondary_topic\n",
    "def analyze_specific_topic(df, topic, num_subtopics=5, n_top=5):\n",
    "    # Filter the dataframe for the specific topic\n",
    "    df_topic = df[df['Primary_Topic'] == topic].copy()\n",
    "    \n",
    "    print(f\"Analyzing topic: {topic}\")\n",
    "    print(f\"Number of papers: {len(df_topic)}\")\n",
    "    \n",
    "    if len(df_topic) < 10:  # Adjust this threshold as needed\n",
    "        print(\"Not enough papers for meaningful subtopic analysis.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Ensure index is unique\n",
    "    df_topic = df_topic.reset_index(drop=True)\n",
    "    \n",
    "    # Prepare the text data\n",
    "    text_data = df_topic['abstract'].fillna('')\n",
    "    \n",
    "    # Create document-term matrix\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    doc_term_matrix = vectorizer.fit_transform(text_data)\n",
    "    \n",
    "    # Create and fit LDA model for subtopics\n",
    "    lda_model = LatentDirichletAllocation(n_components=num_subtopics, random_state=42)\n",
    "    subtopic_distributions = lda_model.fit_transform(doc_term_matrix)\n",
    "    \n",
    "    # Get top words for each subtopic\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    for idx, subtopic in enumerate(lda_model.components_):\n",
    "        top_words = [feature_names[i] for i in subtopic.argsort()[:-10 - 1:-1]]\n",
    "        print(f\"Subtopic {idx + 1}: {', '.join(top_words)}\")\n",
    "    \n",
    "    # Get top papers for each subtopic\n",
    "    top_papers = {}\n",
    "    for subtopic in range(num_subtopics):\n",
    "        subtopic_scores = subtopic_distributions[:, subtopic]\n",
    "        other_subtopics_sum = subtopic_distributions.sum(axis=1) - subtopic_scores\n",
    "        subtopic_dominance = subtopic_scores / (other_subtopics_sum + 1e-10)\n",
    "        top_indices = np.argsort(subtopic_dominance)[-n_top:][::-1]\n",
    "        \n",
    "        top_papers[subtopic] = [\n",
    "            {\n",
    "                'title': df_topic.iloc[i]['title'],\n",
    "                'abstract': df_topic.iloc[i]['abstract'],\n",
    "                'score': float(subtopic_scores[i]),\n",
    "                'dominance_ratio': float(subtopic_dominance[i])\n",
    "            }\n",
    "            for i in top_indices\n",
    "        ]\n",
    "    \n",
    "    # Generate subtopic names using the same method as before - Skipped until copyright breach issue is resolved\n",
    "    \"\"\"subtopic_names = {}\n",
    "    for subtopic, papers in top_papers.items():\n",
    "        abstracts = \"\\n\\n\".join([paper['abstract'] for paper in papers])\n",
    "        keywords = [word for paper in papers for word in paper['abstract'].split()[:10]]\n",
    "        subtopic_name = generate_topic_name(abstracts, keywords)\n",
    "        subtopic_names[subtopic] = subtopic_name\n",
    "    \"\"\"\n",
    "    # Add subtopic scores and names to the dataframe\n",
    "    for i in range(num_subtopics):\n",
    "        df_topic[f'Subtopic_{i+1}_Score'] = subtopic_distributions[:, i]\n",
    "    df_topic['Primary_Subtopic'] = df_topic[[f'Subtopic_{i+1}_Score' for i in range(num_subtopics)]].idxmax(axis=1)\n",
    "   # df_topic['Primary_Subtopic_Name'] = df_topic['Primary_Subtopic'].map(lambda x: subtopic_names[int(x.split('_')[1]) - 1])\n",
    "    \n",
    "    # Update the main dataframe with the new subtopic information\n",
    "    df_update = df.copy()\n",
    "    df_update.loc[df_topic.index, [f'Subtopic_{i+1}_Score' for i in range(num_subtopics)]] = df_topic[[f'Subtopic_{i+1}_Score' for i in range(num_subtopics)]]\n",
    "    df_update.loc[df_topic.index, 'Primary_Subtopic'] = df_topic['Primary_Subtopic']\n",
    "  # df_update.loc[df_topic.index, 'Primary_Subtopic_Name'] = df_topic['Primary_Subtopic_Name']\n",
    "    \n",
    "    return df_update, top_papers, subtopic_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
