{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74701ba5",
   "metadata": {},
   "source": [
    "# Software mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4856d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS\n",
    "# =============================================================================\n",
    "import openai\n",
    "import anthropic\n",
    "import json\n",
    "import time\n",
    "import configparser\n",
    "import tiktoken\n",
    "from typing import List, Dict, Tuple, Optional, Callable\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, asdict\n",
    "import google.generativeai as genai\n",
    "from itertools import combinations\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import traceback\n",
    "import pandas as pd\n",
    "from difflib import get_close_matches, SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed2db008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Initialization functions defined\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# CONFIGURATION & INITIALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "def initialize_openai():\n",
    "    \"\"\"Initialize OpenAI client from config file\"\"\"\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('config_LLM.txt')\n",
    "    api_key = config['LLM'].get('OPENAI_API_KEY')\n",
    "    model_type = config['LLM'].get('MODEL_TYPE_adv', 'gpt-4o-mini')\n",
    "    client = openai.OpenAI(api_key=api_key)\n",
    "    return client, model_type\n",
    "\n",
    "def initialize_anthropic():\n",
    "    \"\"\"Initialize Anthropic client from config file\"\"\"\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('config_LLM.txt')\n",
    "    api_key = config['LLM'].get('ANTHROPIC_API_KEY')\n",
    "    client = anthropic.Anthropic(api_key=api_key) if api_key else None\n",
    "    return client\n",
    "\n",
    "def initialize_google():\n",
    "    \"\"\"Initialize Google Gemini client from config file\"\"\"\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('config_LLM.txt')\n",
    "    api_key = config['LLM'].get('GOOGLE_API_KEY')\n",
    "    if api_key:\n",
    "        genai.configure(api_key=api_key)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "print(\"✓ Initialization functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76adce0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Token counting utilities defined\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# TOKEN COUNTING UTILITIES\n",
    "# =============================================================================\n",
    "\n",
    "def num_tokens_from_string(string: str, model_name: str) -> int:\n",
    "    \"\"\"Get token count with fallback for unsupported models\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model_name)\n",
    "        return len(encoding.encode(string))\n",
    "    except KeyError:\n",
    "        if model_name.startswith('gpt-5'):\n",
    "            encoding = tiktoken.get_encoding(\"o200k_base\")\n",
    "            return len(encoding.encode(string))\n",
    "        elif model_name.startswith('gpt-4'):\n",
    "            encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "            return len(encoding.encode(string))\n",
    "        elif model_name.startswith('claude'):\n",
    "            return int(len(string) / 3.5)\n",
    "        elif model_name.startswith('models/gemini') or model_name.startswith('gemini'):\n",
    "            return int(len(string) / 4)\n",
    "        else:\n",
    "            return len(string) // 4\n",
    "\n",
    "def count_tokens_in_messages(messages: List[Dict], model: str) -> int:\n",
    "    \"\"\"Count tokens in a list of messages\"\"\"\n",
    "    total_tokens = 0\n",
    "    for message in messages:\n",
    "        if isinstance(message.get('content'), str):\n",
    "            total_tokens += num_tokens_from_string(message['content'], model)\n",
    "        total_tokens += 4  # Message overhead\n",
    "    total_tokens += 3  # Completion overhead\n",
    "    return total_tokens\n",
    "\n",
    "print(\"✓ Token counting utilities defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4092181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CreditTracker class defined\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# CREDIT TRACKER\n",
    "# =============================================================================\n",
    "\n",
    "class CreditTracker:\n",
    "    \"\"\"Track API usage and costs across all models\"\"\"\n",
    "    \n",
    "    PRICING = {\n",
    "        # OpenAI\n",
    "        'gpt-4o': {'input': 1.25, 'output': 5.00},\n",
    "        'gpt-4o-mini': {'input': 0.075, 'output': 0.30},\n",
    "        \n",
    "        # Claude\n",
    "        'claude-3-haiku-20240307': {'input': 0.25, 'output': 1.25},\n",
    "        'claude-3-5-haiku-20241022': {'input': 0.80, 'output': 4.00},\n",
    "        'claude-3-5-sonnet-20241022': {'input': 3.00, 'output': 15.00},\n",
    "        'claude-sonnet-4-20250514': {'input': 3.00, 'output': 15.00},\n",
    "        \n",
    "        # Google\n",
    "        'models/gemini-2.5-flash': {'input': 0.075, 'output': 0.30},\n",
    "        'models/gemini-2.0-flash': {'input': 0.075, 'output': 0.30},\n",
    "        'models/gemini-2.0-flash-001': {'input': 0.075, 'output': 0.30},\n",
    "        'gemini-2.0-flash': {'input': 0.075, 'output': 0.30},\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        self.total_input_tokens = 0\n",
    "        self.total_output_tokens = 0\n",
    "        self.total_cached_tokens = 0\n",
    "        self.total_cost = 0\n",
    "        self.model_usage = {}\n",
    "        self.call_count = 0\n",
    "\n",
    "    def update(self, model: str, input_tokens: int, output_tokens: int, cached_tokens: int = 0):\n",
    "        \"\"\"Update usage statistics\"\"\"\n",
    "        self.total_input_tokens += input_tokens\n",
    "        self.total_output_tokens += output_tokens\n",
    "        self.total_cached_tokens += cached_tokens\n",
    "        self.call_count += 1\n",
    "\n",
    "        pricing = self.PRICING.get(model, {'input': 0.00015, 'output': 0.0006})\n",
    "        \n",
    "        input_cost = (input_tokens / 1_000_000) * pricing['input']\n",
    "        output_cost = (output_tokens / 1_000_000) * pricing['output']\n",
    "        call_cost = input_cost + output_cost\n",
    "        self.total_cost += call_cost\n",
    "\n",
    "        if model not in self.model_usage:\n",
    "            self.model_usage[model] = {\n",
    "                'calls': 0, 'input_tokens': 0, 'output_tokens': 0,\n",
    "                'cached_tokens': 0, 'cost': 0\n",
    "            }\n",
    "\n",
    "        self.model_usage[model]['calls'] += 1\n",
    "        self.model_usage[model]['input_tokens'] += input_tokens\n",
    "        self.model_usage[model]['output_tokens'] += output_tokens\n",
    "        self.model_usage[model]['cached_tokens'] += cached_tokens\n",
    "        self.model_usage[model]['cost'] += call_cost\n",
    "\n",
    "    def get_stats(self):\n",
    "        \"\"\"Get current statistics\"\"\"\n",
    "        return {\n",
    "            \"total_calls\": self.call_count,\n",
    "            \"total_input_tokens\": self.total_input_tokens,\n",
    "            \"total_output_tokens\": self.total_output_tokens,\n",
    "            \"total_tokens\": self.total_input_tokens + self.total_output_tokens,\n",
    "            \"total_cost\": round(self.total_cost, 4),\n",
    "            \"average_cost_per_call\": round(self.total_cost / max(self.call_count, 1), 4),\n",
    "            \"model_breakdown\": {\n",
    "                model: {\n",
    "                    'calls': stats['calls'],\n",
    "                    'total_tokens': stats['input_tokens'] + stats['output_tokens'],\n",
    "                    'cost': round(stats['cost'], 4)\n",
    "                }\n",
    "                for model, stats in self.model_usage.items()\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def print_summary(self):\n",
    "        \"\"\"Print formatted summary\"\"\"\n",
    "        stats = self.get_stats()\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"API USAGE SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Total API Calls: {stats['total_calls']}\")\n",
    "        print(f\"Total Tokens: {stats['total_tokens']:,}\")\n",
    "        print(f\"  - Input: {stats['total_input_tokens']:,}\")\n",
    "        print(f\"  - Output: {stats['total_output_tokens']:,}\")\n",
    "        if self.total_cached_tokens > 0:\n",
    "            print(f\"  - Cached: {self.total_cached_tokens:,}\")\n",
    "        print(f\"\\nTotal Cost: ${stats['total_cost']:.4f}\")\n",
    "        print(f\"Average Cost per Call: ${stats['average_cost_per_call']:.4f}\")\n",
    "\n",
    "        if self.model_usage:\n",
    "            print(\"\\nBreakdown by Model:\")\n",
    "            print(\"-\" * 60)\n",
    "            for model, breakdown in stats['model_breakdown'].items():\n",
    "                print(f\"  {model}:\")\n",
    "                print(f\"    Calls: {breakdown['calls']}\")\n",
    "                print(f\"    Tokens: {breakdown['total_tokens']:,}\")\n",
    "                print(f\"    Cost: ${breakdown['cost']:.4f}\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"✓ CreditTracker class defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390050ac",
   "metadata": {},
   "source": [
    "## The method assessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0631f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data structures defined\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# DATA STRUCTURES\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class AssessmentResult:\n",
    "    \"\"\"Single LLM assessment result\"\"\"\n",
    "    software: str\n",
    "    method: str\n",
    "    rank: int\n",
    "    reasoning: str\n",
    "    sources: List[str]\n",
    "    llm_provider: str\n",
    "    input_tokens: int = 0\n",
    "    output_tokens: int = 0\n",
    "\n",
    "@dataclass\n",
    "class ConsensusResult:\n",
    "    \"\"\"Consensus across multiple LLMs\"\"\"\n",
    "    software: str\n",
    "    method: str\n",
    "    final_rank: int\n",
    "    confidence: float\n",
    "    individual_ranks: Dict[str, int]\n",
    "    individual_reasoning: Dict[str, str]\n",
    "    individual_sources: Dict[str, List[str]]\n",
    "    agreement_level: str\n",
    "    total_tokens: int = 0\n",
    "    total_cost: float = 0.0\n",
    "\n",
    "print(\"✓ Data structures defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7397873c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SoftwareMethodAssessor class initialized (Part 1)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# MAIN ASSESSOR CLASS - PART 1: Core Functions\n",
    "# =============================================================================\n",
    "\n",
    "class SoftwareMethodAssessor:\n",
    "    \"\"\"Main class for software-method assessment using multiple LLMs\"\"\"\n",
    "    \n",
    "    def __init__(self, use_config: bool = True, timeout: int = 180, max_retries: int = 3):\n",
    "        \"\"\"Initialize assessor with API clients\"\"\"\n",
    "        if use_config:\n",
    "            self.openai_client, self.default_model = initialize_openai()\n",
    "            self.anthropic_client = initialize_anthropic()\n",
    "            self.google_enabled = initialize_google()\n",
    "        else:\n",
    "            self.openai_client = None\n",
    "            self.anthropic_client = None\n",
    "            self.google_enabled = False\n",
    "            self.default_model = \"gpt-4o-mini\"\n",
    "        \n",
    "        self.credit_tracker = CreditTracker()\n",
    "        self.timeout = timeout\n",
    "        self.max_retries = max_retries\n",
    "        \n",
    "        # System prompt for assessments\n",
    "        self.system_prompt = \"\"\"You are a technical software assessment expert specialized in power systems analysis software.\n",
    "\n",
    "Use this ranking scale:\n",
    "0 = No support (method cannot be implemented at all)\n",
    "1 = Limited possibility for implementation or extension (requires significant workarounds)\n",
    "2 = Indirectly supported through APIs or extensions (requires external tools/plugins)\n",
    "3 = Directly implemented (native feature in the software)\n",
    "\n",
    "CRITICAL: You MUST search for and provide actual references. Your assessment must be based on real, verifiable sources.\n",
    "\n",
    "For each assessment:\n",
    "1. Search for scientific papers demonstrating implementation (IEEE Xplore, ScienceDirect, arXiv, Google Scholar)\n",
    "2. Find official documentation from the software vendor or project website\n",
    "3. Look for GitHub repositories with code examples or open-source implementations\n",
    "4. Check API documentation or extension/plugin capabilities\n",
    "5. Review user forums, technical blogs, Stack Overflow, or case studies\n",
    "\n",
    "Return your response in VALID JSON format with this exact structure:\n",
    "{\n",
    "    \"rank\": <0-3>,\n",
    "    \"reasoning\": \"<detailed explanation citing specific sources by number, e.g., 'According to [1], PSS/E supports...'>\",\n",
    "    \"sources\": [\n",
    "        \"https://example.com/documentation - Official PSS/E Manual on OPF\",\n",
    "        \"https://doi.org/10.1109/... - Paper title by Author et al.\",\n",
    "        \"https://github.com/org/repo/file.py - Implementation example\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "Each source must include both the URL and a brief description separated by ' - '.\n",
    "Minimum 2 sources required for ranks 2-3, minimum 1 source for rank 1.\"\"\"\n",
    "\n",
    "    def calculate_confidence(self, ranks: List[int]) -> Tuple[float, str]:\n",
    "        \"\"\"Calculate confidence score from multiple assessments\"\"\"\n",
    "        if not ranks:\n",
    "            return 0.0, \"no_data\"\n",
    "        \n",
    "        rank_counts = Counter(ranks)\n",
    "        most_common_count = rank_counts.most_common(1)[0][1]\n",
    "        total_ranks = len(ranks)\n",
    "        confidence = most_common_count / total_ranks\n",
    "        \n",
    "        if total_ranks == 1:\n",
    "            agreement_level = \"single_assessment\"\n",
    "        elif confidence == 1.0:\n",
    "            agreement_level = \"perfect_agreement\"\n",
    "        elif confidence >= 0.75:\n",
    "            agreement_level = \"strong_agreement\"\n",
    "        elif confidence >= 0.5:\n",
    "            agreement_level = \"moderate_agreement\"\n",
    "        else:\n",
    "            agreement_level = \"weak_agreement\"\n",
    "        \n",
    "        return confidence, agreement_level\n",
    "\n",
    "    def export_results(self, results: List[ConsensusResult], filename: str):\n",
    "        \"\"\"Export results to JSON file\"\"\"\n",
    "        output_data = [asdict(result) for result in results]\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(output_data, f, indent=2)\n",
    "        print(f\"\\nResults exported to {filename}\")\n",
    "\n",
    "print(\"✓ SoftwareMethodAssessor class initialized (Part 1)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2debcef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Batch prompt creation added\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# MAIN ASSESSOR CLASS - PART 2: Batch Assessment Methods\n",
    "# =============================================================================\n",
    "\n",
    "def create_batch_assessment_prompt(self, batch_items: List[Tuple[str, str]], batch_size: int = None) -> str:\n",
    "    \"\"\"Create a structured prompt for batch assessment\"\"\"\n",
    "    batch_size = batch_size or len(batch_items)\n",
    "    \n",
    "    items_text = \"\"\n",
    "    for idx, (software, method) in enumerate(batch_items, 1):\n",
    "        items_text += f\"\\n{idx}. Software: {software}\\n   Method: {method}\\n\"\n",
    "    \n",
    "    prompt = f\"\"\"You must assess {len(batch_items)} software-method combinations independently.\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "- Treat each pair as completely independent\n",
    "- Do NOT let one assessment influence another\n",
    "- Provide the SAME quality of research and reasoning for ALL items\n",
    "- Each assessment must have its own sources\n",
    "\n",
    "Items to assess:{items_text}\n",
    "\n",
    "For EACH item above, perform independent research and provide sources with URLs.\n",
    "\n",
    "Return a JSON array with exactly {len(batch_items)} objects:\n",
    "[\n",
    "  {{\n",
    "    \"software\": \"<software name>\",\n",
    "    \"method\": \"<method name>\",\n",
    "    \"rank\": <0-3>,\n",
    "    \"reasoning\": \"<detailed explanation citing sources [1], [2], etc.>\",\n",
    "    \"sources\": [\n",
    "      \"https://... - Description\",\n",
    "      \"https://... - Description\"\n",
    "    ]\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "\n",
    "IMPORTANT: Return ONLY the JSON array, no other text.\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Add to SoftwareMethodAssessor class\n",
    "SoftwareMethodAssessor.create_batch_assessment_prompt = create_batch_assessment_prompt\n",
    "\n",
    "print(\"✓ Batch prompt creation added\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37795839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ OpenAI assessment method added\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# OPENAI ASSESSMENT METHOD\n",
    "# =============================================================================\n",
    "\n",
    "def assess_batch_with_openai(self, batch_items: List[Tuple[str, str]], \n",
    "                             model: str = None, debug: bool = False) -> List[AssessmentResult]:\n",
    "    \"\"\"Assess a batch of items with OpenAI\"\"\"\n",
    "    if model is None:\n",
    "        model = self.default_model\n",
    "    \n",
    "    try:\n",
    "        prompt = self.create_batch_assessment_prompt(batch_items)\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"  DEBUG: Batch size: {len(batch_items)}\")\n",
    "        \n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0.3,\n",
    "            max_tokens=4096,\n",
    "            timeout=self.timeout,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "        usage = response.usage\n",
    "        cached_tokens = 0\n",
    "        if hasattr(usage, 'prompt_tokens_details') and usage.prompt_tokens_details:\n",
    "            cached_tokens = getattr(usage.prompt_tokens_details, 'cached_tokens', 0)\n",
    "        \n",
    "        self.credit_tracker.update(\n",
    "            model=model,\n",
    "            input_tokens=usage.prompt_tokens,\n",
    "            output_tokens=usage.completion_tokens,\n",
    "            cached_tokens=cached_tokens\n",
    "        )\n",
    "        \n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        # Parse JSON\n",
    "        try:\n",
    "            parsed = json.loads(content)\n",
    "            if isinstance(parsed, dict):\n",
    "                if 'assessments' in parsed:\n",
    "                    results_data = parsed['assessments']\n",
    "                elif 'results' in parsed:\n",
    "                    results_data = parsed['results']\n",
    "                else:\n",
    "                    results_data = next(v for v in parsed.values() if isinstance(v, list))\n",
    "            else:\n",
    "                results_data = parsed\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR parsing batch response: {e}\")\n",
    "            return []\n",
    "        \n",
    "        # Convert to AssessmentResult objects\n",
    "        assessment_results = []\n",
    "        for item_data in results_data:\n",
    "            rank = int(item_data.get(\"rank\", 0))\n",
    "            sources = item_data.get(\"sources\", [])\n",
    "            reasoning = item_data.get(\"reasoning\", \"\")\n",
    "            \n",
    "            if rank > 0 and len(sources) == 0:\n",
    "                rank = 0\n",
    "                reasoning += \" [Rank lowered to 0: no sources]\"\n",
    "            \n",
    "            assessment_results.append(AssessmentResult(\n",
    "                software=item_data.get(\"software\", \"\"),\n",
    "                method=item_data.get(\"method\", \"\"),\n",
    "                rank=rank,\n",
    "                reasoning=reasoning,\n",
    "                sources=sources,\n",
    "                llm_provider=f\"openai_{model}\",\n",
    "                input_tokens=usage.prompt_tokens // len(batch_items),\n",
    "                output_tokens=usage.completion_tokens // len(batch_items)\n",
    "            ))\n",
    "        \n",
    "        return assessment_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR in OpenAI batch assessment: {e}\")\n",
    "        return []\n",
    "\n",
    "# Add to class\n",
    "SoftwareMethodAssessor.assess_batch_with_openai = assess_batch_with_openai\n",
    "\n",
    "print(\"✓ OpenAI assessment method added\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31f626ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Claude assessment method added\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# CLAUDE ASSESSMENT METHOD\n",
    "# =============================================================================\n",
    "\n",
    "def assess_batch_with_claude(self, batch_items: List[Tuple[str, str]], \n",
    "                             model: str = \"claude-3-5-haiku-20241022\", \n",
    "                             debug: bool = False) -> List[AssessmentResult]:\n",
    "    \"\"\"Assess a batch of items with Claude\"\"\"\n",
    "    if not self.anthropic_client:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        prompt = self.create_batch_assessment_prompt(batch_items)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"  DEBUG: Batch size: {len(batch_items)}\")\n",
    "        \n",
    "        response = self.anthropic_client.messages.create(\n",
    "            model=model,\n",
    "            max_tokens=4096,\n",
    "            temperature=0.3,\n",
    "            timeout=self.timeout,\n",
    "            system=self.system_prompt,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        self.credit_tracker.update(\n",
    "            model=model,\n",
    "            input_tokens=response.usage.input_tokens,\n",
    "            output_tokens=response.usage.output_tokens\n",
    "        )\n",
    "        \n",
    "        content = response.content[0].text\n",
    "        \n",
    "        # Clean markdown code blocks\n",
    "        content = content.strip()\n",
    "        if content.startswith('```'):\n",
    "            lines = content.split('\\n')\n",
    "            start_idx = 0\n",
    "            end_idx = len(lines)\n",
    "            for i, line in enumerate(lines):\n",
    "                if line.strip().startswith('```'):\n",
    "                    if start_idx == 0:\n",
    "                        start_idx = i + 1\n",
    "                    else:\n",
    "                        end_idx = i\n",
    "                        break\n",
    "            content = '\\n'.join(lines[start_idx:end_idx])\n",
    "        \n",
    "        # Parse JSON\n",
    "        try:\n",
    "            parsed = json.loads(content)\n",
    "            if isinstance(parsed, dict):\n",
    "                if 'assessments' in parsed:\n",
    "                    results_data = parsed['assessments']\n",
    "                elif 'results' in parsed:\n",
    "                    results_data = parsed['results']\n",
    "                else:\n",
    "                    results_data = next(v for v in parsed.values() if isinstance(v, list))\n",
    "            else:\n",
    "                results_data = parsed\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR parsing Claude batch response: {e}\")\n",
    "            return []\n",
    "        \n",
    "        # Convert to AssessmentResult objects\n",
    "        assessment_results = []\n",
    "        for item_data in results_data:\n",
    "            rank = int(item_data.get(\"rank\", 0))\n",
    "            sources = item_data.get(\"sources\", [])\n",
    "            reasoning = item_data.get(\"reasoning\", \"\")\n",
    "            \n",
    "            if rank > 0 and len(sources) == 0:\n",
    "                rank = 0\n",
    "                reasoning += \" [Rank lowered to 0: no sources]\"\n",
    "            \n",
    "            assessment_results.append(AssessmentResult(\n",
    "                software=item_data.get(\"software\", \"\"),\n",
    "                method=item_data.get(\"method\", \"\"),\n",
    "                rank=rank,\n",
    "                reasoning=reasoning,\n",
    "                sources=sources,\n",
    "                llm_provider=f\"claude_{model}\",\n",
    "                input_tokens=response.usage.input_tokens // len(batch_items),\n",
    "                output_tokens=response.usage.output_tokens // len(batch_items)\n",
    "            ))\n",
    "        \n",
    "        return assessment_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR in Claude batch assessment: {e}\")\n",
    "        return []\n",
    "\n",
    "# Add to class\n",
    "SoftwareMethodAssessor.assess_batch_with_claude = assess_batch_with_claude\n",
    "\n",
    "print(\"✓ Claude assessment method added\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a212a3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Google assessment method added\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# GOOGLE ASSESSMENT METHOD\n",
    "# =============================================================================\n",
    "\n",
    "def assess_batch_with_google(self, batch_items: List[Tuple[str, str]], \n",
    "                             model: str = \"models/gemini-2.0-flash\", \n",
    "                             debug: bool = False) -> List[AssessmentResult]:\n",
    "    \"\"\"Assess a batch of items with Google Gemini\"\"\"\n",
    "    if not self.google_enabled:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        prompt = self.create_batch_assessment_prompt(batch_items)\n",
    "        \n",
    "        if not model.startswith('models/'):\n",
    "            model = f\"models/{model}\"\n",
    "        \n",
    "        gemini_model = genai.GenerativeModel(\n",
    "            model_name=model,\n",
    "            generation_config={\n",
    "                \"temperature\": 0.3,\n",
    "                \"max_output_tokens\": 4096,\n",
    "            },\n",
    "            system_instruction=self.system_prompt\n",
    "        )\n",
    "        \n",
    "        response = gemini_model.generate_content(\n",
    "            prompt,\n",
    "            generation_config={\n",
    "                \"temperature\": 0.3,\n",
    "                \"max_output_tokens\": 4096,\n",
    "            },\n",
    "            request_options={'timeout': self.timeout}\n",
    "        )\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"  DEBUG: Successfully used model: {model}\")\n",
    "        \n",
    "        # Extract token counts\n",
    "        try:\n",
    "            input_tokens = response.usage_metadata.prompt_token_count\n",
    "            output_tokens = response.usage_metadata.candidates_token_count\n",
    "        except AttributeError:\n",
    "            input_tokens = int(len(prompt.split()) * 1.3)\n",
    "            output_tokens = int(len(response.text.split()) * 1.3)\n",
    "        \n",
    "        self.credit_tracker.update(\n",
    "            model=model,\n",
    "            input_tokens=int(input_tokens),\n",
    "            output_tokens=int(output_tokens)\n",
    "        )\n",
    "        \n",
    "        content = response.text.strip()\n",
    "        \n",
    "        # Clean markdown\n",
    "        if content.startswith('```'):\n",
    "            lines = content.split('\\n')\n",
    "            start_idx = 0\n",
    "            end_idx = len(lines)\n",
    "            for i, line in enumerate(lines):\n",
    "                if line.strip().startswith('```'):\n",
    "                    if start_idx == 0:\n",
    "                        start_idx = i + 1\n",
    "                    else:\n",
    "                        end_idx = i\n",
    "                        break\n",
    "            content = '\\n'.join(lines[start_idx:end_idx])\n",
    "        \n",
    "        # Parse JSON\n",
    "        try:\n",
    "            parsed = json.loads(content)\n",
    "            if isinstance(parsed, dict):\n",
    "                if 'assessments' in parsed:\n",
    "                    results_data = parsed['assessments']\n",
    "                elif 'results' in parsed:\n",
    "                    results_data = parsed['results']\n",
    "                else:\n",
    "                    results_data = next(v for v in parsed.values() if isinstance(v, list))\n",
    "            else:\n",
    "                results_data = parsed\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR parsing Google batch response: {e}\")\n",
    "            return []\n",
    "        \n",
    "        # Convert to AssessmentResult objects\n",
    "        assessment_results = []\n",
    "        for item_data in results_data:\n",
    "            rank = int(item_data.get(\"rank\", 0))\n",
    "            sources = item_data.get(\"sources\", [])\n",
    "            reasoning = item_data.get(\"reasoning\", \"\")\n",
    "            \n",
    "            if rank > 0 and len(sources) == 0:\n",
    "                rank = 0\n",
    "                reasoning += \" [Rank lowered to 0: no sources]\"\n",
    "            \n",
    "            assessment_results.append(AssessmentResult(\n",
    "                software=item_data.get(\"software\", \"\"),\n",
    "                method=item_data.get(\"method\", \"\"),\n",
    "                rank=rank,\n",
    "                reasoning=reasoning,\n",
    "                sources=sources,\n",
    "                llm_provider=f\"google_{model.replace('models/', '')}\",\n",
    "                input_tokens=int(input_tokens) // len(batch_items),\n",
    "                output_tokens=int(output_tokens) // len(batch_items)\n",
    "            ))\n",
    "        \n",
    "        return assessment_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR in Google batch assessment: {e}\")\n",
    "        return []\n",
    "\n",
    "# Add to class\n",
    "SoftwareMethodAssessor.assess_batch_with_google = assess_batch_with_google\n",
    "\n",
    "print(\"✓ Google assessment method added\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0fe144c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Batch creation method corrected\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# BATCH CREATION METHOD (CORRECTED)\n",
    "# =============================================================================\n",
    "\n",
    "def create_batches(self, software_list: List[str], method_list: List[str],\n",
    "                  strategy: str = \"by_software\", batch_size: int = 50) -> List[List[Tuple[str, str]]]:\n",
    "    \"\"\"\n",
    "    Create batches of (software, method) pairs\n",
    "    \n",
    "    Args:\n",
    "        software_list: List of software names\n",
    "        method_list: List of methods\n",
    "        strategy: Batching strategy\n",
    "        batch_size: Size for fixed_size batching\n",
    "    \n",
    "    Returns:\n",
    "        List of batches\n",
    "    \"\"\"\n",
    "    batches = []\n",
    "    \n",
    "    if strategy == \"by_software\":\n",
    "        # One batch per software with all its methods\n",
    "        for software in software_list:\n",
    "            batch = [(software, method) for method in method_list]\n",
    "            batches.append(batch)\n",
    "    \n",
    "    elif strategy == \"by_method\":\n",
    "        # One batch per method with all software\n",
    "        for method in method_list:\n",
    "            batch = [(software, method) for software in software_list]\n",
    "            batches.append(batch)\n",
    "    \n",
    "    elif strategy == \"mixed\":\n",
    "        # Alternate between by_software and by_method\n",
    "        for i, software in enumerate(software_list[:len(software_list)//2 + 1]):\n",
    "            batch = [(software, method) for method in method_list]\n",
    "            batches.append(batch)\n",
    "        for method in method_list:\n",
    "            batch = [(software, method) for software in software_list[len(software_list)//2 + 1:]]\n",
    "            if batch:\n",
    "                batches.append(batch)\n",
    "    \n",
    "    elif strategy == \"fixed_size\":\n",
    "        # Create fixed-size batches across all pairs\n",
    "        all_items = [(sw, method) for sw in software_list for method in method_list]\n",
    "        # ↓↓↓ THIS IS THE FIX ↓↓↓\n",
    "        for i in range(0, len(all_items), batch_size):\n",
    "            batch = all_items[i:i + batch_size]\n",
    "            batches.append(batch)\n",
    "        # ↑↑↑ THIS IS THE FIX ↑↑↑\n",
    "    \n",
    "    return batches\n",
    "\n",
    "# Add to class\n",
    "SoftwareMethodAssessor.create_batches = create_batches\n",
    "\n",
    "print(\"✓ Batch creation method corrected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fdb48282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Main batched assessment method added\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# MAIN BATCHED ASSESSMENT METHOD\n",
    "# =============================================================================\n",
    "\n",
    "def assess_multiple_batched(self, software_list: List[str], method_list: List[str],\n",
    "                           batch_strategy: str = \"by_software\",\n",
    "                           batch_size: int = 100,\n",
    "                           overlap_percentage: float = 0.0,\n",
    "                           use_openai: bool = True,\n",
    "                           use_claude: bool = True,\n",
    "                           use_google: bool = True,\n",
    "                           openai_model: str = None,\n",
    "                           claude_model: str = \"claude-3-5-haiku-20241022\",\n",
    "                           google_model: str = \"models/gemini-2.0-flash\",\n",
    "                           debug: bool = False) -> List[ConsensusResult]:\n",
    "    \"\"\"\n",
    "    Assess multiple software-method combinations using batch processing\n",
    "    \"\"\"\n",
    "    total_items = len(software_list) * len(method_list)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"BATCH ASSESSMENT MODE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total items: {total_items}\")\n",
    "    print(f\"Strategy: {batch_strategy}\")\n",
    "    print(f\"LLMs: OpenAI={use_openai}, Claude={use_claude}, Google={use_google}\")\n",
    "    \n",
    "    # Create batches\n",
    "    batches = self.create_batches(software_list, method_list, batch_strategy)\n",
    "    \n",
    "    print(f\"\\nCreated {len(batches)} batches\")\n",
    "    for i, batch in enumerate(batches, 1):\n",
    "        print(f\"  Batch {i}: {len(batch)} items\")\n",
    "    \n",
    "    # Store all individual assessments\n",
    "    all_assessments = {}  # (software, method) -> list of AssessmentResult\n",
    "    \n",
    "    print(f\"\\n{'-'*70}\")\n",
    "    print(f\"Processing batches...\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    \n",
    "    # Process each batch with each LLM\n",
    "    for batch_idx, batch in enumerate(batches, 1):\n",
    "        print(f\"\\n[Batch {batch_idx}/{len(batches)}] {len(batch)} items\")\n",
    "        \n",
    "        batch_results = []\n",
    "        \n",
    "        if use_openai and self.openai_client:\n",
    "            print(f\"  Assessing with OpenAI...\")\n",
    "            results = self.assess_batch_with_openai(batch, openai_model, debug)\n",
    "            batch_results.extend(results)\n",
    "            time.sleep(1)\n",
    "        \n",
    "        if use_claude and self.anthropic_client:\n",
    "            print(f\"  Assessing with Claude...\")\n",
    "            results = self.assess_batch_with_claude(batch, claude_model, debug)\n",
    "            batch_results.extend(results)\n",
    "            time.sleep(1)\n",
    "        \n",
    "        if use_google and self.google_enabled:\n",
    "            print(f\"  Assessing with Google...\")\n",
    "            results = self.assess_batch_with_google(batch, google_model, debug)\n",
    "            batch_results.extend(results)\n",
    "            time.sleep(1)\n",
    "        \n",
    "        # Store results\n",
    "        for result in batch_results:\n",
    "            key = (result.software, result.method)\n",
    "            if key not in all_assessments:\n",
    "                all_assessments[key] = []\n",
    "            all_assessments[key].append(result)\n",
    "        \n",
    "        # Show progress\n",
    "        stats = self.credit_tracker.get_stats()\n",
    "        print(f\"  Running cost: ${stats['total_cost']:.4f} ({stats['total_tokens']:,} tokens)\")\n",
    "    \n",
    "    # Create consensus results\n",
    "    print(f\"\\n{'-'*70}\")\n",
    "    print(f\"Creating consensus results...\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    \n",
    "    consensus_results = []\n",
    "    \n",
    "    for (software, method), assessments in all_assessments.items():\n",
    "        if len(assessments) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Group by LLM provider (handle duplicates)\n",
    "        by_provider = {}\n",
    "        for assessment in assessments:\n",
    "            if assessment.llm_provider not in by_provider:\n",
    "                by_provider[assessment.llm_provider] = assessment\n",
    "        \n",
    "        assessments = list(by_provider.values())\n",
    "        \n",
    "        ranks = [a.rank for a in assessments]\n",
    "        confidence, agreement_level = self.calculate_confidence(ranks)\n",
    "        \n",
    "        rank_counts = Counter(ranks)\n",
    "        final_rank = rank_counts.most_common(1)[0][0]\n",
    "        \n",
    "        individual_ranks = {a.llm_provider: a.rank for a in assessments}\n",
    "        individual_reasoning = {a.llm_provider: a.reasoning for a in assessments}\n",
    "        individual_sources = {a.llm_provider: a.sources for a in assessments}\n",
    "        \n",
    "        total_tokens = sum(a.input_tokens + a.output_tokens for a in assessments)\n",
    "        \n",
    "        # Calculate cost\n",
    "        total_cost = sum([\n",
    "            self.credit_tracker.PRICING.get(\n",
    "                a.llm_provider.replace('openai_', '').replace('claude_', '').replace('google_', ''),\n",
    "                {'input': 0, 'output': 0}\n",
    "            )['input'] * a.input_tokens / 1_000_000 +\n",
    "            self.credit_tracker.PRICING.get(\n",
    "                a.llm_provider.replace('openai_', '').replace('claude_', '').replace('google_', ''),\n",
    "                {'input': 0, 'output': 0}\n",
    "            )['output'] * a.output_tokens / 1_000_000\n",
    "            for a in assessments\n",
    "        ])\n",
    "        \n",
    "        consensus_results.append(ConsensusResult(\n",
    "            software=software,\n",
    "            method=method,\n",
    "            final_rank=final_rank,\n",
    "            confidence=confidence,\n",
    "            individual_ranks=individual_ranks,\n",
    "            individual_reasoning=individual_reasoning,\n",
    "            individual_sources=individual_sources,\n",
    "            agreement_level=agreement_level,\n",
    "            total_tokens=total_tokens,\n",
    "            total_cost=total_cost\n",
    "        ))\n",
    "    \n",
    "    print(f\"\\nCompleted {len(consensus_results)} assessments\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return consensus_results\n",
    "\n",
    "# Add to class\n",
    "SoftwareMethodAssessor.assess_multiple_batched = assess_multiple_batched\n",
    "\n",
    "print(\"✓ Main batched assessment method added\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cccc9c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Result merger added\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# RESULT MERGER\n",
    "# =============================================================================\n",
    "\n",
    "def merge_assessment_results(self, *result_files: str, output_file: str = \"merged_results.json\",\n",
    "                            merge_strategy: str = \"union\") -> List[ConsensusResult]:\n",
    "    \"\"\"\n",
    "    Merge multiple assessment result JSON files\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"MERGING ASSESSMENT RESULTS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Strategy: {merge_strategy}\")\n",
    "    print(f\"Input files: {len(result_files)}\")\n",
    "    \n",
    "    merged_data = {}\n",
    "    \n",
    "    for file_idx, file_path in enumerate(result_files, 1):\n",
    "        print(f\"\\nProcessing file {file_idx}/{len(result_files)}: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                results = json.load(f)\n",
    "            \n",
    "            print(f\"  Loaded {len(results)} assessments\")\n",
    "            \n",
    "            for result in results:\n",
    "                software = result['software']\n",
    "                method = result['method']\n",
    "                key = (software, method)\n",
    "                \n",
    "                if key not in merged_data:\n",
    "                    merged_data[key] = result\n",
    "                else:\n",
    "                    # Merge: combine all LLM assessments\n",
    "                    merged_data[key]['individual_ranks'].update(result['individual_ranks'])\n",
    "                    merged_data[key]['individual_reasoning'].update(result['individual_reasoning'])\n",
    "                    merged_data[key]['individual_sources'].update(result['individual_sources'])\n",
    "                    merged_data[key]['total_tokens'] += result['total_tokens']\n",
    "                    merged_data[key]['total_cost'] += result['total_cost']\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR loading {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Recalculate consensus\n",
    "    print(f\"\\nRecalculating consensus for merged results...\")\n",
    "    merged_results_list = list(merged_data.values())\n",
    "    \n",
    "    for result in merged_results_list:\n",
    "        ranks = list(result['individual_ranks'].values())\n",
    "        rank_counts = Counter(ranks)\n",
    "        result['final_rank'] = rank_counts.most_common(1)[0][0]\n",
    "        \n",
    "        # Calculate confidence\n",
    "        most_common_count = rank_counts.most_common(1)[0][1]\n",
    "        result['confidence'] = most_common_count / len(ranks)\n",
    "        \n",
    "        if result['confidence'] == 1.0:\n",
    "            result['agreement_level'] = \"perfect_agreement\"\n",
    "        elif result['confidence'] >= 0.75:\n",
    "            result['agreement_level'] = \"strong_agreement\"\n",
    "        elif result['confidence'] >= 0.5:\n",
    "            result['agreement_level'] = \"moderate_agreement\"\n",
    "        else:\n",
    "            result['agreement_level'] = \"weak_agreement\"\n",
    "    \n",
    "    # Save merged results\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(merged_results_list, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✓ Merged results saved to: {output_file}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Convert to ConsensusResult objects\n",
    "    consensus_results = []\n",
    "    for result in merged_results_list:\n",
    "        consensus_results.append(ConsensusResult(\n",
    "            software=result['software'],\n",
    "            method=result['method'],\n",
    "            final_rank=result['final_rank'],\n",
    "            confidence=result['confidence'],\n",
    "            individual_ranks=result['individual_ranks'],\n",
    "            individual_reasoning=result['individual_reasoning'],\n",
    "            individual_sources=result['individual_sources'],\n",
    "            agreement_level=result['agreement_level'],\n",
    "            total_tokens=result['total_tokens'],\n",
    "            total_cost=result['total_cost']\n",
    "        ))\n",
    "    \n",
    "    return consensus_results\n",
    "\n",
    "# Add to class\n",
    "SoftwareMethodAssessor.merge_assessment_results = merge_assessment_results\n",
    "\n",
    "print(\"✓ Result merger added\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf323714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 35 software\n",
      "✓ Loaded 189 methods\n",
      "✓ Total pairs to assess: 6615\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# LOAD YOUR SOFTWARE AND METHOD LISTS\n",
    "# =============================================================================\n",
    "\n",
    "# Replace with your actual data loading\n",
    "# Example:\n",
    "# software_list_all = pd.read_csv('software_list.csv')['Name'].tolist()\n",
    "# method_list_all = pd.read_csv('method_list.csv')['Method'].tolist()\n",
    "\n",
    "# For testing, here's a placeholder:\n",
    "software_list_all = [\n",
    "    'Power Factory Digisilent','DINIS','ERACS','Distribution Network Analysis - ETAP','IPSA',\n",
    "'Power World','PSS/E','PSSE/SINCAL','SKM Power Tools','OpenDSS','Matlab & Simulink','DYMOLA','MathPower',\n",
    "'RelyPES','GridLAB-D','PyPSA (Python for Power System Analysis)','TARA','PyPower/Pandapower','GridCal Sk','MatDyn',\n",
    "'NEPLAN','PSAT','CYMEDIST','Synergi Electric','Dynawo','OpenModellica',\n",
    "'Sienna(PowerModels.jl PowerSystems.jl & PowerSimulations.jl PowerFlows.jl)','POWSYBL','Hitachi Network Manager','Spectrum Power',\n",
    "'CIMPLICITY Scada','eTerra','Netbas','Trimble NIS','GAMS'\n",
    "]\n",
    "\n",
    "method_list_all = [\n",
    "'power flow analysis','security-constrained optimal power flow','security constrained unit commitment',\n",
    "'Non Linear Optimal Power Flow','Multi-Period  Optimisation ','unit commitment','genetic algorithm','neural network',\n",
    "'kalman filter','monte-carlo','random forest','deep-learning','particle swarm optimization','fuzzy logic','time series',\n",
    "'artificial bee colony','stochastic simulation','fault analysis','reinforcement learning','linear programming','mixed integer linear programming',\n",
    "'support vector machine','ensemble-learning','graph-neural network','numerical solvers','global optimization','economic dispatch ED',\n",
    "'probabilistic-forecasting','General Optimization','data envelopment analysis','machine learning','deep neural network','voltage stability',\n",
    "'probabilistic analysis','real-time data analysis','optimal power flow','demand response','optimal capacity configuration','sensitivity analysis',\n",
    "'sequential monte carlo','fuzzy logic','load forecasting','load balancing','power forecasting','state estimation','hosting capacity',\n",
    "'error estimation techniques','stochastic model','failure modeling','loss of load expectancy','system identification','economic dispatch',\n",
    "'time series analysis','multi-objective optimization','expected energy not served','power system flexibility','decision tree',\n",
    "'contingency analysis','load frequency control','power factor correction','voltage control strategy','multi-agent system',\n",
    "'system average interruption duration index','dynamic line rating','static var compensator','dynamic programming','model predictive control',\n",
    "'k-means clustering','linear regression','principal component analysis','fault detection classification',\n",
    "'system average interruption frequency index','stochastic optimization','cost-benefit analysis','fuzzy inference system',\n",
    "'differential evolution','multi-state model','fault tree analysis','reliability economics','short-term load forecasting',\n",
    "'dynamic voltage restorer','dynamic reactive power compensation','shunt active power filter','fault detection diagnosis',\n",
    "'phase-locked loop','power system restoration','load carrying capability elcc','wind power prediction','discrete wavelet transform',\n",
    "'dynamic resource allocation','space vector pulse width modulation','logistic regression','game theory','binary particle swarm',\n",
    "'power system stabilizer','firefly algorithm','sliding mode control','modified ieee rts','heuristic optimization','partial discharge pd',\n",
    "'stochastic programming','simulated annealing','support vector regression','two-stage stochastic','adaptive neuro-fuzzy inference',\n",
    "'predictive modeling','short-term memory lstm network','load shifting','cuckoo search','automatic generation control agc','quantum computing',\n",
    "'power quality disturbance','doubly-fed induction','convolutional neural network cnns','empirical mode decomposition','evolution algorithm',\n",
    "'deep reinforcement learning drl','minimal cut set','tabu search','generative adversarial network','gated recurrent unit',\n",
    "'approximate computing','demand side management dsm','frequency variation','markov chain monte carlo','ant colony optimization',\n",
    "'predictive controller','multi-objective particle swarm optimization','power generation modeling','quantile regression','dynamic pricing',\n",
    "'wavelet transform dwt','modal analysis','power quality assessment','reactive power sharing','quadratic programming','stochastic unit commitment',\n",
    "'interior point method','process regression','second-order cone','energy resilience analysis','metaheuristics','bayesian optimization',\n",
    "'clustering analysis','power transfer distribution factor','harmony search','optimization gwo','fuzzy comprehensive evaluation',\n",
    "'deep deterministic','gaussian process regression','svd','bat algorithm','cumulative distribution function','deep deterministic policy gradient',\n",
    "'genetic programming','sequential quadratic programming','energy demand forecasting','supply chain optimization','levelized cost of energy lcoe',\n",
    "'frequency nadir','multi-output','hybrid system modeling','proton exchange membrane','hybrid acdc microgrid','multiple-input-multiple-output mimo',\n",
    "'alternating direction method','hybrid optimization model','load shedding analysis','non-dominated sorting genetic','deep q-network',\n",
    "'line outage distribution factor','multi-criteria decision analysis','closed-form expression','energy transition modeling','point estimate method',\n",
    "'signal noise ratio','agent-based modeling','environmental impact assessment','data-driven optimization','energy consumption modeling',\n",
    "'state-space modeling','quadrature pase shift keying','multi-fidelity model','stochastic geometry','quadrature amplitude modulation',\n",
    "'orthogonal frequency-division multiplexing','minimum mean square','adaptive modulation','error rate ber performance']\n",
    "\n",
    "\n",
    "\n",
    "print(f\"✓ Loaded {len(software_list_all)} software\")\n",
    "print(f\"✓ Loaded {len(method_list_all)} methods\")\n",
    "print(f\"✓ Total pairs to assess: {len(software_list_all) * len(method_list_all)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "95e10bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RUN 1: OpenAI + Google (batch_size=20)\n",
      "======================================================================\n",
      "Total pairs: 6615\n",
      "Total batches: 331\n",
      "Expected time: 6-8 hours\n",
      "======================================================================\n",
      "\n",
      "[Batch 1/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 190 column 18 (char 17028)\n",
      " ✓ 0\n",
      "[Batch 2/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 192 column 18 (char 18012)\n",
      " ✓ 0\n",
      "[Batch 3/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 4/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 180 column 5 (char 17366)\n",
      " ✓ 0\n",
      "[Batch 5/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 6/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 7/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 179 column 7 (char 18159)\n",
      " ✓ 0\n",
      "[Batch 8/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Expecting property name enclosed in double quotes: line 175 column 574 (char 17896)\n",
      " ✓ 0\n",
      "[Batch 9/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 166 column 5 (char 17782)\n",
      " ✓ 0\n",
      "[Batch 10/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 186 column 18 (char 16738)\n",
      " ✓ 0\n",
      "  Progress: $0.02 | 199 unique pairs\n",
      "[Batch 11/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 12/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 176 column 18 (char 18643)\n",
      " ✓ 0\n",
      "[Batch 13/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 14/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 15/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Expecting property name enclosed in double quotes: line 192 column 4 (char 16876)\n",
      " ✓ 0\n",
      "[Batch 16/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 17/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 18/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 19/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 20/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "  Progress: $0.04 | 400 unique pairs\n",
      "[Batch 21/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 22/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 186 column 18 (char 17518)\n",
      " ✓ 0\n",
      "[Batch 23/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 24/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Expecting value: line 187 column 15 (char 18091)\n",
      " ✓ 0\n",
      "[Batch 25/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 196 column 18 (char 15660)\n",
      " ✓ 0\n",
      "[Batch 26/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 27/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 28/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 176 column 18 (char 17647)\n",
      " ✓ 0\n",
      "[Batch 29/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 30/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "  Progress: $0.06 | 620 unique pairs\n",
      "[Batch 31/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 196 column 18 (char 17687)\n",
      " ✓ 0\n",
      "[Batch 32/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 33/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 34/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 35/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 36/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 37/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 179 column 7 (char 16462)\n",
      " ✓ 0\n",
      "[Batch 38/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 39/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 40/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "  Progress: $0.08 | 935 unique pairs\n",
      "[Batch 41/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 42/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 43/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Expecting value: line 197 column 17 (char 17352)\n",
      " ✓ 0\n",
      "[Batch 44/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 45/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 185 column 5 (char 17165)\n",
      " ✓ 0\n",
      "[Batch 46/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 47/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 186 column 18 (char 17225)\n",
      " ✓ 0\n",
      "[Batch 48/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 49/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 196 column 7 (char 17854)\n",
      " ✓ 0\n",
      "[Batch 50/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "  Progress: $0.10 | 1135 unique pairs\n",
      "  💾 Checkpoint saved: checkpoint_50.pkl\n",
      "[Batch 51/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 182 column 7 (char 17623)\n",
      " ✓ 0\n",
      "[Batch 52/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 53/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 54/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 55/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 56/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 57/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 198 column 7 (char 16977)\n",
      " ✓ 0\n",
      "[Batch 58/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 59/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 190 column 18 (char 15749)\n",
      " ✓ 0\n",
      "[Batch 60/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 171 column 7 (char 16519)\n",
      " ✓ 0\n",
      "  Progress: $0.12 | 1334 unique pairs\n",
      "[Batch 61/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 176 column 18 (char 15863)\n",
      " ✓ 0\n",
      "[Batch 62/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 63/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 199 column 18 (char 14870)\n",
      " ✓ 0\n",
      "[Batch 64/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 166 column 7 (char 15273)\n",
      " ✓ 0\n",
      "[Batch 65/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 169 column 7 (char 15715)\n",
      " ✓ 0\n",
      "[Batch 66/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 67/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 174 column 15 (char 15304)\n",
      " ✓ 0\n",
      "[Batch 68/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 189 column 7 (char 15469)\n",
      " ✓ 0\n",
      "[Batch 69/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 189 column 7 (char 16162)\n",
      " ✓ 0\n",
      "[Batch 70/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Expecting property name enclosed in double quotes: line 177 column 340 (char 16524)\n",
      " ✓ 0\n",
      "  Progress: $0.15 | 1534 unique pairs\n",
      "[Batch 71/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 166 column 18 (char 16045)\n",
      " ✓ 0\n",
      "[Batch 72/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 146 column 18 (char 14769)\n",
      " ✓ 0\n",
      "[Batch 73/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 146 column 18 (char 16278)\n",
      " ✓ 0\n",
      "[Batch 74/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 186 column 18 (char 14648)\n",
      " ✓ 0\n",
      "[Batch 75/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 156 column 18 (char 15504)\n",
      " ✓ 0\n",
      "[Batch 76/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 168 column 7 (char 16443)\n",
      " ✓ 0\n",
      "[Batch 77/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 78/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 79/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 80/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "  Progress: $0.17 | 1733 unique pairs\n",
      "[Batch 81/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 82/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 83/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 84/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 85/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 86/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Expecting ',' delimiter: line 199 column 105 (char 16809)\n",
      " ✓ 0\n",
      "[Batch 87/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 171 column 18 (char 17035)\n",
      " ✓ 0\n",
      "[Batch 88/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 89/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 90/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "  Progress: $0.19 | 1933 unique pairs\n",
      "[Batch 91/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 92/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 93/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 148 column 7 (char 17370)\n",
      " ✓ 0\n",
      "[Batch 94/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 184 column 15 (char 17120)\n",
      " ✓ 0\n",
      "[Batch 95/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 96/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 97/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 98/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 179 column 7 (char 18010)\n",
      " ✓ 0\n",
      "[Batch 99/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 171 column 18 (char 17324)\n",
      " ✓ 0\n",
      "[Batch 100/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 195 column 18 (char 16796)\n",
      " ✓ 0\n",
      "  Progress: $0.21 | 2134 unique pairs\n",
      "  💾 Checkpoint saved: checkpoint_100.pkl\n",
      "[Batch 101/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 102/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Expecting value: line 197 column 125 (char 16272)\n",
      " ✓ 0\n",
      "[Batch 103/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 104/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 105/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 189 column 7 (char 16813)\n",
      " ✓ 0\n",
      "[Batch 106/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 107/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 184 column 18 (char 16834)\n",
      " ✓ 0\n",
      "[Batch 108/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Expecting value: line 188 column 148 (char 17756)\n",
      " ✓ 0\n",
      "[Batch 109/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 110/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 196 column 18 (char 16131)\n",
      " ✓ 0\n",
      "  Progress: $0.23 | 2334 unique pairs\n",
      "[Batch 111/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Expecting ',' delimiter: line 193 column 148 (char 16516)\n",
      " ✓ 0\n",
      "[Batch 112/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 113/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 114/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 198 column 7 (char 16676)\n",
      " ✓ 0\n",
      "[Batch 115/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 116/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 117/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 118/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 119/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 120/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "  Progress: $0.25 | 2533 unique pairs\n",
      "[Batch 121/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 122/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 123/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 124/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 125/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 126/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 127/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 128/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 129/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 130/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Expecting ',' delimiter: line 176 column 180 (char 13842)\n",
      " ✓ 0\n",
      "  Progress: $0.27 | 2734 unique pairs\n",
      "[Batch 131/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 132/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 196 column 18 (char 17572)\n",
      " ✓ 0\n",
      "[Batch 133/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 134/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 135/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 136/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 137/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 138/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 139/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 193 column 17 (char 16065)\n",
      " ✓ 0\n",
      "[Batch 140/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 199 column 7 (char 15586)\n",
      " ✓ 0\n",
      "  Progress: $0.29 | 2934 unique pairs\n",
      "[Batch 141/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 198 column 7 (char 16323)\n",
      " ✓ 0\n",
      "[Batch 142/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 143/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 144/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 145/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 182 column 7 (char 18047)\n",
      " ✓ 0\n",
      "[Batch 146/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 191 column 18 (char 16545)\n",
      " ✓ 0\n",
      "[Batch 147/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 174 column 18 (char 17901)\n",
      " ✓ 0\n",
      "[Batch 148/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 149/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 177 column 5 (char 17818)\n",
      " ✓ 0\n",
      "[Batch 150/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "  Progress: $0.31 | 3218 unique pairs\n",
      "  💾 Checkpoint saved: checkpoint_150.pkl\n",
      "[Batch 151/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 152/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 194 column 7 (char 17307)\n",
      " ✓ 0\n",
      "[Batch 153/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 154/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 155/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 156/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 157/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 186 column 18 (char 16499)\n",
      " ✓ 0\n",
      "[Batch 158/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 159/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 160/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 189 column 7 (char 17234)\n",
      " ✓ 0\n",
      "  Progress: $0.33 | 3437 unique pairs\n",
      "[Batch 161/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 162/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 185 column 18 (char 15956)\n",
      " ✓ 0\n",
      "[Batch 163/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 175 column 5 (char 17083)\n",
      " ✓ 0\n",
      "[Batch 164/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 169 column 18 (char 16338)\n",
      " ✓ 0\n",
      "[Batch 165/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 180 column 18 (char 16367)\n",
      " ✓ 0\n",
      "[Batch 166/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 167/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 168/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 169/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 184 column 7 (char 17599)\n",
      " ✓ 0\n",
      "[Batch 170/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 187 column 7 (char 16982)\n",
      " ✓ 0\n",
      "  Progress: $0.35 | 3638 unique pairs\n",
      "[Batch 171/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 172/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 173/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 174/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 175/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Expecting ',' delimiter: line 196 column 114 (char 13443)\n",
      " ✓ 0\n",
      "[Batch 176/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 177/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 178/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 179/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 180/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "  Progress: $0.37 | 3839 unique pairs\n",
      "[Batch 181/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 182/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 198 column 7 (char 17227)\n",
      " ✓ 0\n",
      "[Batch 183/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 198 column 7 (char 18275)\n",
      " ✓ 0\n",
      "[Batch 184/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 185/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 186/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 196 column 18 (char 17071)\n",
      " ✓ 0\n",
      "[Batch 187/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 188/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 186 column 18 (char 17702)\n",
      " ✓ 0\n",
      "[Batch 189/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 183 column 5 (char 17267)\n",
      " ✓ 0\n",
      "[Batch 190/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "  Progress: $0.39 | 4039 unique pairs\n",
      "[Batch 191/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 192/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Expecting ',' delimiter: line 152 column 525 (char 16256)\n",
      " ✓ 0\n",
      "[Batch 193/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 194/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 195/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 196/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Expecting value: line 198 column 145 (char 17966)\n",
      " ✓ 0\n",
      "[Batch 197/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 198/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 199/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 200/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "  Progress: $0.41 | 4240 unique pairs\n",
      "  💾 Checkpoint saved: checkpoint_200.pkl\n",
      "[Batch 201/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 202/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 203/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 204/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 205/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 206/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 176 column 18 (char 16188)\n",
      " ✓ 0\n",
      "[Batch 207/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 208/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 183 column 18 (char 16747)\n",
      " ✓ 0\n",
      "[Batch 209/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 199 column 7 (char 15714)\n",
      " ✓ 0\n",
      "[Batch 210/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "  Progress: $0.43 | 4438 unique pairs\n",
      "[Batch 211/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 212/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 213/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 214/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 176 column 18 (char 16451)\n",
      " ✓ 0\n",
      "[Batch 215/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 186 column 18 (char 16107)\n",
      " ✓ 0\n",
      "[Batch 216/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 186 column 18 (char 16353)\n",
      " ✓ 0\n",
      "[Batch 217/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 185 column 18 (char 16095)\n",
      " ✓ 0\n",
      "[Batch 218/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 196 column 18 (char 16819)\n",
      " ✓ 0\n",
      "[Batch 219/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 220/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 184 column 18 (char 18056)\n",
      " ✓ 0\n",
      "  Progress: $0.45 | 4637 unique pairs\n",
      "[Batch 221/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 179 column 7 (char 17246)\n",
      " ✓ 0\n",
      "[Batch 222/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 168 column 7 (char 17469)\n",
      " ✓ 0\n",
      "[Batch 223/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 199 column 7 (char 17577)\n",
      " ✓ 0\n",
      "[Batch 224/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Expecting value: line 156 column 17 (char 17375)\n",
      " ✓ 0\n",
      "[Batch 225/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 226/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 186 column 18 (char 16641)\n",
      " ✓ 0\n",
      "[Batch 227/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 228/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 229/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 230/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "  Progress: $0.47 | 4838 unique pairs\n",
      "[Batch 231/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 186 column 18 (char 15207)\n",
      " ✓ 0\n",
      "[Batch 232/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 166 column 18 (char 16191)\n",
      " ✓ 0\n",
      "[Batch 233/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Expecting value: line 177 column 15 (char 14689)\n",
      " ✓ 0\n",
      "[Batch 234/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 176 column 18 (char 16977)\n",
      " ✓ 0\n",
      "[Batch 235/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 194 column 5 (char 16667)\n",
      " ✓ 0\n",
      "[Batch 236/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 198 column 7 (char 16779)\n",
      " ✓ 0\n",
      "[Batch 237/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Expecting ',' delimiter: line 186 column 14 (char 16138)\n",
      " ✓ 0\n",
      "[Batch 238/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 239/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 240/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 185 column 18 (char 17739)\n",
      " ✓ 0\n",
      "  Progress: $0.50 | 5037 unique pairs\n",
      "[Batch 241/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 242/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 243/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 244/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 245/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Expecting property name enclosed in double quotes: line 173 column 33 (char 16766)\n",
      " ✓ 0\n",
      "[Batch 246/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Expecting property name enclosed in double quotes: line 184 column 4 (char 17534)\n",
      " ✓ 0\n",
      "[Batch 247/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 160 column 17 (char 16638)\n",
      " ✓ 0\n",
      "[Batch 248/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 249/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 171 column 18 (char 17314)\n",
      " ✓ 0\n",
      "[Batch 250/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 173 column 17 (char 17226)\n",
      " ✓ 0\n",
      "  Progress: $0.52 | 5236 unique pairs\n",
      "  💾 Checkpoint saved: checkpoint_250.pkl\n",
      "[Batch 251/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 184 column 18 (char 16762)\n",
      " ✓ 0\n",
      "[Batch 252/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 253/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 196 column 18 (char 16737)\n",
      " ✓ 0\n",
      "[Batch 254/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 188 column 7 (char 16507)\n",
      " ✓ 0\n",
      "[Batch 255/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 256/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 257/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 258/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Expecting value: line 168 column 110 (char 17434)\n",
      " ✓ 0\n",
      "[Batch 259/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 180 column 7 (char 16820)\n",
      " ✓ 0\n",
      "[Batch 260/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "  Progress: $0.54 | 5437 unique pairs\n",
      "[Batch 261/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 188 column 17 (char 16369)\n",
      " ✓ 0\n",
      "[Batch 262/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 263/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 264/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Expecting property name enclosed in double quotes: line 185 column 15 (char 17358)\n",
      " ✓ 0\n",
      "[Batch 265/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 266/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 267/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 268/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 269/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 196 column 18 (char 17713)\n",
      " ✓ 0\n",
      "[Batch 270/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "  Progress: $0.56 | 5638 unique pairs\n",
      "[Batch 271/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 196 column 18 (char 17744)\n",
      " ✓ 0\n",
      "[Batch 272/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 273/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 274/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 275/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 179 column 7 (char 16794)\n",
      " ✓ 0\n",
      "[Batch 276/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 190 column 7 (char 17356)\n",
      " ✓ 0\n",
      "[Batch 277/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 146 column 18 (char 18147)\n",
      " ✓ 0\n",
      "[Batch 278/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Expecting value: line 170 column 12 (char 16291)\n",
      " ✓ 0\n",
      "[Batch 279/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 168 column 7 (char 16237)\n",
      " ✓ 0\n",
      "[Batch 280/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 199 column 7 (char 17094)\n",
      " ✓ 0\n",
      "  Progress: $0.58 | 5838 unique pairs\n",
      "[Batch 281/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 176 column 18 (char 18180)\n",
      " ✓ 0\n",
      "[Batch 282/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 283/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Expecting property name enclosed in double quotes: line 173 column 34 (char 17147)\n",
      " ✓ 0\n",
      "[Batch 284/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 285/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 286/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 287/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 148 column 7 (char 15164)\n",
      " ✓ 0\n",
      "[Batch 288/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 166 column 18 (char 16770)\n",
      " ✓ 0\n",
      "[Batch 289/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 169 column 7 (char 16538)\n",
      " ✓ 0\n",
      "[Batch 290/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 186 column 18 (char 16018)\n",
      " ✓ 0\n",
      "  Progress: $0.60 | 6039 unique pairs\n",
      "[Batch 291/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 156 column 18 (char 14150)\n",
      " ✓ 0\n",
      "[Batch 292/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 179 column 7 (char 16542)\n",
      " ✓ 0\n",
      "[Batch 293/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 197 column 5 (char 16125)\n",
      " ✓ 0\n",
      "[Batch 294/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 295/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 296/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 297/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 199 column 7 (char 16289)\n",
      " ✓ 0\n",
      "[Batch 298/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 299/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 300/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "  Progress: $0.62 | 6239 unique pairs\n",
      "  💾 Checkpoint saved: checkpoint_300.pkl\n",
      "[Batch 301/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 302/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 303/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Expecting value: line 168 column 186 (char 16863)\n",
      " ✓ 0\n",
      "[Batch 304/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 305/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 306/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 307/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 308/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 309/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 310/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "  Progress: $0.64 | 6438 unique pairs\n",
      "[Batch 311/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 312/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Expecting ',' delimiter: line 180 column 6 (char 18175)\n",
      " ✓ 0\n",
      "[Batch 313/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 314/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 315/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 316/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 317/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 318/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 319/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 320/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "  Progress: $0.66 | 6638 unique pairs\n",
      "[Batch 321/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 322/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 323/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 324/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 325/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 144 column 7 (char 17174)\n",
      " ✓ 0\n",
      "[Batch 326/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 327/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 178 column 5 (char 16941)\n",
      " ✓ 0\n",
      "[Batch 328/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google... ✓ 20\n",
      "[Batch 329/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Expecting property name enclosed in double quotes: line 175 column 15 (char 17346)\n",
      " ✓ 0\n",
      "[Batch 330/331] 20 items\n",
      "  OpenAI... ✓ 20\n",
      "  Google...  ERROR parsing Google batch response: Unterminated string starting at: line 188 column 18 (char 16732)\n",
      " ✓ 0\n",
      "  Progress: $0.68 | 6839 unique pairs\n",
      "[Batch 331/331] 15 items\n",
      "  OpenAI... ✓ 15\n",
      "  Google... ✓ 15\n",
      "\n",
      "======================================================================\n",
      "Creating consensus results...\n",
      "======================================================================\n",
      "\n",
      "Results exported to software_analysis_final\\run1_openai_google_20251019_015733.json\n",
      "\n",
      "============================================================\n",
      "API USAGE SUMMARY\n",
      "============================================================\n",
      "Total API Calls: 662\n",
      "Total Tokens: 2,697,446\n",
      "  - Input: 562,734\n",
      "  - Output: 2,134,712\n",
      "\n",
      "Total Cost: $0.6826\n",
      "Average Cost per Call: $0.0010\n",
      "\n",
      "Breakdown by Model:\n",
      "------------------------------------------------------------\n",
      "  gpt-4o-mini:\n",
      "    Calls: 331\n",
      "    Tokens: 1,156,214\n",
      "    Cost: $0.2849\n",
      "  models/gemini-2.0-flash:\n",
      "    Calls: 331\n",
      "    Tokens: 1,541,232\n",
      "    Cost: $0.3977\n",
      "============================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "RUN 1 COMPLETE\n",
      "======================================================================\n",
      "✓ Completed: 6855 unique assessments\n",
      "✗ Failed batches: 0\n",
      "\n",
      "Files:\n",
      "  JSON: software_analysis_final\\run1_openai_google_20251019_015733.json\n",
      "  CSV: software_analysis_final\\run1_openai_google_20251019_015733.csv\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# WORKING SOLUTION: Direct batch processing (GUARANTEED TO WORK)\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "output_dir = Path(\"software_analysis_final\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "assessor = SoftwareMethodAssessor(use_config=True, timeout=180)\n",
    "\n",
    "# Create batches manually with batch_size=20\n",
    "all_pairs = [(sw, method) for sw in software_list_all for method in method_list_all]\n",
    "batch_size = 20\n",
    "batches = [all_pairs[i:i + batch_size] for i in range(0, len(all_pairs), batch_size)]\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"RUN 1: OpenAI + Google (batch_size={batch_size})\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total pairs: {len(all_pairs)}\")\n",
    "print(f\"Total batches: {len(batches)}\")\n",
    "print(f\"Expected time: 6-8 hours\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "all_assessments = {}\n",
    "failed_batches = []\n",
    "\n",
    "for batch_idx, batch in enumerate(batches, 1):\n",
    "    print(f\"[Batch {batch_idx}/{len(batches)}] {len(batch)} items\", flush=True)\n",
    "    \n",
    "    # OpenAI\n",
    "    try:\n",
    "        print(f\"  OpenAI...\", end='', flush=True)\n",
    "        openai_results = assessor.assess_batch_with_openai(batch, 'gpt-4o-mini', debug=False)\n",
    "        for result in openai_results:\n",
    "            key = (result.software, result.method)\n",
    "            if key not in all_assessments:\n",
    "                all_assessments[key] = []\n",
    "            all_assessments[key].append(result)\n",
    "        print(f\" ✓ {len(openai_results)}\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\" ✗ {str(e)[:50]}\", flush=True)\n",
    "        failed_batches.append(('openai', batch_idx))\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Google\n",
    "    try:\n",
    "        print(f\"  Google...\", end='', flush=True)\n",
    "        google_results = assessor.assess_batch_with_google(batch, 'models/gemini-2.0-flash', debug=False)\n",
    "        for result in google_results:\n",
    "            key = (result.software, result.method)\n",
    "            if key not in all_assessments:\n",
    "                all_assessments[key] = []\n",
    "            all_assessments[key].append(result)\n",
    "        print(f\" ✓ {len(google_results)}\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\" ✗ {str(e)[:50]}\", flush=True)\n",
    "        failed_batches.append(('google', batch_idx))\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Progress\n",
    "    if batch_idx % 10 == 0:\n",
    "        stats = assessor.credit_tracker.get_stats()\n",
    "        print(f\"  Progress: ${stats['total_cost']:.2f} | {len(all_assessments)} unique pairs\")\n",
    "    \n",
    "    # Checkpoint every 50 batches\n",
    "    if batch_idx % 50 == 0:\n",
    "        checkpoint_file = output_dir / f\"checkpoint_{batch_idx}.pkl\"\n",
    "        with open(checkpoint_file, 'wb') as f:\n",
    "            pickle.dump(all_assessments, f)\n",
    "        print(f\"  💾 Checkpoint saved: {checkpoint_file.name}\")\n",
    "\n",
    "# Create consensus\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Creating consensus results...\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "consensus_results = []\n",
    "for (software, method), assessments in all_assessments.items():\n",
    "    # Remove duplicates by provider\n",
    "    by_provider = {}\n",
    "    for a in assessments:\n",
    "        if a.llm_provider not in by_provider:\n",
    "            by_provider[a.llm_provider] = a\n",
    "    assessments = list(by_provider.values())\n",
    "    \n",
    "    if len(assessments) == 0:\n",
    "        continue\n",
    "    \n",
    "    ranks = [a.rank for a in assessments]\n",
    "    confidence, agreement_level = assessor.calculate_confidence(ranks)\n",
    "    rank_counts = Counter(ranks)\n",
    "    final_rank = rank_counts.most_common(1)[0][0]\n",
    "    \n",
    "    consensus_results.append(ConsensusResult(\n",
    "        software=software,\n",
    "        method=method,\n",
    "        final_rank=final_rank,\n",
    "        confidence=confidence,\n",
    "        individual_ranks={a.llm_provider: a.rank for a in assessments},\n",
    "        individual_reasoning={a.llm_provider: a.reasoning for a in assessments},\n",
    "        individual_sources={a.llm_provider: a.sources for a in assessments},\n",
    "        agreement_level=agreement_level,\n",
    "        total_tokens=sum(a.input_tokens + a.output_tokens for a in assessments),\n",
    "        total_cost=0.0\n",
    "    ))\n",
    "\n",
    "# Save results\n",
    "run1_file = output_dir / f\"run1_openai_google_{timestamp}.json\"\n",
    "assessor.export_results(consensus_results, str(run1_file))\n",
    "\n",
    "results_df = pd.DataFrame([{\n",
    "    'software': r.software,\n",
    "    'method': r.method,\n",
    "    'final_rank': r.final_rank,\n",
    "    'confidence': r.confidence,\n",
    "    'agreement_level': r.agreement_level,\n",
    "    'num_llms': len(r.individual_ranks)\n",
    "} for r in consensus_results])\n",
    "\n",
    "csv_file = output_dir / f\"run1_openai_google_{timestamp}.csv\"\n",
    "results_df.to_csv(csv_file, index=False)\n",
    "\n",
    "# Final summary\n",
    "assessor.credit_tracker.print_summary()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"RUN 1 COMPLETE\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"✓ Completed: {len(consensus_results)} unique assessments\")\n",
    "print(f\"✗ Failed batches: {len(failed_batches)}\")\n",
    "if failed_batches:\n",
    "    print(f\"  Failed: {failed_batches[:5]}{'...' if len(failed_batches) > 5 else ''}\")\n",
    "print(f\"\\nFiles:\")\n",
    "print(f\"  JSON: {run1_file}\")\n",
    "print(f\"  CSV: {csv_file}\")\n",
    "print(f\"{'='*70}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c9146f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RUN 2: Claude + Google Assessment\n",
      "======================================================================\n",
      "NOTE: Google appears in both runs for validation\n",
      "\n",
      "======================================================================\n",
      "BATCH ASSESSMENT MODE\n",
      "======================================================================\n",
      "Total items: 6615\n",
      "Strategy: by_software\n",
      "LLMs: OpenAI=False, Claude=True, Google=True\n",
      "\n",
      "Created 35 batches\n",
      "  Batch 1: 189 items\n",
      "  Batch 2: 189 items\n",
      "  Batch 3: 189 items\n",
      "  Batch 4: 189 items\n",
      "  Batch 5: 189 items\n",
      "  Batch 6: 189 items\n",
      "  Batch 7: 189 items\n",
      "  Batch 8: 189 items\n",
      "  Batch 9: 189 items\n",
      "  Batch 10: 189 items\n",
      "  Batch 11: 189 items\n",
      "  Batch 12: 189 items\n",
      "  Batch 13: 189 items\n",
      "  Batch 14: 189 items\n",
      "  Batch 15: 189 items\n",
      "  Batch 16: 189 items\n",
      "  Batch 17: 189 items\n",
      "  Batch 18: 189 items\n",
      "  Batch 19: 189 items\n",
      "  Batch 20: 189 items\n",
      "  Batch 21: 189 items\n",
      "  Batch 22: 189 items\n",
      "  Batch 23: 189 items\n",
      "  Batch 24: 189 items\n",
      "  Batch 25: 189 items\n",
      "  Batch 26: 189 items\n",
      "  Batch 27: 189 items\n",
      "  Batch 28: 189 items\n",
      "  Batch 29: 189 items\n",
      "  Batch 30: 189 items\n",
      "  Batch 31: 189 items\n",
      "  Batch 32: 189 items\n",
      "  Batch 33: 189 items\n",
      "  Batch 34: 189 items\n",
      "  Batch 35: 189 items\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Processing batches...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "[Batch 1/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Unterminated string starting at: line 217 column 15 (char 17584)\n",
      "  Running cost: $0.0056 (12,671 tokens)\n",
      "\n",
      "[Batch 2/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Expecting property name enclosed in double quotes: line 246 column 289 (char 15200)\n",
      "  Running cost: $0.0105 (23,926 tokens)\n",
      "\n",
      "[Batch 3/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Unterminated string starting at: line 307 column 18 (char 16601)\n",
      "  Running cost: $0.0154 (35,206 tokens)\n",
      "\n",
      "[Batch 4/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Unterminated string starting at: line 297 column 18 (char 15809)\n",
      "  Running cost: $0.0209 (47,941 tokens)\n",
      "\n",
      "[Batch 5/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Unterminated string starting at: line 249 column 7 (char 15963)\n",
      "  Running cost: $0.0256 (59,188 tokens)\n",
      "\n",
      "[Batch 6/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Unterminated string starting at: line 210 column 18 (char 17088)\n",
      "  Running cost: $0.0304 (70,294 tokens)\n",
      "\n",
      "[Batch 7/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Unterminated string starting at: line 207 column 18 (char 15229)\n",
      "  Running cost: $0.0352 (81,835 tokens)\n",
      "\n",
      "[Batch 8/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Unterminated string starting at: line 186 column 18 (char 14535)\n",
      "  Running cost: $0.0406 (94,286 tokens)\n",
      "\n",
      "[Batch 9/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Unterminated string starting at: line 269 column 7 (char 15380)\n",
      "  Running cost: $0.0457 (106,141 tokens)\n",
      "\n",
      "[Batch 10/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Unterminated string starting at: line 241 column 18 (char 15574)\n",
      "  Running cost: $0.0506 (117,545 tokens)\n",
      "\n",
      "[Batch 11/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Unterminated string starting at: line 223 column 17 (char 16430)\n",
      "  Running cost: $0.0560 (129,769 tokens)\n",
      "\n",
      "[Batch 12/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Unterminated string starting at: line 206 column 18 (char 16146)\n",
      "  Running cost: $0.0610 (141,412 tokens)\n",
      "\n",
      "[Batch 13/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Unterminated string starting at: line 226 column 5 (char 16899)\n",
      "  Running cost: $0.0659 (152,825 tokens)\n",
      "\n",
      "[Batch 14/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Unterminated string starting at: line 315 column 5 (char 15515)\n",
      "  Running cost: $0.0708 (164,257 tokens)\n",
      "\n",
      "[Batch 15/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Expecting ',' delimiter: line 220 column 6 (char 15692)\n",
      "  Running cost: $0.0763 (176,510 tokens)\n",
      "\n",
      "[Batch 16/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Expecting property name enclosed in double quotes: line 233 column 361 (char 16069)\n",
      "  Running cost: $0.0823 (190,624 tokens)\n",
      "\n",
      "[Batch 17/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Expecting property name enclosed in double quotes: line 333 column 24 (char 15258)\n",
      "  Running cost: $0.0874 (201,957 tokens)\n",
      "\n",
      "[Batch 18/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Unterminated string starting at: line 226 column 18 (char 14609)\n",
      "  Running cost: $0.0931 (214,657 tokens)\n",
      "\n",
      "[Batch 19/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Expecting value: line 306 column 12 (char 16888)\n",
      "  Running cost: $0.0984 (226,600 tokens)\n",
      "\n",
      "[Batch 20/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Unterminated string starting at: line 286 column 18 (char 17907)\n",
      "  Running cost: $0.1033 (237,866 tokens)\n",
      "\n",
      "[Batch 21/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Unterminated string starting at: line 266 column 18 (char 16562)\n",
      "  Running cost: $0.1081 (249,124 tokens)\n",
      "\n",
      "[Batch 22/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Unterminated string starting at: line 272 column 18 (char 16097)\n",
      "  Running cost: $0.1130 (260,412 tokens)\n",
      "\n",
      "[Batch 23/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Unterminated string starting at: line 228 column 7 (char 14996)\n",
      "  Running cost: $0.1179 (272,010 tokens)\n",
      "\n",
      "[Batch 24/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Expecting property name enclosed in double quotes: line 193 column 36 (char 16657)\n",
      "  Running cost: $0.1230 (283,795 tokens)\n",
      "\n",
      "[Batch 25/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Unterminated string starting at: line 270 column 18 (char 15718)\n",
      "  Running cost: $0.1281 (295,250 tokens)\n",
      "\n",
      "[Batch 26/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Unterminated string starting at: line 207 column 18 (char 17164)\n",
      "  Running cost: $0.1332 (306,927 tokens)\n",
      "\n",
      "[Batch 27/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Unterminated string starting at: line 209 column 5 (char 16456)\n",
      "  Running cost: $0.1426 (326,900 tokens)\n",
      "\n",
      "[Batch 28/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Unterminated string starting at: line 277 column 5 (char 15154)\n",
      "  Running cost: $0.1480 (338,706 tokens)\n",
      "\n",
      "[Batch 29/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Unterminated string starting at: line 213 column 17 (char 17799)\n",
      "  Running cost: $0.1532 (350,543 tokens)\n",
      "\n",
      "[Batch 30/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Unterminated string starting at: line 250 column 7 (char 16284)\n",
      "  Running cost: $0.1581 (361,832 tokens)\n",
      "\n",
      "[Batch 31/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Unterminated string starting at: line 183 column 17 (char 16737)\n",
      "  Running cost: $0.1638 (374,584 tokens)\n",
      "\n",
      "[Batch 32/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Expecting ',' delimiter: line 255 column 14 (char 17039)\n",
      "  Running cost: $0.1689 (385,866 tokens)\n",
      "\n",
      "[Batch 33/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Unterminated string starting at: line 368 column 7 (char 15822)\n",
      "  Running cost: $0.1739 (397,329 tokens)\n",
      "\n",
      "[Batch 34/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Unterminated string starting at: line 228 column 7 (char 17146)\n",
      "  Running cost: $0.1793 (408,987 tokens)\n",
      "\n",
      "[Batch 35/35] 189 items\n",
      "  Assessing with Claude...\n",
      "  ERROR parsing Claude batch response: Expecting value: line 1 column 1 (char 0)\n",
      "  Assessing with Google...\n",
      "  ERROR parsing Google batch response: Unterminated string starting at: line 203 column 18 (char 15267)\n",
      "  Running cost: $0.1842 (420,259 tokens)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Creating consensus results...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Completed 0 assessments\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Results exported to software_analysis_final\\run2_claude_google_20251019_015733.json\n",
      "\n",
      "============================================================\n",
      "API USAGE SUMMARY\n",
      "============================================================\n",
      "Total API Calls: 70\n",
      "Total Tokens: 420,259\n",
      "  - Input: 272,517\n",
      "  - Output: 147,742\n",
      "\n",
      "Total Cost: $0.1842\n",
      "Average Cost per Call: $0.0026\n",
      "\n",
      "Breakdown by Model:\n",
      "------------------------------------------------------------\n",
      "  claude-3-5-haiku-20241022:\n",
      "    Calls: 35\n",
      "    Tokens: 144,610\n",
      "    Cost: $0.1314\n",
      "  models/gemini-2.0-flash:\n",
      "    Calls: 35\n",
      "    Tokens: 275,649\n",
      "    Cost: $0.0528\n",
      "============================================================\n",
      "\n",
      "\n",
      "✓ Run 2 complete!\n",
      "  JSON: software_analysis_final\\run2_claude_google_20251019_015733.json\n",
      "  CSV: software_analysis_final\\run2_claude_google_20251019_015733.csv\n",
      "  Completed 0 assessments\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# EXECUTION: RUN 2 - Claude + Google (for overlap validation)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RUN 2: Claude + Google Assessment\")\n",
    "print(\"=\"*70)\n",
    "print(\"NOTE: Google appears in both runs for validation\")\n",
    "\n",
    "# Create fresh assessor to reset token tracking\n",
    "assessor_run2 = SoftwareMethodAssessor(use_config=True, timeout=180)\n",
    "\n",
    "# Run assessment\n",
    "results_run2 = assessor_run2.assess_multiple_batched(\n",
    "    software_list=software_list_all,\n",
    "    method_list=method_list_all,\n",
    "    batch_strategy=\"by_software\",\n",
    "    use_openai=False,\n",
    "    use_google=True,  # Google overlap with Run 1\n",
    "    use_claude=True,\n",
    "    claude_model='claude-3-5-haiku-20241022',\n",
    "    google_model='models/gemini-2.0-flash'\n",
    ")\n",
    "\n",
    "# Save results\n",
    "run2_file = output_dir / f\"run2_claude_google_{timestamp}.json\"\n",
    "assessor_run2.export_results(results_run2, str(run2_file))\n",
    "\n",
    "# Save as CSV\n",
    "results_df2 = pd.DataFrame([{\n",
    "    'software': r.software,\n",
    "    'method': r.method,\n",
    "    'final_rank': r.final_rank,\n",
    "    'confidence': r.confidence,\n",
    "    'agreement_level': r.agreement_level,\n",
    "    'num_llms': len(r.individual_ranks)\n",
    "} for r in results_run2])\n",
    "\n",
    "csv_file2 = output_dir / f\"run2_claude_google_{timestamp}.csv\"\n",
    "results_df2.to_csv(csv_file2, index=False)\n",
    "\n",
    "# Print summary\n",
    "assessor_run2.credit_tracker.print_summary()\n",
    "print(f\"\\n✓ Run 2 complete!\")\n",
    "print(f\"  JSON: {run2_file}\")\n",
    "print(f\"  CSV: {csv_file2}\")\n",
    "print(f\"  Completed {len(results_run2)} assessments\")\n",
    "print(\"\\n\" + \"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6e78e825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MERGING RUN 1 AND RUN 2\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "MERGING ASSESSMENT RESULTS\n",
      "======================================================================\n",
      "Strategy: union\n",
      "Input files: 2\n",
      "\n",
      "Processing file 1/2: software_analysis_final\\run1_openai_google_20251019_015733.json\n",
      "  Loaded 6855 assessments\n",
      "\n",
      "Processing file 2/2: software_analysis_final\\run2_claude_google_20251019_015733.json\n",
      "  Loaded 0 assessments\n",
      "\n",
      "Recalculating consensus for merged results...\n",
      "\n",
      "✓ Merged results saved to: software_analysis_final\\merged_final_20251019_015733.json\n",
      "======================================================================\n",
      "\n",
      "\n",
      "✓ Merged 6855 unique assessments\n",
      "✓ Merged CSV saved to: software_analysis_final\\merged_final_20251019_015733.csv\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# MERGE BOTH RUNS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MERGING RUN 1 AND RUN 2\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create merger instance\n",
    "merger = SoftwareMethodAssessor(use_config=True)\n",
    "\n",
    "# Merge results\n",
    "merged_results = merger.merge_assessment_results(\n",
    "    str(run1_file),\n",
    "    str(run2_file),\n",
    "    output_file=str(output_dir / f\"merged_final_{timestamp}.json\"),\n",
    "    merge_strategy=\"union\"\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Merged {len(merged_results)} unique assessments\")\n",
    "\n",
    "# Create summary CSV\n",
    "merged_df = pd.DataFrame([{\n",
    "    'software': r.software,\n",
    "    'method': r.method,\n",
    "    'final_rank': r.final_rank,\n",
    "    'confidence': r.confidence,\n",
    "    'agreement_level': r.agreement_level,\n",
    "    'num_llms': len(r.individual_ranks),\n",
    "    'llms_used': ', '.join(r.individual_ranks.keys()),\n",
    "    'total_cost': r.total_cost\n",
    "} for r in merged_results])\n",
    "\n",
    "merged_csv = output_dir / f\"merged_final_{timestamp}.csv\"\n",
    "merged_df.to_csv(merged_csv, index=False)\n",
    "\n",
    "print(f\"✓ Merged CSV saved to: {merged_csv}\")\n",
    "print(\"\\n\" + \"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "39ae55c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "GOOGLE OVERLAP VALIDATION\n",
      "======================================================================\n",
      "Checking consistency of Google assessments across both runs...\n",
      "\n",
      "⚠ No Google overlap found in merged results\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# GOOGLE OVERLAP VALIDATION ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GOOGLE OVERLAP VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"Checking consistency of Google assessments across both runs...\")\n",
    "\n",
    "google_consistency = []\n",
    "\n",
    "for result in merged_results:\n",
    "    # Find Google assessments from both runs\n",
    "    google_ranks = []\n",
    "    google_providers = []\n",
    "    \n",
    "    for llm, rank in result.individual_ranks.items():\n",
    "        if 'google' in llm.lower():\n",
    "            google_ranks.append(rank)\n",
    "            google_providers.append(llm)\n",
    "    \n",
    "    if len(google_ranks) == 2:  # Google assessed twice\n",
    "        consistency_record = {\n",
    "            'software': result.software,\n",
    "            'method': result.method,\n",
    "            'google_rank_run1': google_ranks[0],\n",
    "            'google_rank_run2': google_ranks[1],\n",
    "            'difference': abs(google_ranks[0] - google_ranks[1]),\n",
    "            'consistent': google_ranks[0] == google_ranks[1],\n",
    "            'final_rank': result.final_rank,\n",
    "            'confidence': result.confidence\n",
    "        }\n",
    "        google_consistency.append(consistency_record)\n",
    "\n",
    "# Create consistency report\n",
    "consistency_df = pd.DataFrame(google_consistency)\n",
    "consistency_file = output_dir / f\"google_consistency_{timestamp}.csv\"\n",
    "consistency_df.to_csv(consistency_file, index=False)\n",
    "\n",
    "# Calculate statistics\n",
    "if len(google_consistency) > 0:\n",
    "    perfect_consistency = sum(1 for c in google_consistency if c['consistent'])\n",
    "    within_one = sum(1 for c in google_consistency if c['difference'] <= 1)\n",
    "    \n",
    "    print(f\"\\nGoogle Consistency Statistics:\")\n",
    "    print(f\"  Total pairs assessed by Google in both runs: {len(google_consistency)}\")\n",
    "    print(f\"  Perfect consistency (exact same rank): {perfect_consistency} ({perfect_consistency/len(google_consistency)*100:.1f}%)\")\n",
    "    print(f\"  Within ±1 rank: {within_one} ({within_one/len(google_consistency)*100:.1f}%)\")\n",
    "    print(f\"  Average rank difference: {consistency_df['difference'].mean():.2f}\")\n",
    "    print(f\"  Max rank difference: {consistency_df['difference'].max()}\")\n",
    "    \n",
    "    # Show distribution of differences\n",
    "    print(f\"\\nDifference distribution:\")\n",
    "    diff_counts = consistency_df['difference'].value_counts().sort_index()\n",
    "    for diff, count in diff_counts.items():\n",
    "        print(f\"    Difference {int(diff)}: {count} pairs ({count/len(google_consistency)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n✓ Consistency report saved to: {consistency_file}\")\n",
    "else:\n",
    "    print(\"\\n⚠ No Google overlap found in merged results\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7cb87251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FINAL RESULTS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Total unique pairs assessed: 6855\n",
      "\n",
      "LLM Coverage Distribution:\n",
      "  2 LLMs: 3,622 pairs (52.8%)\n",
      "  1 LLMs: 3,233 pairs (47.2%)\n",
      "\n",
      "Average Confidence: 84.99%\n",
      "\n",
      "Agreement Level Distribution:\n",
      "  perfect_agreement: 4,797 pairs (70.0%)\n",
      "  moderate_agreement: 2,058 pairs (30.0%)\n",
      "\n",
      "Rank Distribution:\n",
      "  Rank 0: 1,662 pairs (24.2%)\n",
      "  Rank 1: 2,202 pairs (32.1%)\n",
      "  Rank 2: 1,928 pairs (28.1%)\n",
      "  Rank 3: 1,063 pairs (15.5%)\n",
      "\n",
      "Cost Breakdown:\n",
      "  Run 1 (OpenAI + Google): $0.00\n",
      "  Run 2 (Claude + Google): $0.00\n",
      "  Total: $0.00\n",
      "  Average per pair: $0.0000\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# FINAL RESULTS ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# LLM coverage analysis\n",
    "print(f\"\\nTotal unique pairs assessed: {len(merged_results)}\")\n",
    "\n",
    "llm_coverage = {}\n",
    "for result in merged_results:\n",
    "    num_llms = len(result.individual_ranks)\n",
    "    llm_coverage[num_llms] = llm_coverage.get(num_llms, 0) + 1\n",
    "\n",
    "print(f\"\\nLLM Coverage Distribution:\")\n",
    "for num_llms in sorted(llm_coverage.keys(), reverse=True):\n",
    "    print(f\"  {num_llms} LLMs: {llm_coverage[num_llms]:,} pairs ({llm_coverage[num_llms]/len(merged_results)*100:.1f}%)\")\n",
    "\n",
    "# Confidence analysis\n",
    "avg_confidence = sum(r.confidence for r in merged_results) / len(merged_results)\n",
    "print(f\"\\nAverage Confidence: {avg_confidence:.2%}\")\n",
    "\n",
    "confidence_levels = Counter([r.agreement_level for r in merged_results])\n",
    "print(f\"\\nAgreement Level Distribution:\")\n",
    "for level, count in confidence_levels.most_common():\n",
    "    print(f\"  {level}: {count:,} pairs ({count/len(merged_results)*100:.1f}%)\")\n",
    "\n",
    "# Rank distribution\n",
    "rank_dist = Counter([r.final_rank for r in merged_results])\n",
    "print(f\"\\nRank Distribution:\")\n",
    "for rank in sorted(rank_dist.keys()):\n",
    "    print(f\"  Rank {rank}: {rank_dist[rank]:,} pairs ({rank_dist[rank]/len(merged_results)*100:.1f}%)\")\n",
    "\n",
    "# Cost analysis\n",
    "total_cost_run1 = sum(r.total_cost for r in results_run1)\n",
    "total_cost_run2 = sum(r.total_cost for r in results_run2)\n",
    "total_cost_combined = total_cost_run1 + total_cost_run2\n",
    "\n",
    "print(f\"\\nCost Breakdown:\")\n",
    "print(f\"  Run 1 (OpenAI + Google): ${total_cost_run1:.2f}\")\n",
    "print(f\"  Run 2 (Claude + Google): ${total_cost_run2:.2f}\")\n",
    "print(f\"  Total: ${total_cost_combined:.2f}\")\n",
    "print(f\"  Average per pair: ${total_cost_combined/len(merged_results):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f916f229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CREATING SUMMARY REPORT\n",
      "======================================================================\n",
      "✓ Summary report saved to: software_analysis_final\\_SUMMARY_20251019_015733.json\n",
      "\n",
      "======================================================================\n",
      "ALL OUTPUT FILES\n",
      "======================================================================\n",
      "\n",
      "Directory: c:\\git_repos\\Literature-search-and-analysis\\software_analysis_final\n",
      "\n",
      "Assessment Results:\n",
      "  1. run1_openai_google_20251019_015733.json - Run 1 results (JSON)\n",
      "  2. run1_openai_google_20251019_015733.csv - Run 1 results (CSV)\n",
      "  3. run2_claude_google_20251019_015733.json - Run 2 results (JSON)\n",
      "  4. run2_claude_google_20251019_015733.csv - Run 2 results (CSV)\n",
      "  5. merged_final_20251019_015733.json - Combined results (JSON)\n",
      "  6. merged_final_20251019_015733.csv - Combined results (CSV)\n",
      "\n",
      "Quality Reports:\n",
      "  7. google_consistency_20251019_015733.csv - Google validation\n",
      "  8. _SUMMARY_20251019_015733.json - Master summary\n",
      "\n",
      "======================================================================\n",
      "✓ ASSESSMENT COMPLETE!\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# CREATE COMPREHENSIVE SUMMARY REPORT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING SUMMARY REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary_report = {\n",
    "    'timestamp': timestamp,\n",
    "    'assessment_info': {\n",
    "        'total_software': len(software_list_all),\n",
    "        'total_methods': len(method_list_all),\n",
    "        'total_pairs_assessed': len(merged_results),\n",
    "        'expected_pairs': len(software_list_all) * len(method_list_all)\n",
    "    },\n",
    "    'runs': {\n",
    "        'run1': {\n",
    "            'llms': 'OpenAI + Google',\n",
    "            'pairs_assessed': len(results_run1),\n",
    "            'cost': round(total_cost_run1, 4)\n",
    "        },\n",
    "        'run2': {\n",
    "            'llms': 'Claude + Google',\n",
    "            'pairs_assessed': len(results_run2),\n",
    "            'cost': round(total_cost_run2, 4)\n",
    "        }\n",
    "    },\n",
    "    'validation': {\n",
    "        'google_overlap_pairs': len(google_consistency),\n",
    "        'google_perfect_consistency_rate': perfect_consistency/len(google_consistency) if len(google_consistency) > 0 else 0,\n",
    "        'google_within_one_rate': within_one/len(google_consistency) if len(google_consistency) > 0 else 0\n",
    "    },\n",
    "    'quality_metrics': {\n",
    "        'average_confidence': round(avg_confidence, 4),\n",
    "        'llm_coverage': {str(k): v for k, v in llm_coverage.items()},\n",
    "        'agreement_levels': {k: v for k, v in confidence_levels.items()},\n",
    "        'rank_distribution': {str(k): v for k, v in rank_dist.items()}\n",
    "    },\n",
    "    'costs': {\n",
    "        'total_cost': round(total_cost_combined, 4),\n",
    "        'cost_per_pair': round(total_cost_combined/len(merged_results), 6),\n",
    "        'run1_cost': round(total_cost_run1, 4),\n",
    "        'run2_cost': round(total_cost_run2, 4)\n",
    "    },\n",
    "    'files': {\n",
    "        'run1_json': str(run1_file.name),\n",
    "        'run1_csv': str(csv_file.name),\n",
    "        'run2_json': str(run2_file.name),\n",
    "        'run2_csv': str(csv_file2.name),\n",
    "        'merged_json': f\"merged_final_{timestamp}.json\",\n",
    "        'merged_csv': f\"merged_final_{timestamp}.csv\",\n",
    "        'consistency_report': str(consistency_file.name)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save summary report\n",
    "summary_file = output_dir / f\"_SUMMARY_{timestamp}.json\"\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary_report, f, indent=2)\n",
    "\n",
    "print(f\"✓ Summary report saved to: {summary_file}\")\n",
    "\n",
    "# Print final file list\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"ALL OUTPUT FILES\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nDirectory: {output_dir.absolute()}\\n\")\n",
    "print(\"Assessment Results:\")\n",
    "print(f\"  1. {run1_file.name} - Run 1 results (JSON)\")\n",
    "print(f\"  2. {csv_file.name} - Run 1 results (CSV)\")\n",
    "print(f\"  3. {run2_file.name} - Run 2 results (JSON)\")\n",
    "print(f\"  4. {csv_file2.name} - Run 2 results (CSV)\")\n",
    "print(f\"  5. merged_final_{timestamp}.json - Combined results (JSON)\")\n",
    "print(f\"  6. {merged_csv.name} - Combined results (CSV)\")\n",
    "print(f\"\\nQuality Reports:\")\n",
    "print(f\"  7. {consistency_file.name} - Google validation\")\n",
    "print(f\"  8. {summary_file.name} - Master summary\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"✓ ASSESSMENT COMPLETE!\")\n",
    "print(f\"{'='*70}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "745873f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ResultCSVMerger class defined\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# CSV MERGER - INTEGRATE WITH YOUR EXISTING DATA\n",
    "# =============================================================================\n",
    "\n",
    "class ResultCSVMerger:\n",
    "    \"\"\"Merge LLM results into existing CSV\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file: str, delimiter: str = ';', \n",
    "                 software_name_column: str = 'Name',\n",
    "                 method_start_column: str = 'Numerical-solvers'):\n",
    "        \"\"\"Initialize merger\"\"\"\n",
    "        self.csv_file = Path(csv_file)\n",
    "        self.delimiter = delimiter\n",
    "        self.software_name_column = software_name_column\n",
    "        self.method_start_column = method_start_column\n",
    "        \n",
    "        # Load CSV\n",
    "        self.df = pd.read_csv(csv_file, delimiter=delimiter, encoding='utf-8-sig')\n",
    "        \n",
    "        # Identify columns\n",
    "        self.info_columns = []\n",
    "        self.method_columns = []\n",
    "        found_methods = False\n",
    "        \n",
    "        for col in self.df.columns:\n",
    "            if col == method_start_column:\n",
    "                found_methods = True\n",
    "            if found_methods:\n",
    "                self.method_columns.append(col)\n",
    "            else:\n",
    "                self.info_columns.append(col)\n",
    "        \n",
    "        print(f\"✓ CSV Merger initialized:\")\n",
    "        print(f\"  File: {csv_file}\")\n",
    "        print(f\"  Software rows: {len(self.df)}\")\n",
    "        print(f\"  Info columns: {len(self.info_columns)}\")\n",
    "        print(f\"  Method columns: {len(self.method_columns)}\")\n",
    "    \n",
    "    def _normalize_name(self, name: str) -> str:\n",
    "        \"\"\"Normalize name for matching\"\"\"\n",
    "        if pd.isna(name):\n",
    "            return \"\"\n",
    "        name = str(name).lower()\n",
    "        name = ''.join(c if c.isalnum() or c in ' -' else ' ' for c in name)\n",
    "        return ' '.join(name.split())\n",
    "    \n",
    "    def merge_llm_results(self, llm_results_file: str, output_file: str,\n",
    "                         min_confidence: float = 0.5,\n",
    "                         overwrite_existing: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"Merge LLM results into CSV\"\"\"\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"MERGING LLM RESULTS INTO EXISTING CSV\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Load LLM results\n",
    "        with open(llm_results_file, 'r') as f:\n",
    "            llm_results = json.load(f)\n",
    "        \n",
    "        print(f\"Loaded {len(llm_results)} LLM assessments\")\n",
    "        \n",
    "        # Create working copy\n",
    "        df_merged = self.df.copy()\n",
    "        \n",
    "        # Track updates\n",
    "        updates = 0\n",
    "        skipped_low_conf = 0\n",
    "        skipped_not_found = 0\n",
    "        \n",
    "        for result in llm_results:\n",
    "            if result['confidence'] < min_confidence:\n",
    "                skipped_low_conf += 1\n",
    "                continue\n",
    "            \n",
    "            # Find matching software row\n",
    "            software_norm = self._normalize_name(result['software'])\n",
    "            df_norm = self.df[self.software_name_column].apply(self._normalize_name)\n",
    "            \n",
    "            matches = df_norm[df_norm == software_norm]\n",
    "            if len(matches) == 0:\n",
    "                skipped_not_found += 1\n",
    "                continue\n",
    "            \n",
    "            row_idx = matches.index[0]\n",
    "            \n",
    "            # Find matching method column\n",
    "            method_norm = self._normalize_name(result['method'])\n",
    "            method_cols_norm = {self._normalize_name(col): col for col in self.method_columns}\n",
    "            \n",
    "            if method_norm not in method_cols_norm:\n",
    "                skipped_not_found += 1\n",
    "                continue\n",
    "            \n",
    "            method_col = method_cols_norm[method_norm]\n",
    "            \n",
    "            # Update value\n",
    "            if overwrite_existing or pd.isna(df_merged.at[row_idx, method_col]):\n",
    "                df_merged.at[row_idx, method_col] = result['final_rank']\n",
    "                updates += 1\n",
    "        \n",
    "        # Save\n",
    "        df_merged.to_csv(output_file, sep=self.delimiter, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        print(f\"\\nMerge Statistics:\")\n",
    "        print(f\"  Updated: {updates}\")\n",
    "        print(f\"  Skipped (low confidence): {skipped_low_conf}\")\n",
    "        print(f\"  Skipped (not found): {skipped_not_found}\")\n",
    "        print(f\"\\n✓ Updated CSV saved to: {output_file}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        return df_merged\n",
    "\n",
    "print(\"✓ ResultCSVMerger class defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7e0bb16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV Merger initialized:\n",
      "  File: input_data\\Software_method_implementation_score.csv\n",
      "  Software rows: 39\n",
      "  Info columns: 21\n",
      "  Method columns: 190\n",
      "\n",
      "======================================================================\n",
      "MERGING LLM RESULTS INTO EXISTING CSV\n",
      "======================================================================\n",
      "Loaded 6855 LLM assessments\n",
      "\n",
      "Merge Statistics:\n",
      "  Updated: 6121\n",
      "  Skipped (low confidence): 0\n",
      "  Skipped (not found): 734\n",
      "\n",
      "✓ Updated CSV saved to: software_analysis_final\\software_methods_updated_20251019_015733.csv\n",
      "======================================================================\n",
      "\n",
      "\n",
      "✓ Your CSV has been updated with LLM assessment results!\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# FINAL STEP: MERGE INTO YOUR EXISTING CSV\n",
    "# =============================================================================\n",
    "\n",
    "# Run this AFTER both assessment runs are complete\n",
    "\n",
    "# Initialize CSV merger with YOUR file\n",
    "csv_merger = ResultCSVMerger(\n",
    "    csv_file=\"input_data\\Software_method_implementation_score.csv\",  # ← Replace with your file path\n",
    "    delimiter=';',\n",
    "    software_name_column='Name',\n",
    "    method_start_column='Numerical-solvers'\n",
    ")\n",
    "\n",
    "# Merge results into your CSV\n",
    "updated_csv = csv_merger.merge_llm_results(\n",
    "    llm_results_file=str(output_dir / f\"merged_final_{timestamp}.json\"),\n",
    "    output_file=str(output_dir / f\"software_methods_updated_{timestamp}.csv\"),\n",
    "    min_confidence=0.4,  # Only use results with ≥60% confidence\n",
    "    overwrite_existing=True  # Overwrite existing scores\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Your CSV has been updated with LLM assessment results!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6ba41d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# FIXED METHOD MIS CALCULATOR - HANDLES EUROPEAN DECIMALS\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "class MethodMISCalculator:\n",
    "    \"\"\"\n",
    "    Calculate Method Implementation Score (MIS) for each METHOD\n",
    "    and update the MIS row in CSV (handles European decimal format)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file: str, delimiter: str = ';',\n",
    "                 software_name_column: str = 'Name',\n",
    "                 osmm_column: str = 'OSMM Score',\n",
    "                 method_start_column: str = 'Numerical-solvers',\n",
    "                 mis_row_name: str = 'Gjennomsnittlig score (MIS)'):\n",
    "        \"\"\"Initialize MIS calculator\"\"\"\n",
    "        self.csv_file = Path(csv_file)\n",
    "        self.delimiter = delimiter\n",
    "        self.software_name_column = software_name_column\n",
    "        self.osmm_column = osmm_column\n",
    "        self.mis_row_name = mis_row_name\n",
    "        \n",
    "        # Load CSV - don't try to parse decimals automatically\n",
    "        self.df = pd.read_csv(csv_file, delimiter=delimiter, encoding='utf-8-sig', \n",
    "                              dtype=str)  # ← Read everything as string first\n",
    "        \n",
    "        # Identify method columns\n",
    "        self.info_columns = []\n",
    "        self.method_columns = []\n",
    "        found_methods = False\n",
    "        \n",
    "        for col in self.df.columns:\n",
    "            if col == method_start_column:\n",
    "                found_methods = True\n",
    "            if found_methods:\n",
    "                self.method_columns.append(col)\n",
    "            else:\n",
    "                self.info_columns.append(col)\n",
    "        \n",
    "        print(f\"✓ Method MIS Calculator initialized:\")\n",
    "        print(f\"  File: {csv_file}\")\n",
    "        print(f\"  Software: {len(self.df)}\")\n",
    "        print(f\"  Methods: {len(self.method_columns)}\")\n",
    "    \n",
    "    \n",
    "    def _convert_to_float(self, value) -> float:\n",
    "        \"\"\"\n",
    "        Convert value to float, handling European format (comma as decimal)\n",
    "        \n",
    "        Args:\n",
    "            value: Value to convert (can be string with comma or period)\n",
    "        \n",
    "        Returns:\n",
    "            Float value or 0.0 if conversion fails\n",
    "        \"\"\"\n",
    "        if pd.isna(value) or value == '' or value == ' ':\n",
    "            return 0.0\n",
    "        \n",
    "        try:\n",
    "            # Convert to string and clean\n",
    "            value_str = str(value).strip()\n",
    "            \n",
    "            # Replace comma with period for European decimals\n",
    "            value_str = value_str.replace(',', '.')\n",
    "            \n",
    "            # Remove any spaces\n",
    "            value_str = value_str.replace(' ', '')\n",
    "            \n",
    "            # Convert to float\n",
    "            return float(value_str)\n",
    "        except (ValueError, TypeError):\n",
    "            return 0.0\n",
    "    \n",
    "    \n",
    "    def calculate_method_mis_scores(self, output_file: str = None,\n",
    "                                    include_details: bool = True,\n",
    "                                    exclude_mis_row: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"Calculate MIS scores for each METHOD\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"CALCULATING MIS SCORES PER METHOD\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        method_results = []\n",
    "        detailed_results = []\n",
    "        \n",
    "        for method_col in self.method_columns:\n",
    "            method_name = method_col\n",
    "            method_scores = []\n",
    "            software_scores = []\n",
    "            \n",
    "            for idx, row in self.df.iterrows():\n",
    "                software_name = row[self.software_name_column]\n",
    "                \n",
    "                # Skip MIS row if requested\n",
    "                if exclude_mis_row and software_name == self.mis_row_name:\n",
    "                    continue\n",
    "                \n",
    "                # Convert OSMM score using helper function\n",
    "                osmm_score = self._convert_to_float(row[self.osmm_column])\n",
    "                \n",
    "                # Convert method rank using helper function\n",
    "                method_rank = self._convert_to_float(row[method_col])\n",
    "                \n",
    "                # Calculate score: OSMM × method_rank\n",
    "                score = osmm_score * method_rank\n",
    "                method_scores.append(score)\n",
    "                \n",
    "                # Store detailed result\n",
    "                if include_details:\n",
    "                    detailed_results.append({\n",
    "                        'method': method_name,\n",
    "                        'software': software_name,\n",
    "                        'osmm_score': osmm_score,\n",
    "                        'method_rank': method_rank,\n",
    "                        'contribution_to_mis': score\n",
    "                    })\n",
    "                \n",
    "                software_scores.append({\n",
    "                    'software': software_name,\n",
    "                    'osmm': osmm_score,\n",
    "                    'rank': method_rank,\n",
    "                    'score': score\n",
    "                })\n",
    "            \n",
    "            # Calculate MIS for this method\n",
    "            mis_score = np.mean(method_scores) if method_scores else 0.0\n",
    "            \n",
    "            # Statistics\n",
    "            software_with_implementation = sum(1 for s in software_scores if s['rank'] > 0)\n",
    "            total_software = len(software_scores)\n",
    "            coverage = (software_with_implementation / total_software * 100) if total_software > 0 else 0.0\n",
    "            \n",
    "            # Rank distribution\n",
    "            rank_counts = Counter([s['rank'] for s in software_scores])\n",
    "            \n",
    "            method_results.append({\n",
    "                'method': method_name,\n",
    "                'mis_score': round(mis_score, 4),\n",
    "                'software_with_implementation': software_with_implementation,\n",
    "                'total_software': total_software,\n",
    "                'coverage_percentage': round(coverage, 2),\n",
    "                'avg_rank': round(np.mean([s['rank'] for s in software_scores]), 2),\n",
    "                'rank_0_count': rank_counts.get(0.0, 0),\n",
    "                'rank_1_count': rank_counts.get(1.0, 0),\n",
    "                'rank_2_count': rank_counts.get(2.0, 0),\n",
    "                'rank_3_count': rank_counts.get(3.0, 0),\n",
    "                'max_score': round(max(method_scores), 4) if method_scores else 0.0,\n",
    "                'min_score': round(min(method_scores), 4) if method_scores else 0.0\n",
    "            })\n",
    "        \n",
    "        # Create results DataFrame\n",
    "        results_df = pd.DataFrame(method_results)\n",
    "        results_df = results_df.sort_values('mis_score', ascending=False)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"METHOD MIS CALCULATION COMPLETE\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"\\nTop 20 Methods by MIS Score:\")\n",
    "        print(results_df[['method', 'mis_score', 'coverage_percentage', 'avg_rank']].head(20).to_string(index=False))\n",
    "        \n",
    "        # Save if requested\n",
    "        if output_file:\n",
    "            results_df.to_csv(output_file, index=False)\n",
    "            print(f\"\\n✓ Method MIS scores saved to: {output_file}\")\n",
    "            \n",
    "            if include_details:\n",
    "                detailed_df = pd.DataFrame(detailed_results)\n",
    "                detail_file = Path(output_file).parent / f\"{Path(output_file).stem}_detailed.csv\"\n",
    "                detailed_df.to_csv(detail_file, index=False)\n",
    "                print(f\"✓ Detailed method scores saved to: {detail_file}\")\n",
    "        \n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    \n",
    "    def update_mis_row_in_csv(self, output_file: str = None, \n",
    "                              use_comma_as_decimal: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Update the MIS row in the CSV with calculated MIS scores\n",
    "        \n",
    "        Args:\n",
    "            output_file: Output file path (if None, overwrites original)\n",
    "            use_comma_as_decimal: If True, format MIS values with comma as decimal (European)\n",
    "        \n",
    "        Returns:\n",
    "            Updated DataFrame\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"UPDATING MIS ROW IN CSV\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Calculate MIS scores per method\n",
    "        print(f\"\\nCalculating MIS scores for each method...\")\n",
    "        \n",
    "        method_mis_scores = {}\n",
    "        \n",
    "        for method_col in self.method_columns:\n",
    "            method_scores = []\n",
    "            \n",
    "            for idx, row in self.df.iterrows():\n",
    "                software_name = row[self.software_name_column]\n",
    "                \n",
    "                # Skip the MIS row itself\n",
    "                if software_name == self.mis_row_name:\n",
    "                    continue\n",
    "                \n",
    "                # Convert using helper function\n",
    "                osmm_score = self._convert_to_float(row[self.osmm_column])\n",
    "                method_rank = self._convert_to_float(row[method_col])\n",
    "                \n",
    "                # Calculate score\n",
    "                score = osmm_score * method_rank\n",
    "                method_scores.append(score)\n",
    "            \n",
    "            # Calculate average MIS for this method\n",
    "            mis_score = np.mean(method_scores) if method_scores else 0.0\n",
    "            method_mis_scores[method_col] = mis_score\n",
    "        \n",
    "        print(f\"  Calculated MIS for {len(method_mis_scores)} methods\")\n",
    "        \n",
    "        # Find the MIS row\n",
    "        mis_row_idx = self.df[self.df[self.software_name_column] == self.mis_row_name].index\n",
    "        \n",
    "        if len(mis_row_idx) == 0:\n",
    "            print(f\"\\n⚠ MIS row '{self.mis_row_name}' not found in CSV!\")\n",
    "            print(f\"  Creating new row...\")\n",
    "            \n",
    "            # Create new row\n",
    "            new_row = {col: '' for col in self.df.columns}\n",
    "            new_row[self.software_name_column] = self.mis_row_name\n",
    "            \n",
    "            # Add to dataframe\n",
    "            self.df = pd.concat([self.df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "            mis_row_idx = self.df[self.df[self.software_name_column] == self.mis_row_name].index\n",
    "        \n",
    "        mis_row_idx = mis_row_idx[0]\n",
    "        \n",
    "        print(f\"\\n✓ Found MIS row at index {mis_row_idx}\")\n",
    "        print(f\"  Updating {len(method_mis_scores)} method columns...\")\n",
    "        \n",
    "        # Update each method column with its MIS score\n",
    "        updates_made = 0\n",
    "        for method_col, mis_score in method_mis_scores.items():\n",
    "            if method_col in self.df.columns:\n",
    "                old_value = self.df.at[mis_row_idx, method_col]\n",
    "                \n",
    "                # Format the score\n",
    "                if use_comma_as_decimal:\n",
    "                    # European format: comma as decimal, 4 decimals\n",
    "                    formatted_score = f\"{mis_score:.4f}\".replace('.', ',')\n",
    "                else:\n",
    "                    # US format: period as decimal, 4 decimals\n",
    "                    formatted_score = f\"{mis_score:.4f}\"\n",
    "                \n",
    "                self.df.at[mis_row_idx, method_col] = formatted_score\n",
    "                updates_made += 1\n",
    "                \n",
    "                # Show some examples\n",
    "                if updates_made <= 5:\n",
    "                    old_display = str(old_value) if old_value else 'empty'\n",
    "                    print(f\"    {method_col}: {old_display} → {formatted_score}\")\n",
    "        \n",
    "        if updates_made > 5:\n",
    "            print(f\"    ... and {updates_made - 5} more\")\n",
    "        \n",
    "        # Clear OSMM Score column for MIS row\n",
    "        if self.osmm_column in self.df.columns:\n",
    "            self.df.at[mis_row_idx, self.osmm_column] = ''\n",
    "        \n",
    "        # Determine output file\n",
    "        if output_file is None:\n",
    "            output_file = self.csv_file\n",
    "        \n",
    "        # Save updated CSV\n",
    "        self.df.to_csv(output_file, sep=self.delimiter, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        print(f\"\\n✓ Updated CSV saved to: {output_file}\")\n",
    "        print(f\"  Total updates: {updates_made} method columns\")\n",
    "        print(f\"  Decimal format: {'Comma (European)' if use_comma_as_decimal else 'Period (US)'}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    \n",
    "    def show_mis_row_preview(self):\n",
    "        \"\"\"Show preview of the MIS row values\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"MIS ROW PREVIEW\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Calculate MIS scores\n",
    "        method_mis_scores = {}\n",
    "        \n",
    "        for method_col in self.method_columns:\n",
    "            method_scores = []\n",
    "            \n",
    "            for idx, row in self.df.iterrows():\n",
    "                software_name = row[self.software_name_column]\n",
    "                \n",
    "                # Skip the MIS row\n",
    "                if software_name == self.mis_row_name:\n",
    "                    continue\n",
    "                \n",
    "                osmm_score = self._convert_to_float(row[self.osmm_column])\n",
    "                method_rank = self._convert_to_float(row[method_col])\n",
    "                \n",
    "                score = osmm_score * method_rank\n",
    "                method_scores.append(score)\n",
    "            \n",
    "            mis_score = np.mean(method_scores) if method_scores else 0.0\n",
    "            method_mis_scores[method_col] = mis_score\n",
    "        \n",
    "        # Sort by MIS score\n",
    "        sorted_methods = sorted(method_mis_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\nTop 20 Methods by MIS Score:\")\n",
    "        print(f\"{'Method':<50} {'MIS Score':>10}\")\n",
    "        print(f\"{'-'*61}\")\n",
    "        for method, score in sorted_methods[:20]:\n",
    "            method_display = method[:47] + \"...\" if len(method) > 50 else method\n",
    "            print(f\"{method_display:<50} {score:>10.4f}\")\n",
    "        \n",
    "        print(f\"\\nBottom 10 Methods by MIS Score:\")\n",
    "        print(f\"{'Method':<50} {'MIS Score':>10}\")\n",
    "        print(f\"{'-'*61}\")\n",
    "        for method, score in sorted_methods[-10:]:\n",
    "            method_display = method[:47] + \"...\" if len(method) > 50 else method\n",
    "            print(f\"{method_display:<50} {score:>10.4f}\")\n",
    "        \n",
    "        print(f\"\\nStatistics:\")\n",
    "        scores = list(method_mis_scores.values())\n",
    "        print(f\"  Total methods: {len(scores)}\")\n",
    "        print(f\"  Mean MIS: {np.mean(scores):.4f}\")\n",
    "        print(f\"  Median MIS: {np.median(scores):.4f}\")\n",
    "        print(f\"  Std Dev: {np.std(scores):.4f}\")\n",
    "        print(f\"  Min: {min(scores):.4f}\")\n",
    "        print(f\"  Max: {max(scores):.4f}\")\n",
    "        \n",
    "        print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    \n",
    "    def create_method_comparison_report(self, output_file: str = \"method_mis_comparison.csv\"):\n",
    "        \"\"\"Create comprehensive comparison report\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"CREATING METHOD COMPARISON REPORT\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        comparison_data = []\n",
    "        \n",
    "        for method_col in self.method_columns:\n",
    "            implementations = []\n",
    "            \n",
    "            for idx, row in self.df.iterrows():\n",
    "                software_name = row[self.software_name_column]\n",
    "                \n",
    "                # Skip MIS row\n",
    "                if software_name == self.mis_row_name:\n",
    "                    continue\n",
    "                \n",
    "                osmm_score = self._convert_to_float(row[self.osmm_column])\n",
    "                method_rank = self._convert_to_float(row[method_col])\n",
    "                \n",
    "                implementations.append({\n",
    "                    'osmm': osmm_score,\n",
    "                    'rank': method_rank,\n",
    "                    'score': osmm_score * method_rank\n",
    "                })\n",
    "            \n",
    "            # Calculate statistics\n",
    "            mis_score = np.mean([imp['score'] for imp in implementations])\n",
    "            implemented = sum(1 for imp in implementations if imp['rank'] > 0)\n",
    "            direct_impl = sum(1 for imp in implementations if imp['rank'] == 3.0)\n",
    "            indirect_impl = sum(1 for imp in implementations if imp['rank'] == 2.0)\n",
    "            limited_impl = sum(1 for imp in implementations if imp['rank'] == 1.0)\n",
    "            \n",
    "            implementing_osmm = [imp['osmm'] for imp in implementations if imp['rank'] > 0]\n",
    "            avg_implementing_osmm = np.mean(implementing_osmm) if implementing_osmm else 0.0\n",
    "            \n",
    "            comparison_data.append({\n",
    "                'method': method_col,\n",
    "                'mis_score': round(mis_score, 4),\n",
    "                'total_software': len(implementations),\n",
    "                'implemented_in': implemented,\n",
    "                'coverage_pct': round(implemented / len(implementations) * 100, 2),\n",
    "                'direct_implementations': direct_impl,\n",
    "                'indirect_implementations': indirect_impl,\n",
    "                'limited_implementations': limited_impl,\n",
    "                'not_supported': len(implementations) - implemented,\n",
    "                'avg_implementation_rank': round(np.mean([imp['rank'] for imp in implementations]), 2),\n",
    "                'avg_osmm_of_implementers': round(avg_implementing_osmm, 2),\n",
    "                'implementation_maturity_score': round(avg_implementing_osmm * (implemented / len(implementations)), 2)\n",
    "            })\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        comparison_df = comparison_df.sort_values('mis_score', ascending=False)\n",
    "        comparison_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(f\"\\n✓ Method comparison report saved to: {output_file}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        return comparison_df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c249e4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Method MIS Calculator initialized:\n",
      "  File: software_analysis_output/software_methods_updated.csv\n",
      "  Software: 39\n",
      "  Methods: 190\n",
      "\n",
      "======================================================================\n",
      "MIS ROW PREVIEW\n",
      "======================================================================\n",
      "\n",
      "Top 20 Methods by MIS Score:\n",
      "Method                                              MIS Score\n",
      "-------------------------------------------------------------\n",
      "power flow analysis                                    1.1734\n",
      "numerical solvers                                      1.0308\n",
      "contingency analysis                                   0.9961\n",
      "voltage control strategy                               0.9705\n",
      "automatic generation control agc                       0.9705\n",
      "power system stabilizer                                0.9595\n",
      "static var compensator                                 0.9453\n",
      "voltage stability                                      0.9413\n",
      "load balancing                                         0.9395\n",
      "security constrained unit commitment                   0.9311\n",
      "frequency nadir                                        0.9268\n",
      "dynamic reactive power compensation                    0.9174\n",
      "reactive power sharing                                 0.9174\n",
      "power quality disturbance                              0.9034\n",
      "power forecasting                                      0.9018\n",
      "load shedding analysis                                 0.9018\n",
      "state-space modeling                                   0.8945\n",
      "fault analysis                                         0.8847\n",
      "frequency variation                                    0.8837\n",
      "optimal capacity configuration                         0.8818\n",
      "\n",
      "Bottom 10 Methods by MIS Score:\n",
      "Method                                              MIS Score\n",
      "-------------------------------------------------------------\n",
      "empirical mode decomposition                           0.0684\n",
      "gated recurrent unit                                   0.0684\n",
      "data envelopment analysis                              0.0642\n",
      "quantum computing                                      0.0632\n",
      "deep deterministic policy gradient                     0.0616\n",
      "deep q-network                                         0.0616\n",
      "deep reinforcement learning drl                        0.0547\n",
      "generative adversarial network                         0.0521\n",
      "deep deterministic                                     0.0479\n",
      "Numerical-solvers                                      0.0000\n",
      "\n",
      "Statistics:\n",
      "  Total methods: 190\n",
      "  Mean MIS: 0.5318\n",
      "  Median MIS: 0.5689\n",
      "  Std Dev: 0.2884\n",
      "  Min: 0.0000\n",
      "  Max: 1.1734\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# USAGE WITH FIXED DECIMAL HANDLING\n",
    "# =============================================================================\n",
    "\n",
    "# Initialize calculator\n",
    "method_mis_calc = MethodMISCalculator(\n",
    "    csv_file=\"software_analysis_output/software_methods_updated.csv\",\n",
    "    delimiter=';',\n",
    "    software_name_column='Name',\n",
    "    osmm_column='OSMM Score',\n",
    "    method_start_column='Numerical-solvers',\n",
    "    mis_row_name='Gjennomsnittlig score (MIS)'\n",
    ")\n",
    "\n",
    "# Preview\n",
    "method_mis_calc.show_mis_row_preview()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f5e79d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "UPDATING MIS ROW IN CSV\n",
      "======================================================================\n",
      "\n",
      "Calculating MIS scores for each method...\n",
      "  Calculated MIS for 190 methods\n",
      "\n",
      "✓ Found MIS row at index 36\n",
      "  Updating 190 method columns...\n",
      "    Numerical-solvers: nan → 0,0000\n",
      "    power flow analysis:  1,2726  → 1,1734\n",
      "    security-constrained optimal power flow:  0,9437  → 0,8587\n",
      "    security constrained unit commitment:  1,0415  → 0,9311\n",
      "    Non Linear Optimal Power Flow:  0,7133  → 0,6574\n",
      "    ... and 185 more\n",
      "\n",
      "✓ Updated CSV saved to: software_methods_with_mis_updated.csv\n",
      "  Total updates: 190 method columns\n",
      "  Decimal format: Comma (European)\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "CALCULATING MIS SCORES PER METHOD\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "METHOD MIS CALCULATION COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Top 20 Methods by MIS Score:\n",
      "                              method  mis_score  coverage_percentage  avg_rank\n",
      "                 power flow analysis     1.1734                97.37      3.84\n",
      "                   numerical solvers     1.0308                92.11      3.40\n",
      "                contingency analysis     0.9961                92.11      3.16\n",
      "    automatic generation control agc     0.9705               100.00      3.08\n",
      "            voltage control strategy     0.9705               100.00      3.08\n",
      "             power system stabilizer     0.9595               100.00      3.04\n",
      "              static var compensator     0.9453               100.00      2.97\n",
      "                   voltage stability     0.9413               100.00      3.00\n",
      "                      load balancing     0.9395               100.00      2.97\n",
      "security constrained unit commitment     0.9311               100.00      2.99\n",
      "                     frequency nadir     0.9268               100.00      2.93\n",
      " dynamic reactive power compensation     0.9174               100.00      2.89\n",
      "              reactive power sharing     0.9174               100.00      2.89\n",
      "           power quality disturbance     0.9034                94.74      2.85\n",
      "              load shedding analysis     0.9018               100.00      2.85\n",
      "                   power forecasting     0.9018               100.00      2.90\n",
      "                state-space modeling     0.8945               100.00      2.84\n",
      "                      fault analysis     0.8847                89.47      2.96\n",
      "                 frequency variation     0.8837                94.74      2.78\n",
      "      optimal capacity configuration     0.8818               100.00      2.83\n",
      "\n",
      "✓ Method MIS scores saved to: method_mis_scores.csv\n",
      "✓ Detailed method scores saved to: method_mis_scores_detailed.csv\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "CREATING METHOD COMPARISON REPORT\n",
      "======================================================================\n",
      "\n",
      "✓ Method comparison report saved to: method_mis_comparison.csv\n",
      "======================================================================\n",
      "\n",
      "\n",
      "✓ All MIS calculations complete with proper decimal handling!\n"
     ]
    }
   ],
   "source": [
    "# Update CSV with European format (comma as decimal)\n",
    "updated_df = method_mis_calc.update_mis_row_in_csv(\n",
    "    output_file=\"software_methods_with_mis_updated.csv\",\n",
    "    use_comma_as_decimal=True  # ← European format\n",
    ")\n",
    "\n",
    "# Create reports\n",
    "method_mis_scores = method_mis_calc.calculate_method_mis_scores(\n",
    "    output_file=\"method_mis_scores.csv\",\n",
    "    include_details=True\n",
    ")\n",
    "\n",
    "method_comparison = method_mis_calc.create_method_comparison_report(\n",
    "    output_file=\"method_mis_comparison.csv\"\n",
    ")\n",
    "\n",
    "print(\"\\n✓ All MIS calculations complete with proper decimal handling!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "literature-search-and-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
