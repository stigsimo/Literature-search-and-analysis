{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ed1c751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 1 - Imports and Setup\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import configparser\n",
    "import tiktoken\n",
    "import logging\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Phrases\n",
    "import openai\n",
    "\n",
    "SAVE_DIR = \"Saved_files_new\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a369bb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 2 OpenAI Setup\n",
    "# %%\n",
    "class CreditTracker:\n",
    "    def __init__(self):\n",
    "        self.total_tokens = 0\n",
    "        self.total_cost = 0\n",
    "        self.cost_per_1k_tokens = 0.00015\n",
    "    def update(self, tokens):\n",
    "        self.total_tokens += tokens\n",
    "        self.total_cost += (tokens / 1000) * self.cost_per_1k_tokens\n",
    "    def get_stats(self):\n",
    "        return {\"total_tokens\": self.total_tokens, \"total_cost\": round(self.total_cost, 4)}\n",
    "\n",
    "def initialize_openai():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('config_LLM.txt')\n",
    "    api_key = config['LLM'].get('OPENAI_API_KEY')\n",
    "    model_type = config['LLM'].get('MODEL_TYPE')\n",
    "    client = openai.OpenAI(api_key=api_key)\n",
    "    return client, model_type\n",
    "\n",
    "def num_tokens_from_string(string: str, model_name: str) -> int:\n",
    "    encoding = tiktoken.encoding_for_model(model_name)\n",
    "    return len(encoding.encode(string))\n",
    "\n",
    "client, model_type = initialize_openai()\n",
    "credit_tracker = CreditTracker()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "445ee09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 Utility Functions & Preprocessing\n",
    "def extract_keywords_from_filename(filename):\n",
    "    base = os.path.splitext(os.path.basename(filename))[0]\n",
    "    parts = base.split('_')\n",
    "    keywords = [part for i, part in enumerate(parts) if i > 2 and part != 'results' and not part.isdigit()]\n",
    "    return keywords\n",
    "\n",
    "def get_custom_stop_words(search_keywords=None):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words_to_keep = set()\n",
    "    if search_keywords:\n",
    "        for keyword in search_keywords:\n",
    "            keyword = keyword.lower()\n",
    "            words_to_keep.add(keyword)\n",
    "            for word in keyword.split():\n",
    "                words_to_keep.add(word)\n",
    "    stop_words = stop_words - words_to_keep\n",
    "    scientific_terms = {'et', 'al','ref','reference','references','cited','cite',\n",
    "        'fig','figure','figures','table','tables','chart','charts',\n",
    "        'published','journal','conference','proceedings','vol','volume','pp','page','pages','doi'}\n",
    "    stop_words = stop_words.union(scientific_terms)\n",
    "    return stop_words\n",
    "\n",
    "def preprocess_text(text, search_keywords=None, min_word_length=2, remove_numbers=True):\n",
    "    if not isinstance(text, (str, int, float)):\n",
    "        return ''\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    if remove_numbers:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s-]', '', text)\n",
    "    text = re.sub(r'--+', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = get_custom_stop_words(search_keywords)\n",
    "    tokens = [t for t in tokens if len(t) >= min_word_length and t not in stop_words and len(t) > 1 and not t.isdigit()]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    try:\n",
    "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    except:\n",
    "        pass\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess_dataframe(df, text_col, search_keywords, processed_col='processed_text'):\n",
    "    df[text_col] = df[text_col].fillna('').astype(str)\n",
    "    df[processed_col] = df[text_col].apply(lambda x: preprocess_text(x, search_keywords))\n",
    "    return df[df[processed_col].str.strip() != '']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80e31361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 Data Loading & Cleaning\n",
    "\n",
    "filename = \"semantic_scholar_2025_02_14_reliability_resilience_power_systems_results.csv\"\n",
    "filepath = os.path.join(\"Saved_files\", filename)\n",
    "df = pd.read_csv(filepath, sep=\";\")\n",
    "df['text'] = df['title'].fillna('') + ' ' + df['abstract'].fillna('')\n",
    "search_keywords = extract_keywords_from_filename(filename)\n",
    "df = preprocess_dataframe(df, text_col='text', search_keywords=search_keywords)\n",
    "\n",
    "def clean_fields_of_study(s):\n",
    "    valid_fields = ['Computer Science', 'Economics', 'Engineering', 'Physics', 'Mathematics',\n",
    "        'Medicine','Business','Environmental Science','Chemistry','Materials Science',\n",
    "        'Geography','Biology','Geology','Political Science','Psychology','Com']\n",
    "    if pd.isna(s) or s == '[]':\n",
    "        return [\"Unknown\"]\n",
    "    if isinstance(s, str):\n",
    "        fields = [field.strip().strip(\"'\\\"\") for field in s.strip('[]').split(',')]\n",
    "        return [f if f in valid_fields else \"Unknown\" for f in fields] or [\"Unknown\"]\n",
    "    return [\"Unknown\"]\n",
    "\n",
    "df['fieldsOfStudy'] = df['fieldsOfStudy'].apply(clean_fields_of_study)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c23bc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 12:46:05,484 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of methods found: 44, Top method phrases:['confidence interval estimation', 'particle swarm optimization', 'analytical hierarchy process', 'countermeasure analysis', 'pso algorithm', 'control strategy', 'fuzzy logic', 'neural network', 'genetic algorithm', 'contextual bandits', 'model predictive control', 'deep learning', 'signal processing', 'computer simulation', 'multi-agent systems']\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 Candidate Extraction & LLM Filtering\n",
    "def extract_candidate_terms(df, text_col='processed_text', max_features=20000):\n",
    "    vectorizer = CountVectorizer(\n",
    "        ngram_range=(1, 4),\n",
    "        max_df=0.95,\n",
    "        min_df=2,\n",
    "        max_features=max_features,\n",
    "        token_pattern=r'\\b[\\w-]+\\b'\n",
    "    )\n",
    "    matrix = vectorizer.fit_transform(df[text_col].fillna(''))\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    freqs = matrix.sum(axis=0).A1\n",
    "    return [term for term, freq in sorted(zip(terms, freqs), key=lambda x: x[1], reverse=True)]\n",
    "\n",
    "def get_method_phrases(corpus_terms, client, model_type, credit_tracker):\n",
    "    import ast\n",
    "    sample_terms = ', '.join(corpus_terms[:100])\n",
    "    prompt = f\"\"\"Here are the most frequent terms from a corpus of scientific papers:\n",
    "{sample_terms}\n",
    "\n",
    "From the full list: {', '.join(corpus_terms)}\n",
    "Extract ONLY the terms that represent specific methodologies, techniques, or named approaches. Focus on specific computational, statistical, engineering, and reliability analysis methods. Do NOT include generic system names or vague phrases such as \"analysis\", \"Grid Planning\", \"Risk Assesment\" by themselves ; do include things like \"monte carlo simulation\", \"optimal power flow\", \"genetic algorithm\", \"fault tree analysis\", etc.\n",
    "\n",
    "Return as a Python list with no code blocks or formatting.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_type,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    try:\n",
    "        return ast.literal_eval(content)\n",
    "    except:\n",
    "        content = content.replace('[', '').replace(']', '').replace('\"', '').replace(\"'\",'')\n",
    "        return [term.strip() for term in content.split(',') if len(term.strip()) > 3]\n",
    "\n",
    "def clean_method_phrases_fixed(method_phrases):\n",
    "    cleaned_phrases = []\n",
    "    for phrase in method_phrases:\n",
    "        cleaned = phrase.strip().replace('``````','').replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", '').replace('\\\\n', ' ')\n",
    "        cleaned = ' '.join(cleaned.split())\n",
    "        if len(cleaned) > 2:\n",
    "            cleaned_phrases.append(cleaned.lower())\n",
    "    return list(set(cleaned_phrases))\n",
    "\n",
    "candidate_terms = extract_candidate_terms(df, text_col='processed_text')\n",
    "method_phrases = get_method_phrases(candidate_terms, client, model_type, credit_tracker)\n",
    "method_phrases = clean_method_phrases_fixed(method_phrases)\n",
    "print(f\"Number of methods found: {len(method_phrases)}, Top method phrases:{ method_phrases[:15]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dc6d2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 12:46:11,012 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-20 12:46:11,016 - INFO - LLM mapped 44 methods to abbreviations/variants.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 Synonym/Abbreviation Mapping and Standardization\n",
    "def get_method_abbreviation_dict(method_phrases, client, model_type, credit_tracker, batch_size=100):\n",
    "    import ast\n",
    "    results = {}\n",
    "    for i in range(0, len(method_phrases), batch_size):\n",
    "        batch = method_phrases[i:i+batch_size]\n",
    "        prompt = f\"\"\"For each of the following phrases, extract ALL common scientific abbreviations, synonyms, and aliases for methods/techniques. Format the response strictly as a Python dictionary.\n",
    "\n",
    "Methods:\n",
    "{chr(10).join(batch)}\n",
    "Only return methods as keys and aliases as list values. No intros, explanations, code blocks, or categories.\"\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_type,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a scientific abbreviation expert.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        start, end = content.find('{'), content.rfind('}')+1\n",
    "        method_dict = {}\n",
    "        if start >= 0 and end > start:\n",
    "            try:\n",
    "                method_dict = ast.literal_eval(content[start:end])\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to parse dictionary from LLM batch: {e}\")\n",
    "        results.update(method_dict)\n",
    "    logger.info(f\"LLM mapped {len(results)} methods to abbreviations/variants.\")\n",
    "    return results\n",
    "\n",
    "def build_abbr_to_canonical_map(method_dict):\n",
    "    abbr_map = {}\n",
    "    for canonical, variants in method_dict.items():\n",
    "        abbr_map[canonical.lower()] = canonical\n",
    "        for v in variants:\n",
    "            abbr_map[v.lower()] = canonical\n",
    "    return abbr_map\n",
    "\n",
    "import re\n",
    "def standardize_methods_in_text(text, abbr_to_canonical):\n",
    "    sorted_vars = sorted(abbr_to_canonical, key=lambda x: -len(x))\n",
    "    for var in sorted_vars:\n",
    "        pattern = r'\\b' + re.escape(var) + r'\\b'\n",
    "        text = re.sub(pattern, abbr_to_canonical[var], text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "method_dict = get_method_abbreviation_dict(method_phrases, client, model_type, credit_tracker, batch_size=100)\n",
    "abbr_to_canonical_map = build_abbr_to_canonical_map(method_dict)\n",
    "df['standardized_text'] = df['processed_text'].apply(lambda t: standardize_methods_in_text(t, abbr_to_canonical_map))\n",
    "method_vocabulary = sorted(method_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d6efbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "# A: Build TFIDF Score Matrix\n",
    "\n",
    "def compute_tfidf_scores(processed_texts, method_phrases, ngram_range=(1, 4), min_df=1, max_df=0.95, norm='l2'):\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        vocabulary=method_phrases, ngram_range=ngram_range,\n",
    "        min_df=min_df, max_df=max_df, norm=norm\n",
    "    )\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(processed_texts)\n",
    "    scores = tfidf_matrix.toarray()\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    return scores, feature_names\n",
    "# \n",
    "# B: Build LDA Score Matrix\n",
    "\n",
    "def compute_lda_scores(processed_texts, method_phrases, ngram_range=(1, 3), n_topics=100, max_iter=20):\n",
    "    vectorizer = CountVectorizer(\n",
    "        vocabulary=method_phrases, ngram_range=ngram_range, token_pattern=r'\\b[\\w-]+\\b'\n",
    "    )\n",
    "    doc_term_matrix = vectorizer.fit_transform(processed_texts)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    if n_topics >= 2:\n",
    "        lda = LatentDirichletAllocation(n_components=n_topics, learning_method='batch', \n",
    "                                       random_state=42, max_iter=max_iter)\n",
    "        lda_matrix = lda.fit_transform(doc_term_matrix)\n",
    "    else:\n",
    "        lda_matrix = np.zeros((doc_term_matrix.shape[0], len(method_phrases)))\n",
    "    return lda_matrix, feature_names\n",
    "# \n",
    "# Compound (partial phrase/proximity) Score Matrix\n",
    "\n",
    "def compute_compound_scores(df, method_phrases, processed_col='standardized_text', window=80, min_word_len=4):\n",
    "    n_docs = len(df)\n",
    "    n_methods = len(method_phrases)\n",
    "    scores = np.zeros((n_docs, n_methods), dtype=np.float32)\n",
    "    docs = df[processed_col].fillna('').str.lower().tolist()\n",
    "    for j, phrase in enumerate(method_phrases):\n",
    "        phrase_l = phrase.lower()\n",
    "        words = [w for w in phrase_l.split() if len(w) >= min_word_len]\n",
    "        for i, text in enumerate(docs):\n",
    "            if phrase_l in text:\n",
    "                scores[i, j] = 1.0\n",
    "            elif len(words) > 1:\n",
    "                matches = sum(1 for w in words if w in text)\n",
    "                scores[i, j] = matches / len(words)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e15d05c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Combine Scores\n",
    "\n",
    "def combine_method_scores(tfidf_scores, lda_scores, compound_scores, weights=(0.4, 0.3, 0.3)):\n",
    "    # weights = (tfidf, lda, compound)\n",
    "    return weights[0]*tfidf_scores + weights[1]*lda_scores + weights[2]*compound_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ae083ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Assign Top-N Methods by Total Score\n",
    "\n",
    "def assign_top_methods_by_total_score(df, total_scores, method_names, top_n=3, min_score=0.03):\n",
    "    for rank in range(top_n):\n",
    "        top_method = []\n",
    "        top_score = []\n",
    "        for row in total_scores:\n",
    "            idxs = np.argsort(row)[::-1]\n",
    "            nth_idx = idxs[rank] if rank < len(idxs) else None\n",
    "            if nth_idx is not None and row[nth_idx] >= min_score:\n",
    "                top_method.append(method_names[nth_idx])\n",
    "                top_score.append(row[nth_idx])\n",
    "            else:\n",
    "                top_method.append(\"LowConfidence\")\n",
    "                top_score.append(row[nth_idx] if nth_idx is not None else 0.0)\n",
    "        df[f'Top_{rank+1}_Method'] = top_method\n",
    "        df[f'Top_{rank+1}_Score'] = top_score\n",
    "    df['Primary_Method'] = df['Top_1_Method']\n",
    "    df['Primary_Method_Score'] = df['Top_1_Score']\n",
    "    # Assign confidence\n",
    "    conf = []\n",
    "    for m1, s1, m2, s2, m3, s3 in zip(\n",
    "        df['Top_1_Method'], df['Top_1_Score'],\n",
    "        df['Top_2_Method'], df['Top_2_Score'],\n",
    "        df['Top_3_Method'], df['Top_3_Score']\n",
    "    ):\n",
    "        if m1 != \"LowConfidence\" and s1 > 2 * max(0.05, s2):\n",
    "            conf.append(\"super_confident\")\n",
    "        elif m1 != \"LowConfidence\":\n",
    "            conf.append(\"confident\")\n",
    "        else:\n",
    "            conf.append(\"low_confidence\")\n",
    "    df['Method_Confidence'] = conf\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0044999b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full combined score matrix for all methods saved to: Saved_files_new\\semantic_scholar_2025_08_20_method_all_combined_scores.csv\n",
      "                Primary_Method  Primary_Method_Score Method_Confidence  \\\n",
      "0     reliability-based design              0.156818         confident   \n",
      "1          computer simulation              0.156818         confident   \n",
      "2     variable frequency drive              0.206818         confident   \n",
      "3  statistical process control              0.306818         confident   \n",
      "4  statistical process control              0.206818         confident   \n",
      "\n",
      "                  Top_1_Method  Top_1_Score                 Top_2_Method  \\\n",
      "0     reliability-based design     0.156818  statistical process control   \n",
      "1          computer simulation     0.156818          dynamic programming   \n",
      "2     variable frequency drive     0.206818             load forecasting   \n",
      "3  statistical process control     0.306818       support vector machine   \n",
      "4  statistical process control     0.206818           optimal power flow   \n",
      "\n",
      "   Top_2_Score             Top_3_Method  Top_3_Score  \n",
      "0     0.106818       optimal power flow     0.106818  \n",
      "1     0.156818       linear programming     0.156818  \n",
      "2     0.156818    system identification     0.156818  \n",
      "3     0.206818  demand response program     0.206818  \n",
      "4     0.206818         control strategy     0.156818  \n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Main Pipeline - Call the Functions\n",
    "\n",
    "# 1. Compute the three score matrices\n",
    "tfidf_scores, tfidf_method_names = compute_tfidf_scores(df['standardized_text'], method_phrases, ngram_range=(1, 4))\n",
    "lda_n_topics = min(len(method_phrases), 100)\n",
    "lda_scores, lda_method_names = compute_lda_scores(df['standardized_text'], method_phrases, ngram_range=(1, 3), n_topics=lda_n_topics)\n",
    "# Make sure order matches\n",
    "assert list(tfidf_method_names) == list(lda_method_names), \"Method name order mismatch\"\n",
    "\n",
    "compound_scores = compute_compound_scores(df, tfidf_method_names, processed_col='standardized_text', window=150, min_word_len=4)\n",
    "\n",
    "# 2. Combine all scores\n",
    "total_scores = combine_method_scores(tfidf_scores, lda_scores, compound_scores, weights=(0.4, 0.3, 0.3))\n",
    "\n",
    "# 3. Assign top-N by combined score\n",
    "df = assign_top_methods_by_total_score(df, total_scores, tfidf_method_names, top_n=3, min_score=0.03)\n",
    "\n",
    "# 4. Save full score matrix for review if desired\n",
    "combined_score_df = pd.DataFrame(total_scores, columns=tfidf_method_names, index=df.index)\n",
    "score_save_path = os.path.join(\n",
    "    SAVE_DIR, f\"semantic_scholar_{datetime.now().strftime('%Y_%m_%d')}_method_all_combined_scores.csv\"\n",
    ")\n",
    "combined_score_df.to_csv(score_save_path)\n",
    "print(f\"Full combined score matrix for all methods saved to: {score_save_path}\")\n",
    "\n",
    "# 5. Output sample assignments\n",
    "print(df[['Primary_Method', 'Primary_Method_Score', 'Method_Confidence',\n",
    "          'Top_1_Method', 'Top_1_Score', 'Top_2_Method', 'Top_2_Score', 'Top_3_Method', 'Top_3_Score']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d47e1d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Method_TFIDF_Score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\git_repos\\Literature-search-and-analysis\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Method_TFIDF_Score'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcanonical\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(variants[:\u001b[38;5;241m5\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Run diagnostics\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m comprehensive_diagnostics_granular(df, \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMethod_TFIDF_Score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto_numpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), method_phrases, method_dict)\n",
      "File \u001b[1;32mc:\\git_repos\\Literature-search-and-analysis\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\git_repos\\Literature-search-and-analysis\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Method_TFIDF_Score'"
     ]
    }
   ],
   "source": [
    "# Cell 11 Diagnostics Function \n",
    "def comprehensive_diagnostics_granular(\n",
    "    df,\n",
    "    tfidf_scores,\n",
    "    method_phrases,\n",
    "    method_dict=None\n",
    "):\n",
    "    n_docs = len(df)\n",
    "    n_methods = len(method_phrases)\n",
    "    print(\"=== DIAGNOSTICS ===\")\n",
    "    print(f\"Total docs: {n_docs}, Total unique methods: {n_methods}\")\n",
    "    if method_dict is not None:\n",
    "        n_variants = sum(1 + len(variants) for variants in method_dict.values())\n",
    "        print(f\"Total method variants (including abbreviations): {n_variants}\")\n",
    "\n",
    "    # Coverage\n",
    "    tfidf_nonzero = (tfidf_scores > 0).any(axis=1).sum()\n",
    "    print(f\"\\nTF-IDF coverage: {tfidf_nonzero}/{n_docs} ({100*tfidf_nonzero/n_docs:.1f}%)\")\n",
    "\n",
    "    # Confidence distribution\n",
    "    if 'Method_Confidence' in df.columns:\n",
    "        conf_dist = df['Method_Confidence']\n",
    "        if isinstance(conf_dist, pd.DataFrame):\n",
    "            conf_dist = conf_dist.iloc[:, 0]\n",
    "        print(\"Confidence stats:\")\n",
    "        print(conf_dist.value_counts())\n",
    "\n",
    "    if 'Method_Label' in df.columns:\n",
    "        lbl_dist = df['Method_Label']\n",
    "        if isinstance(lbl_dist, pd.DataFrame):\n",
    "            lbl_dist = lbl_dist.iloc[:, 0]\n",
    "        print(\"Assigned method stats:\")\n",
    "        print(lbl_dist.value_counts().head(10))\n",
    "\n",
    "    print(\"\\nSample methods (from canonical vocabulary):\")\n",
    "    print(method_phrases[:10])\n",
    "    if method_dict is not None:\n",
    "        print(\"\\nAbbreviation mapping examples:\")\n",
    "        for i, (canonical, variants) in enumerate(list(method_dict.items())[:5]):\n",
    "            print(f\"  {canonical}: {', '.join(variants[:5])}\")\n",
    "\n",
    "# Run diagnostics\n",
    "comprehensive_diagnostics_granular(df, df['Method_TFIDF_Score'].to_numpy().reshape(-1, 1), method_phrases, method_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "literature-search-and-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
